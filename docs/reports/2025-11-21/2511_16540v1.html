<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks</title>
    <style>
        :root {
            /* 配色方案：Slate + Indigo */
            --primary-color: #4f46e5;
            --bg-body: #f8fafc;
            --bg-paper: #ffffff;
            --text-main: #1e293b;      /* Slate 800 */
            --text-body: #334155;      /* Slate 700 - 正文颜色略浅，减少视觉疲劳 */
            --text-secondary: #64748b; /* Slate 500 */
            --border-color: #e2e8f0;
            --code-bg: #f1f5f9;
            
            /* 警告色 */
            --warn-bg: #fff7ed;
            --warn-text: #9a3412;
            --warn-border: #fdba74;

            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-body);
            line-height: 1.8; /* 增加行高，适合阅读 */
            padding: 40px 20px;
            min-height: 100vh;
        }

        /* 阅读容器：限制宽度以提升阅读体验 */
        .container {
            max-width: 800px;
            margin: 0 auto;
            background-color: var(--bg-paper);
            border-radius: 16px; /* 更圆润的角 */
            padding: 40px 60px; /* 宽敞的内边距 */
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03);
        }

        /* 顶部导航 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            font-size: 14px;
        }

        .nav-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            transition: color 0.2s;
        }

        .nav-link:hover { color: var(--primary-color); }
        .nav-link::before { content: "←"; margin-right: 5px; }
        
        .arxiv-link {
            background-color: #f1f5f9;
            color: var(--text-main);
            padding: 6px 12px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
        }
        
        .arxiv-link:hover {
            background-color: #e2e8f0;
            color: var(--primary-color);
        }

        /* 论文头部信息 */
        .paper-header {
            margin-bottom: 40px;
        }

        .paper-title {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        /* 标签组 */
        .tags-wrapper {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .tag {
            background-color: #e0e7ff; /* Indigo 100 */
            color: #4338ca;            /* Indigo 700 */
            font-size: 12px;
            padding: 4px 10px;
            border-radius: 99px;
            font-weight: 500;
        }

        /* 元数据栏 */
        .metadata-box {
            background-color: #f8fafc;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            font-size: 14px;
            color: var(--text-secondary);
        }

        .meta-item {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .meta-label {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #94a3b8;
        }

        .meta-value {
            font-weight: 600;
            color: var(--text-main);
        }
        
        .score-badge {
            color: var(--primary-color);
        }

        /* 核心图片展示 */
        .core-image-container {
            margin: 40px 0;
            text-align: center;
            background-color: #f8fafc;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .core-image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        
        .image-caption {
            margin-top: 10px;
            font-size: 13px;
            color: var(--text-secondary);
            font-style: italic;
        }

        /* 警告框 */
        .warning-box {
            background-color: var(--warn-bg);
            border-left: 4px solid var(--warn-border);
            color: var(--warn-text);
            padding: 15px;
            border-radius: 0 6px 6px 0;
            margin: 20px 0;
            font-size: 14px;
        }

        /* 章节标题 */
        .section-header {
            display: flex;
            align-items: center;
            margin-top: 50px;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px dashed var(--border-color);
        }

        .section-header h2 {
            font-size: 24px;
            font-weight: 700;
            color: var(--text-main);
            margin: 0;
            position: relative;
        }
        
        /* 章节前的装饰点 */
        .section-header h2::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: var(--primary-color);
            border-radius: 50%;
            margin-right: 12px;
            vertical-align: middle;
        }

        /* Markdown 内容样式重置 - 极简学术风 */
        .content-body {
            font-size: 17px; /* 略大的字号适合阅读 */
            color: var(--text-body);
        }

        .content-body p {
            margin-bottom: 1.5em;
            text-align: justify;
        }

        .content-body h3 {
            font-size: 20px;
            font-weight: 600;
            color: var(--text-main);
            margin-top: 2em;
            margin-bottom: 1em;
        }
        
        .content-body h4 {
            font-size: 18px;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }

        .content-body ul, .content-body ol {
            margin-bottom: 1.5em;
            padding-left: 1.5em;
        }

        .content-body li {
            margin-bottom: 0.5em;
        }

        .content-body strong {
            color: var(--text-main);
            font-weight: 600;
        }
        
        /* 引用块 - 学术风 */
        .content-body blockquote {
            border-left: 4px solid var(--primary-color);
            background-color: #f8fafc;
            padding: 16px 20px;
            margin: 20px 0;
            font-style: italic;
            color: var(--text-secondary);
            border-radius: 0 8px 8px 0;
        }

        /* 代码块 */
        .content-body pre {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid var(--border-color);
        }

        .content-body code {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
            color: #d63384; /* 类似 GitHub 的代码红 */
        }
        
        .content-body pre code {
            color: inherit;
            padding: 0;
            background-color: transparent;
        }

        /* Footer */
        .footer {
            margin-top: 80px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        /* 移动端适配 */
        @media (max-width: 768px) {
            body { padding: 0; }
            
            .container {
                border-radius: 0;
                padding: 30px 20px;
                box-shadow: none;
            }

            .paper-title { font-size: 26px; }
            
            .metadata-box {
                flex-direction: column;
                gap: 15px;
            }
            
            .content-body { font-size: 16px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="index.html" class="nav-link">返回今日简报</a>
            <a href="http://arxiv.org/abs/2511.16540v1" target="_blank" class="arxiv-link">PDF / arXiv ↗</a>
        </div>

        <div class="paper-header">
            <h1 class="paper-title">Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks</h1>
            
            
            <div class="tags-wrapper">
                
                <span class="tag">大型语言模型</span>
                
                <span class="tag">可解释性</span>
                
                <span class="tag">文本分类</span>
                
                <span class="tag">激活分析</span>
                
                <span class="tag">深层激活</span>
                
            </div>
            

            <div class="metadata-box">
                
                <div class="meta-item" style="flex: 2; min-width: 200px;">
                    <span class="meta-label">作者单位</span>
                    <span class="meta-value">Independent, Chalmers Technical University, Lund University</span>
                </div>
                
                
                <div class="meta-item">
                    <span class="meta-label">推荐指数</span>
                    <span class="meta-value score-badge">0.498</span>
                </div>
                
                <div class="meta-item">
                    <span class="meta-label">arXiv ID</span>
                    <span class="meta-value">2511.16540v1</span>
                </div>
            </div>

            
        </div>

        
        <div class="core-image-container">
            
            <img src="../../images/2025-11-21/f831edf780b1a6f938207f62a62ce8e54d9fd32bfcebd87618ab809ddc50ba9e.jpg" alt="核心思路示意图" />
            <div class="image-caption">图 1：论文核心方法/架构示意图</div>
        </div>
        

        <div class="section-header">
            <h2>快速简介</h2>
        </div>
        <div class="content-body">
            <p>本文提出了一种通过分析大型语言模型（LLM）激活来分类文本块的框架，解决了LLM可解释性的问题。研究表明，使用Mistral-7B模型的激活，简单分类器可实现高达98%的F1分数，验证了深层激活在文本类别表示中的有效性，并展示了利用LLM生成高质量数据集的潜力。</p>
        </div>

        <div class="section-header">
            <h2>深度解读</h2>
        </div>
        <div class="content-body">
            
                <h3>现有问题</h3>

<p>本文旨在解决大型语言模型（LLM）的可解释性问题，特别是如何理解模型对整个文本块（而不仅仅是单个词元）的内部表示。随着LLM的应用越来越广泛，理解其工作原理和监控其输出变得至关重要。当前的研究需要从单个词元的预测扩展到对更大文本单元的整体理解，并探索如何利用LLM高效地生成和标注高质量的数据集。</p>

<h3>Hypothesis</h3>

<p>核心假设是：LLM的内部激活（activations）包含了关于文本块类别的高层、可区分的信息。具体来说：
- 可以通过分析LLM的激活，使用简单的分类器以极高的准确率预测出文本块的类别。
- 模型中更深层次的激活比浅层激活包含更丰富、更有效的文本结构表示，因此在分类任务上表现更好。
- LLM的生成能力可以被用来系统地创建多样化、高质量的标注数据集，以支持模型分析和训练。</p>

<h3>相关研究</h3>

<ul>
<li><strong>机制可解释性研究</strong>：主要关注单个词元的激活，以理解模型的内部工作机制。</li>
<li><strong>探测（Probing）研究</strong>：通过训练简单的分类器（探针）来从模型激活中提取人类可理解的概念或语言学属性。</li>
<li><strong>文本表示与分类</strong>：涉及文本嵌入和传统分类方法的研究，但通常不关注模型内部的表示。</li>
<li><strong>LLM的应用</strong>：利用Mistral-7B等模型进行文本生成，以及使用GPT系列模型进行数据标注。</li>
</ul>

<h3>解决方案</h3>

<p>本解释整合了多个论文片段，旨在全面阐述该研究提出的核心解决方案：一个通过分析大型语言模型（LLM）内部激活来预测文本体裁的新型框架。该方案从数据集的构建开始，到模型的训练与验证，最终实现对LLM输出的深层理解与监控。</p>

<hr />

<h4><strong>第一步：构建高质量的多样化数据集</strong></h4>

<p>解决方案的基础是创建一个高质量、多样化且经过标注的文本数据集。这个过程结合了自动化生成和人工审核，确保了数据的可靠性。</p>

<ol>
<li><p><strong>定义文本类别：</strong>
研究人员首先明确了五个核心的文本体裁类别，作为分类和标注的标准：</p>

<ul>
<li><strong>叙述 (narrative):</strong> 描述一系列关联事件的文本。</li>
<li><strong>解释性 (explanatory):</strong> 对某个事物进行阐述或说明的文本。</li>
<li><strong>指令性 (instructional):</strong> 提供如何操作或执行某事的详细步骤。</li>
<li><strong>演讲 (speech):</strong> 口头表达的书面记录。</li>
<li><strong>代码 (code):</strong> 计算机程序代码。</li>
</ul></li>
<li><p><strong>生成提示与文本：</strong></p>

<ul>
<li><strong>手动与合成提示：</strong> 首先，研究团队手动创建了一系列开放式提示，旨在引导LLM生成特定类别的文本。随后，利用 <strong>Mistral-7B-Instruct-v2.0</strong> 模型，基于已有提示生成更多样的合成提示，以扩大数据集的广度。</li>
<li><strong>文本生成：</strong> 使用这些提示，通过 Mistral 模型生成大量文本，每个输出限制在500个tokens以内。</li>
</ul></li>
<li><p><strong>文本分块与标注：</strong></p>

<ul>
<li><strong>定义分析单元：</strong> 论文的核心创新之一是将分析单元从单个词元（token）提升到更自然的 <strong>文本块（chunk）</strong>。文本块通常等同于一个段落，主要通过双换行符（<code>\n\n</code>）进行分割。</li>
<li><strong>自动化标注：</strong> 生成的文本被提交给更强大的 <strong>GPT-4 Turbo Preview</strong> 模型。该模型根据预设的提示，将连续的文本分割成独立的文本块，并为每个块标注其所属的体裁类别。</li>
</ul></li>
<li><p><strong>人工审核与最终数据集：</strong></p>

<ul>
<li>所有经过自动标注的文本块都由人类审查员进行审核，移除标注错误的样本，以确保最终数据集的准确性。</li>
<li>最终，该过程产生了包含3914个独特文本块的标注数据集，涵盖了上述五个类别。</li>
</ul></li>
</ol>

<h4><strong>第二步：设计并训练预测框架</strong></h4>

<p>在高质量数据集的基础上，研究团队设计并实施了一个预测框架，其目标是从LLM的内部激活中提取并分类文本块的体裁信息。</p>

<ol>
<li><p><strong>激活提取：</strong></p>

<ul>
<li>对于数据集中的每一个文本块，研究人员将其输入到 <strong>Mistral-7B</strong> 模型中。</li>
<li>在模型处理文本的过程中，提取其在不同层的 <strong>激活值（activations）</strong> 和 <strong>残差流（residual stream）</strong>。这些激活值被认为是模型对文本块内部表征的“快照”，包含了丰富的语义和结构信息。</li>
</ul></li>
<li><p><strong>分类器训练：</strong></p>

<ul>
<li>提取出的激活值被用作特征，输入到一系列简单的浅层学习分类器中进行训练。</li>
<li>研究中使用了多种来自 <code>scikit-learn</code> 库的分类器，并通过 <code>Lazy Predict</code> 库简化了在同一数据集上测试多种算法的过程。</li>
<li><strong>评价指标：</strong> 使用 <strong>宏F1分数（Macro F1-score）</strong> 作为主要评价指标，以确保在类别不均衡的数据集中，每个类别都得到同等的重视。数据集按照80%训练集和20%测试集的比例进行划分。</li>
</ul></li>
<li><p><strong>实验验证与分析：</strong></p>

<ul>
<li><strong>控制实验：</strong> 为了验证分类器确实是从有意义的激活模式中学习，而不是依赖虚假关联，研究团队构建了一个随机模型作为对照组。结果显示，真实模型的表现远超随机模型，证明了方法的有效性。</li>
<li><strong>维度缩减分析：</strong> 为了进一步探究模型如何表征不同体裁，研究人员使用 <strong>Qwen3-Embedding-0.6B</strong> 模型对文本块进行编码，并应用 <strong>PHATE</strong> 算法进行维度缩减和可视化。结果观察到，降维后的数据点呈现出与标注类别相对应的聚类趋势，表明模型的激活中确实编码了体裁相关的信息。</li>
</ul></li>
</ol>

<h4><strong>核心优势与结论</strong></h4>

<p>该解决方案通过创新的方法，为理解和解释LLM的行为提供了新的视角。</p>

<ul>
<li><strong>提高可解释性：</strong> 通过将分析单元从单个词元（token）提升到语义完整的文本块（chunk），该框架能够提供更高层次、更符合人类直觉的解释，帮助我们理解LLM是如何组织和生成复杂文本的。</li>
<li><strong>高效的输出监控：</strong> 该方法能够有效监控LLM的输出内容，自动识别其生成的文本体裁。实验结果表明，该框架能够以高达98%（合成数据集）和71%（CORE数据集）的F1分数成功提取文本体裁。</li>
<li><strong>自然的分析单元：</strong> 使用文本块作为分析单位，使得分析结果更易于理解和应用，特别是在文本生成和分类任务中具有重要价值。</li>
</ul>

<p><strong>总结来说，</strong> 本论文提出的详细解决方案是一个端到端的框架：它首先通过一个严谨的流程构建了一个高质量的标注数据集，然后利用该数据集训练分类器，以解码LLM在处理自然文本块时的内部激活。该框架不仅在预测文本体裁方面表现出色，更重要的是，它为深入探索LLM的内部工作机制、提升其可解释性和可靠性奠定了坚实的基础。</p>

<h3>实验设计</h3>

<ul>
<li><strong>数据集</strong>：构建了一个包含多种类别（如教学、解释、演讲、叙事、代码）的自定义合成数据集，并与一个现有的真实世界数据集（CORE）进行对比。</li>
<li><strong>模型</strong>：实验主要使用Mistral-7B模型来生成文本和提取激活。</li>
<li><strong>评估</strong>：使用简单的分类器作为探针模型，将数据集划分为训练集和测试集（如80%/20%），并使用宏F1分数（Macro F1-score）作为主要评估指标，以衡量分类性能。同时，将探针模型的性能与基线（如随机猜测）进行比较。</li>
</ul>

<h3>数据集和代码</h3>

<ul>
<li>实验中使用的数据集和代码已在GitHub上公开。</li>
<li><strong>链接</strong>：https://github.com/Aza-Spearal/Trajectories-Probing</li>
</ul>

<h3>实验结果</h3>

<ul>
<li><strong>高准确率</strong>：实验表明，使用从Mistral-7B激活中训练的简单分类器，可以在文本块分类任务上达到高达98%的F1分数，远超基线模型。</li>
<li><strong>深层表示更优</strong>：探测模型的性能随着所用激活层数的加深而提高，证实了更深层次的激活能够更好地表示文本的类别结构。</li>
<li><strong>方法有效性</strong>：探针模型在合成数据集上的表现优于在CORE数据集上的表现，这表明通过LLM生成的数据集能够有效地用于分析模型的内部表示。</li>
</ul>

<h3>论文贡献</h3>

<ol>
<li><strong>提出了一种新的可解释性框架</strong>：通过分析激活来理解和分类LLM生成的整个文本块，将可解释性研究从词元级别扩展到文本块级别。</li>
<li><strong>验证了LLM激活的表示能力</strong>：通过实验证明，LLM的激活中线性地编码了丰富的文本类别信息，并且深层激活的表示能力更强。</li>
<li><strong>提供了一种高效的数据生成方法</strong>：展示了如何利用LLM（如Mistral-7B和GPT-4）系统地生成和标注高质量、多样化的文本数据集，为未来的相关研究提供了宝贵的资源和思路。</li>
</ol>

            
        </div>

        <div class="footer">
            <p>Generated by AI Paper Review System at 2025-11-21 19:45:54</p>
            <p style="margin-top: 10px;">
                <a href="https://jycarlos1019.pp.ua">系统首页</a> • 
                <a href="../../search.html">搜索归档</a>
            </p>
        </div>
    </div>
</body>
</html>