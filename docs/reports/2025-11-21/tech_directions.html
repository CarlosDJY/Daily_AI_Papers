<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>主流技术方向 - 2025-11-21</title>
    <style>
        :root {
            /* 系统配色 */
            --primary-color: #4f46e5;   /* Indigo */
            --direction-color: #f97316; /* Orange 500 */
            --direction-dark: #c2410c;  /* Orange 700 */
            --direction-light: #fff7ed; /* Orange 50 */
            --direction-border: #fdba74;/* Orange 300 */
            
            --bg-body: #f8fafc;
            --bg-card: #ffffff;
            --text-main: #0f172a;
            --text-secondary: #64748b;
            --text-light: #94a3b8;
            --border-color: #e2e8f0;
            
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            
            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        /* SVG 图标 */
        .icon {
            width: 1.1em;
            height: 1.1em;
            display: inline-block;
            vertical-align: middle;
            stroke-width: 2;
            stroke: currentColor;
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
        }

        /* 导航栏 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            font-size: 14px;
        }

        .back-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: flex;
            align-items: center;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--primary-color); }
        .back-link .icon { margin-right: 6px; }

        .date-badge {
            background-color: #e0e7ff;
            color: var(--primary-color);
            padding: 4px 12px;
            border-radius: 99px;
            font-weight: 600;
            font-size: 13px;
        }

        /* 头部 */
        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .header h1 {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
            margin-bottom: 12px;
        }
        
        .header h1 .icon { color: var(--direction-color); width: 36px; height: 36px; }
        
        .header .subtitle { 
            color: var(--text-secondary);
            font-size: 16px;
        }

        /* --- Tab 导航栏 --- */
        .tabs-container {
            position: sticky;
            top: 20px;
            z-index: 100;
            margin-bottom: 30px;
            /* 磨砂玻璃效果 */
            background: rgba(248, 250, 252, 0.9);
            backdrop-filter: blur(8px);
            padding: 10px 0;
            border-radius: 16px;
        }

        .tabs-scroll {
            display: flex;
            gap: 12px;
            overflow-x: auto;
            padding: 4px 4px 12px 4px; /* 底部留空间给滚动条或阴影 */
            
            /* 隐藏滚动条但保持可滚动 */
            scrollbar-width: none; /* Firefox */
            -ms-overflow-style: none;  /* IE 10+ */
            
            /* 鼠标交互优化 */
            cursor: grab; /* 提示可拖拽 */
            user-select: none; /* 防止拖拽时选中文字 */
        }
        
        .tabs-scroll::-webkit-scrollbar { 
            display: none; /* Chrome/Safari */
        }

        /* 拖拽时的光标状态 */
        .tabs-scroll.active {
            cursor: grabbing;
        }

        .tab-btn {
            flex-shrink: 0;
            background-color: var(--bg-card);
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
            padding: 10px 20px;
            border-radius: 99px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: var(--shadow-sm);
            display: flex;
            align-items: center;
            gap: 8px;
            /* 防止图片/文字干扰拖拽 */
            pointer-events: auto; 
        }

        .tab-btn:hover {
            border-color: var(--direction-border);
            color: var(--direction-color);
            transform: translateY(-2px);
        }

        .tab-btn.active {
            background-color: var(--direction-color);
            color: white;
            border-color: var(--direction-color);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3); /* Orange glow */
        }

        .tab-count {
            background-color: rgba(0,0,0,0.1);
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 11px;
        }
        
        .tab-btn.active .tab-count {
            background-color: rgba(255,255,255,0.2);
        }

        /* --- 内容区域 --- */
        .tab-pane {
            display: none; /* 默认隐藏 */
            animation: fadeIn 0.3s ease-out;
        }
        
        .tab-pane.active {
            display: block;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .direction-block {
            background-color: var(--bg-card);
            border-radius: 20px;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-md);
            overflow: hidden;
        }

        .direction-header-info {
            background-color: var(--direction-light);
            padding: 24px 30px;
            border-bottom: 1px solid var(--direction-border);
        }
        
        .direction-title-lg {
            font-size: 22px;
            font-weight: 800;
            color: var(--direction-dark);
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .direction-desc-lg {
            font-size: 15px;
            color: #9a3412;
            opacity: 0.9;
        }

        /* --- 手风琴论文列表 --- */
        .paper-list {
            padding: 10px 30px 30px;
        }

        .paper-item {
            border-bottom: 1px solid var(--border-color);
        }
        
        .paper-item:last-child { border-bottom: none; }

        /* 折叠头部 (可点击) */
        .paper-header {
            padding: 20px 0;
            cursor: pointer;
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            gap: 16px;
            group: paper-header;
        }

        .paper-title-row {
            flex-grow: 1;
        }

        .paper-title {
            font-size: 17px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            transition: color 0.2s;
            margin-bottom: 6px;
        }
        
        .paper-header:hover .paper-title {
            color: var(--direction-color);
        }

        .paper-meta-preview {
            font-size: 13px;
            color: var(--text-light);
            display: flex;
            align-items: center;
            gap: 12px;
        }
        
        .score-badge {
            background-color: #f3f4f6;
            color: var(--text-secondary);
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            font-size: 12px;
        }
        
        .toggle-icon {
            color: var(--text-light);
            transition: transform 0.3s ease, color 0.2s;
            flex-shrink: 0;
            margin-top: 4px;
        }
        
        /* 激活状态样式 */
        .paper-item.expanded .toggle-icon {
            transform: rotate(180deg);
            color: var(--direction-color);
        }
        
        .paper-item.expanded .paper-title {
            color: var(--direction-color);
        }

        /* 折叠内容区 */
        .paper-body {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
            padding-left: 4px; /* 微调对齐 */
        }
        
        .paper-content-inner {
            padding-bottom: 24px;
            color: var(--text-secondary);
            font-size: 15px;
            line-height: 1.7;
            text-align: justify;
        }
        
        .paper-links {
            margin-top: 12px;
            padding-top: 12px;
            border-top: 1px dashed var(--border-color);
            display: flex;
            gap: 16px;
            font-size: 13px;
        }
        
        .action-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        
        .action-link:hover { text-decoration: underline; }

        /* 空状态 */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            background-color: var(--bg-card);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .footer {
            margin-top: 60px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }
        
        .footer a { color: var(--text-secondary); text-decoration: none; }

        @media (max-width: 640px) {
            .container { padding: 20px 15px; }
            .direction-header-info { padding: 20px; }
            .paper-list { padding: 0 20px 20px; }
            .paper-title { font-size: 16px; }
        }
    </style>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 1. Tab 栏鼠标拖拽滚动 (Draggable Scroll)
            const slider = document.querySelector('.tabs-scroll');
            let isDown = false;
            let startX;
            let scrollLeft;
            let isDragging = false; // 区分是“点击”还是“拖拽”

            slider.addEventListener('mousedown', (e) => {
                isDown = true;
                isDragging = false;
                slider.classList.add('active');
                startX = e.pageX - slider.offsetLeft;
                scrollLeft = slider.scrollLeft;
            });

            slider.addEventListener('mouseleave', () => {
                isDown = false;
                slider.classList.remove('active');
            });

            slider.addEventListener('mouseup', () => {
                isDown = false;
                slider.classList.remove('active');
                // 如果是拖拽结束，为了防止触发 click，我们在 click 事件中做判断
                setTimeout(() => { isDragging = false; }, 0);
            });

            slider.addEventListener('mousemove', (e) => {
                if (!isDown) return;
                e.preventDefault(); // 防止选中文字
                const x = e.pageX - slider.offsetLeft;
                const walk = (x - startX) * 2; // 滚动速度系数
                slider.scrollLeft = scrollLeft - walk;
                
                // 如果移动距离超过 5px，则视为拖拽，不是点击
                if (Math.abs(walk) > 5) {
                    isDragging = true;
                }
            });

            // 2. Tab 切换逻辑
            const tabs = document.querySelectorAll('.tab-btn');
            const panes = document.querySelectorAll('.tab-pane');

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    // 如果刚才是在拖拽，则拦截点击，不切换 Tab
                    if (isDragging) {
                        e.preventDefault();
                        e.stopPropagation();
                        return;
                    }

                    // 移除所有激活状态
                    tabs.forEach(t => t.classList.remove('active'));
                    panes.forEach(p => p.classList.remove('active'));

                    // 激活当前
                    tab.classList.add('active');
                    const targetId = tab.getAttribute('data-target');
                    document.getElementById(targetId).classList.add('active');
                    
                    // 滚动 Tab 到可见区域 (移动端/拖拽后体验优化)
                    tab.scrollIntoView({ behavior: 'smooth', block: 'nearest', inline: 'center' });
                });
            });

            // 3. 论文折叠/展开逻辑
            const paperHeaders = document.querySelectorAll('.paper-header');

            paperHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const item = this.parentElement;
                    const body = item.querySelector('.paper-body');
                    
                    // 切换状态
                    item.classList.toggle('expanded');
                    
                    if (item.classList.contains('expanded')) {
                        body.style.maxHeight = body.scrollHeight + "px";
                    } else {
                        body.style.maxHeight = null;
                    }
                });
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <!-- 导航 -->
        <div class="nav-bar">
            <a href="index.html" class="back-link">
                <svg class="icon" viewBox="0 0 24 24"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
                返回每日简报
            </a>
            <div class="date-badge">2025-11-21</div>
        </div>

        <!-- 头部 -->
        <div class="header">
            <h1>
                <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="10"></circle><polygon points="16.24 7.76 14.12 14.12 7.76 16.24 9.88 9.88 16.24 7.76"></polygon></svg>
                每日主流技术方向
            </h1>
            <div class="subtitle">聚合 RAG / LLM / Agent 等核心赛道，点击下方标签切换</div>
        </div>

        
        
        
        
        <!-- Tab 导航栏 (可拖拽) -->
        <div class="tabs-container">
            <div class="tabs-scroll">
                
                <button class="tab-btn active" data-target="tab-1">
                    Agent
                    <span class="tab-count">2</span>
                </button>
                
                <button class="tab-btn " data-target="tab-2">
                    Alignment
                    <span class="tab-count">5</span>
                </button>
                
                <button class="tab-btn " data-target="tab-3">
                    LLM
                    <span class="tab-count">39</span>
                </button>
                
                <button class="tab-btn " data-target="tab-4">
                    Multimodal
                    <span class="tab-count">10</span>
                </button>
                
                <button class="tab-btn " data-target="tab-5">
                    Optimization
                    <span class="tab-count">4</span>
                </button>
                
                <button class="tab-btn " data-target="tab-6">
                    RAG
                    <span class="tab-count">2</span>
                </button>
                
                <button class="tab-btn " data-target="tab-7">
                    RL
                    <span class="tab-count">24</span>
                </button>
                
                <button class="tab-btn " data-target="tab-8">
                    Vision
                    <span class="tab-count">3</span>
                </button>
                
            </div>
        </div>

        <!-- 内容区域 -->
        <div class="content-wrapper">
            
            <div id="tab-1" class="tab-pane active">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Agent
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 2 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在智能体（Agent）技术方向的研究主要集中在提升模型的解释性和多模态数据处理能力。论文《SurvAgent》提出了一种层次化的多智能体系统，旨在改善癌症生存预测的透明度和准确性，而《JudgeBoard》则关注小型语言模型在推理评估中的表现，强调了对答案正确性的直接判断需求。这些研究反映出在智能体应用中，增强模型的可解释性和多样性处理能力是当前的共性趋势，具有重要的临床和技术潜在价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16635v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16635v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15958v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15958v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-2" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Alignment
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 5 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在对齐（Alignment）技术方向的研究动态主要集中在提高自动化系统在特定领域的准确性与可靠性。研究者们通过建立新的基准和评估框架，探讨如何更好地评估和优化自动语音识别（ASR）和大语言模型（LLMs）的表现，以确保其输出与人类意图相符。此外，针对模型在实际应用中的不确定性和幻觉现象，提出了新的量化方法和验证工具，显示出对安全性和准确性的重视。这些研究不仅推动了技术的进步，也为实际应用提供了更高的潜在价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16544v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16544v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16438v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16438v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16324v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16324v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16275v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16275v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16198v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16198v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-3" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            LLM
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 39 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前大型语言模型（LLM）领域的研究动态主要集中在多模态交互、推理能力的提升以及高效模型架构的构建上。研究者们探索如何在生成过程中实时整合文本推理与视觉生成，提升模型的理解和生成能力。同时，针对模型训练成本的挑战，提出了更为高效的多任务学习框架，以支持多种规模和应用目标。此外，随着多模态检索和长视频理解的进步，研究者们也在努力解决信息丢失和上下文理解等问题，展现出该领域在实际应用中的潜在价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16671v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16671v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16664v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16664v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16654v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16654v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Codec2Vec: Self-Supervised Speech Representation Learning Using Neural Speech Codecs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in neural audio codecs have not only enabled superior audio compression but also enhanced speech synthesis techniques. Researchers are now exploring their potential as universal acoustic feature extractors for a broader range of speech processing tasks. Building on this trend, we introduce Codec2Vec, the first speech representation learning framework that relies exclusively on discrete audio codec units. This approach offers several advantages, including improved data storage and transmission efficiency, faster training, and enhanced data privacy. We explore masked prediction with various training target derivation strategies to thoroughly understand the effectiveness of this framework. Evaluated on the SUPERB benchmark, Codec2Vec achieves competitive performance compared to continuous-input models while reducing storage requirements by up to 16.5x and training time by 2.3x, showcasing its scalability and efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16639v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16639v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16635v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16635v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16595v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16595v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16590v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16590v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16577v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16577v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16544v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16544v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16543v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16543v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16540v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16540v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16528v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16528v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiMo-Embodied: X-Embodied Foundation Model Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16518v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16518v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Music Recommendation with Large Language Models: Challenges, Opportunities, and Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Music Recommender Systems (MRS) have long relied on an information-retrieval framing, where progress is measured mainly through accuracy on retrieval-oriented subtasks. While effective, this reductionist paradigm struggles to address the deeper question of what makes a good recommendation, and attempts to broaden evaluation, through user studies or fairness analyses, have had limited impact. The emergence of Large Language Models (LLMs) disrupts this framework: LLMs are generative rather than ranking-based, making standard accuracy metrics questionable. They also introduce challenges such as hallucinations, knowledge cutoffs, non-determinism, and opaque training data, rendering traditional train/test protocols difficult to interpret. At the same time, LLMs create new opportunities, enabling natural-language interaction and even allowing models to act as evaluators.
  This work argues that the shift toward LLM-driven MRS requires rethinking evaluation. We first review how LLMs reshape user modeling, item modeling, and natural-language recommendation in music. We then examine evaluation practices from NLP, highlighting methodologies and open challenges relevant to MRS. Finally, we synthesize insights-focusing on how LLM prompting applies to MRS, to outline a structured set of success and risk dimensions. Our goal is to provide the MRS community with an updated, pedagogical, and cross-disciplinary perspective on evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16478v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16478v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Arctic-Extract Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16470v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16470v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Anatomy of an Idiom: Tracing Non-Compositionality in Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16467v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16467v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ESGBench: A Benchmark for Explainable ESG Question Answering in Corporate Sustainability Reports
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We present ESGBench, a benchmark dataset and evaluation framework designed to assess explainable ESG question answering systems using corporate sustainability reports. The benchmark consists of domain-grounded questions across multiple ESG themes, paired with human-curated answers and supporting evidence to enable fine-grained evaluation of model reasoning. We analyze the performance of state-of-the-art LLMs on ESGBench, highlighting key challenges in factual consistency, traceability, and domain alignment. ESGBench aims to accelerate research in transparent and accountable ESG-focused AI systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16438v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16438v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16423v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16423v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Classification of worldwide news articles by perceived quality, 2018-2024
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16416v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16416v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AICC: Parse HTML Finer, Make Models Better -- A 7.3T AI-Ready Corpus Built by a Model-Based HTML Parser
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While web data quality is crucial for large language models, most curation efforts focus on filtering and deduplication,treating HTML-to-text extraction as a fixed pre-processing step. Existing web corpora rely on heuristic-based extractors like Trafilatura, which struggle to preserve document structure and frequently corrupt structured elements such as formulas, codes, and tables. We hypothesize that improving extraction quality can be as impactful as aggressive filtering strategies for downstream performance. We introduce MinerU-HTML, a novel extraction pipeline that reformulates content extraction as a sequence labeling problem solved by a 0.6B-parameter language model. Unlike text-density heuristics, MinerU-HTML leverages semantic understanding and employs a two-stage formatting pipeline that explicitly categorizes semantic elements before converting to Markdown. Crucially, its model-based approach is inherently scalable, whereas heuristic methods offer limited improvement pathways. On MainWebBench, our benchmark of 7,887 annotated web pages, MinerU-HTML achieves 81.8\% ROUGE-N F1 compared to Trafilatura's 63.6\%, with exceptional structured element preservation (90.9\% for code blocks, 94.0\% for formulas). Using MinerU-HTML, we construct AICC (AI-ready Common Crawl), a 7.3-trillion token multilingual corpus from two Common Crawl snapshots. In controlled pretraining experiments where AICC and Trafilatura-extracted TfCC undergo identical filtering, models trained on AICC (62B tokens) achieve 50.8\% average accuracy across 13 benchmarks, outperforming TfCC by 1.08pp-providing direct evidence that extraction quality significantly impacts model capabilities. AICC also surpasses RefinedWeb and FineWeb on key benchmarks. We publicly release MainWebBench, MinerU-HTML, and AICC, demonstrating that HTML extraction is a critical, often underestimated component of web corpus construction.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16397v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16397v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16353v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16353v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        NLP Datasets for Idiom and Figurative Language Tasks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Idiomatic and figurative language form a large portion of colloquial speech and writing. With social media, this informal language has become more easily observable to people and trainers of large language models (LLMs) alike. While the advantage of large corpora seems like the solution to all machine learning and Natural Language Processing (NLP) problems, idioms and figurative language continue to elude LLMs. Finetuning approaches are proving to be optimal, but better and larger datasets can help narrow this gap even further. The datasets presented in this paper provide one answer, while offering a diverse set of categories on which to build new models and develop new approaches. A selection of recent idiom and figurative language datasets were used to acquire a combined idiom list, which was used to retrieve context sequences from a large corpus. One large-scale dataset of potential idiomatic and figurative language expressions and two additional human-annotated datasets of definite idiomatic and figurative language expressions were created to evaluate the baseline ability of pre-trained language models in handling figurative meaning through idiom recognition (detection) tasks. The resulting datasets were post-processed for model agnostic training compatibility, utilized in training, and evaluated on slot labeling and sequence tagging.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16345v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16345v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16334v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16334v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16331v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16331v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16324v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16324v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16275v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16275v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16209v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16209v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SemanticCite: Citation Verification with AI-Powered Full-Text Analysis and Evidence-Based Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Effective scientific communication depends on accurate citations that validate sources and guide readers to supporting evidence. Yet academic literature faces mounting challenges: semantic citation errors that misrepresent sources, AI-generated hallucinated references, and traditional citation formats that point to entire papers without indicating which sections substantiate specific claims. We introduce SemanticCite, an AI-powered system that verifies citation accuracy through full-text source analysis while providing rich contextual information via detailed reasoning and relevant text snippets. Our approach combines multiple retrieval methods with a four-class classification system (Supported, Partially Supported, Unsupported, Uncertain) that captures nuanced claim-source relationships and enables appropriate remedial actions for different error types. Our experiments show that fine-tuned lightweight language models achieve performance comparable to large commercial systems with significantly lower computational requirements, making large-scale citation verification practically feasible. The system provides transparent, evidence-based explanations that support user understanding and trust. We contribute a comprehensive dataset of over 1,000 citations with detailed alignments, functional classifications, semantic annotations, and bibliometric metadata across eight disciplines, alongside fine-tuned models and the complete verification framework as open-source software. SemanticCite addresses critical challenges in research integrity through scalable citation verification, streamlined peer review, and quality control for AI-generated content, providing an open-source foundation for maintaining citation accuracy at scale.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16198v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16198v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16147v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16147v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16122v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16122v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Early science acceleration experiments with GPT-5
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16072v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16072v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning Tractable Distributions Of Language Model Continuations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16054v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16054v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Liars' Bench: Evaluating Lie Detectors for Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16035v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16035v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16018v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16018v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        QueryGym: A Toolkit for Reproducible LLM-Based Query Reformulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We present QueryGym, a lightweight, extensible Python toolkit that supports large language model (LLM)-based query reformulation. This is an important tool development since recent work on llm-based query reformulation has shown notable increase in retrieval effectiveness. However, while different authors have sporadically shared the implementation of their methods, there is no unified toolkit that provides a consistent implementation of such methods, which hinders fair comparison, rapid experimentation, consistent benchmarking and reliable deployment. QueryGym addresses this gap by providing a unified framework for implementing, executing, and comparing llm-based reformulation methods. The toolkit offers: (1) a Python API for applying diverse LLM-based methods, (2) a retrieval-agnostic interface supporting integration with backends such as Pyserini and PyTerrier, (3) a centralized prompt management system with versioning and metadata tracking, (4) built-in support for benchmarks like BEIR and MS MARCO, and (5) a completely open-source extensible implementation available to all researchers. QueryGym is publicly available at https://github.com/radinhamidi/QueryGym.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15996v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15996v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CARE-RAG - Clinical Assessment and Reasoning in RAG
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15994v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15994v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TOD-ProcBench: Benchmarking Complex Instruction-Following in Task-Oriented Dialogues
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In real-world task-oriented dialogue (TOD) settings, agents are required to strictly adhere to complex instructions while conducting multi-turn conversations with customers. These instructions are typically presented in natural language format and include general guidelines and step-by-step procedures with complex constraints. Existing TOD benchmarks often oversimplify the complex nature of these instructions by reducing them to simple schemas composed of intents, slots, and API call configurations. To address this gap and systematically benchmark LLMs' instruction-following capabilities, we propose TOD-ProcBench, a challenging benchmark featuring complex process instructions with intricate, fine-grained constraints that evaluates various LLMs' abilities to understand and follow instructions in multi-turn TODs. Our benchmark dataset comprises instruction documents derived from the high-quality ABCD dataset with corresponding conversations under human quality control. We formulate fine-grained constraints and action procedures as multi-level condition-action instruction statements. We design three tasks to comprehensively benchmark LLMs' complex instruction-following capabilities in multi-turn TODs. Task 1 evaluates how LLMs retrieve the most relevant statement from a complex instruction and predict the corresponding next action. In Task 2, we synthesize instruction-violating responses by injecting inconsistencies and manipulating the original instructions, and then we analyze how effectively LLMs can identify instruction-violating responses. Task 3 investigates LLMs' abilities in conditional generation of instruction-following responses based on the original complex instructions. Additionally, we conduct studies on the impact of multilingual settings and different instruction text formats on compliance performance. We release our benchmark under the Llama 3.3 Community License Agreement.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15976v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15976v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15958v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15958v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-4" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Multimodal
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 10 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前多模态技术研究的主要动态集中在提升生成与检索的效率及准确性，尤其是在视觉生成与文本推理的结合方面。研究者们逐渐重视模型的实时交互能力和多模态数据的整合，旨在提升模型的透明度和可解释性。此外，跨领域的基础模型如MiMo-Embodied和Arctic-Extract的出现，展示了多模态技术在实际应用中的潜在价值，尤其是在自主驾驶和文档处理等领域。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16671v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16671v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16654v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16654v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SurvAgent: Hierarchical CoT-Enhanced Case Banking and Dichotomy-Based Multi-Agent System for Multimodal Survival Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Survival analysis is critical for cancer prognosis and treatment planning, yet existing methods lack the transparency essential for clinical adoption. While recent pathology agents have demonstrated explainability in diagnostic tasks, they face three limitations for survival prediction: inability to integrate multimodal data, ineffective region-of-interest exploration, and failure to leverage experiential learning from historical cases. We introduce SurvAgent, the first hierarchical chain-of-thought (CoT)-enhanced multi-agent system for multimodal survival prediction. SurvAgent consists of two stages: (1) WSI-Gene CoT-Enhanced Case Bank Construction employs hierarchical analysis through Low-Magnification Screening, Cross-Modal Similarity-Aware Patch Mining, and Confidence-Aware Patch Mining for pathology images, while Gene-Stratified analysis processes six functional gene categories. Both generate structured reports with CoT reasoning, storing complete analytical processes for experiential learning. (2) Dichotomy-Based Multi-Expert Agent Inference retrieves similar cases via RAG and integrates multimodal reports with expert predictions through progressive interval refinement. Extensive experiments on five TCGA cohorts demonstrate SurvAgent's superority over conventional methods, proprietary MLLMs, and medical agents, establishing a new paradigm for explainable AI-driven survival prediction in precision oncology.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16635v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16635v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16595v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16595v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiMo-Embodied: X-Embodied Foundation Model Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16518v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16518v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Arctic-Extract Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16470v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16470v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16423v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16423v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16334v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16334v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning Tractable Distributions Of Language Model Continuations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16054v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16054v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-5" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Optimization
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 4 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前优化技术方向的研究动态主要集中在提升大型语言模型（LLMs）的效率与安全性。研究者们通过提出新框架（如Nemotron Elastic和Prism）来实现模型训练与推荐系统的解耦优化，从而降低资源消耗和提高性能。同时，针对提示工程的复杂性，自动化提示优化（APO）方法逐渐受到重视，旨在简化这一过程并增强模型的实用性。这些研究不仅推动了LLMs在多场景应用中的发展，也为其安全性提供了新的防护思路。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Nemotron Elastic: Towards Efficient Many-in-One Reasoning LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Training a family of large language models targeting multiple scales and deployment objectives is prohibitively expensive, requiring separate training runs for each different size. Recent work on model compression through pruning and knowledge distillation has reduced this cost; however, this process still incurs hundreds of billions of tokens worth of training cost per compressed model. In this paper, we present Nemotron Elastic, a framework for building reasoning-oriented LLMs, including hybrid Mamba-Attention architectures, that embed multiple nested submodels within a single parent model, each optimized for different deployment configurations and budgets. Each of these submodels shares weights with the parent model and can be extracted zero-shot during deployment without additional training or fine-tuning. We enable this functionality through an end-to-end trained router, tightly coupled to a two-stage training curriculum designed specifically for reasoning models. We additionally introduce group-aware SSM elastification that preserves Mamba's structural constraints, heterogeneous MLP elastification, normalized MSE-based layer importance for improved depth selection, and knowledge distillation enabling simultaneous multi-budget optimization. We apply Nemotron Elastic to the Nemotron Nano V2 12B model, simultaneously producing a 9B and a 6B model using only 110B training tokens; this results in over 360x cost reduction compared to training model families from scratch, and around 7x compared to SoTA compression techniques. Each of the nested models performs on par or better than the SoTA in accuracy. Moreover, unlike other compression methods, the nested capability of our approach allows having a many-in-one reasoning model that has constant deployment memory against the number of models in the family.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16664v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16664v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16543v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16543v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PSM: Prompt Sensitivity Minimization via LLM-Guided Black-Box Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    System prompts are critical for guiding the behavior of Large Language Models (LLMs), yet they often contain proprietary logic or sensitive information, making them a prime target for extraction attacks. Adversarial queries can successfully elicit these hidden instructions, posing significant security and privacy risks. Existing defense mechanisms frequently rely on heuristics, incur substantial computational overhead, or are inapplicable to models accessed via black-box APIs. This paper introduces a novel framework for hardening system prompts through shield appending, a lightweight approach that adds a protective textual layer to the original prompt. Our core contribution is the formalization of prompt hardening as a utility-constrained optimization problem. We leverage an LLM-as-optimizer to search the space of possible SHIELDs, seeking to minimize a leakage metric derived from a suite of adversarial attacks, while simultaneously preserving task utility above a specified threshold, measured by semantic fidelity to baseline outputs. This black-box, optimization-driven methodology is lightweight and practical, requiring only API access to the target and optimizer LLMs. We demonstrate empirically that our optimized SHIELDs significantly reduce prompt leakage against a comprehensive set of extraction attacks, outperforming established baseline defenses without compromising the model's intended functionality. Our work presents a paradigm for developing robust, utility-aware defenses in the escalating landscape of LLM security. The code is made public on the following link: https://github.com/psm-defense/psm
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16209v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16209v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16122v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16122v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-6" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RAG
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 2 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前RAG（检索增强生成）技术的研究动态主要集中在多模态信息的有效利用和临床应用中的推理能力提升。研究者们发现，现有的多模态RAG系统在处理图像信息时存在信息损失的问题，而在临床环境中，尽管有权威证据的支持，模型的推理能力仍然不足，导致错误输出。这些趋势表明，未来的研究需要更加关注如何优化信息检索与推理之间的连接，以提高模型在复杂场景中的实用性和准确性。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Comparison of Text-Based and Image-Based Retrieval in Multimodal Retrieval Augmented Generation Large Language Model Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in Retrieval-Augmented Generation (RAG) have enabled Large Language Models (LLMs) to access multimodal knowledge bases containing both text and visual information such as charts, diagrams, and tables in financial documents. However, existing multimodal RAG systems rely on LLM-based summarization to convert images into text during preprocessing, storing only text representations in vector databases, which causes loss of contextual information and visual details critical for downstream retrieval and question answering. To address this limitation, we present a comprehensive comparative analysis of two retrieval approaches for multimodal RAG systems, including text-based chunk retrieval (where images are summarized into text before embedding) and direct multimodal embedding retrieval (where images are stored natively in the vector space). We evaluate all three approaches across 6 LLM models and a two multi-modal embedding models on a newly created financial earnings call benchmark comprising 40 question-answer pairs, each paired with 2 documents (1 image and 1 text chunk). Experimental results demonstrate that direct multimodal embedding retrieval significantly outperforms LLM-summary-based approaches, achieving absolute improvements of 13% in mean average precision (mAP@5) and 11% in normalized discounted cumulative gain. These gains correspond to relative improvements of 32% in mAP@5 and 20% in nDCG@5, providing stronger evidence of their practical impact. We additionally find that direct multimodal retrieval produces more accurate and factually consistent answers as measured by LLM-as-a-judge pairwise comparisons. We demonstrate that LLM summarization introduces information loss during preprocessing, whereas direct multimodal embeddings preserve visual context for retrieval and inference.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16654v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16654v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CARE-RAG - Clinical Assessment and Reasoning in RAG
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15994v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15994v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-7" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RL
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 24 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在强化学习（RL）领域的研究动态主要集中在多模态交互、长视频理解和智能代理的鲁棒性等方面。研究者们逐渐认识到在生成过程中融入文本推理的重要性，并探索如何提高模型在复杂环境中的适应性，同时也在努力提升自然语言理解的准确性和效率。这些趋势表明，跨领域的整合和动态适应能力将是未来RL技术发展的重要方向，具有广泛的应用潜力。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Thinking-while-Generating: Interleaving Textual Reasoning throughout Visual Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in visual generation have increasingly explored the integration of reasoning capabilities. They incorporate textual reasoning, i.e., think, either before (as pre-planning) or after (as post-refinement) the generation process, yet they lack on-the-fly multimodal interaction during the generation itself. In this preliminary study, we introduce Thinking-while-Generating (TwiG), the first interleaved framework that enables co-evolving textual reasoning throughout the visual generation process. As visual content is progressively generating, textual reasoning is interleaved to both guide upcoming local regions and reflect on previously synthesized ones. This dynamic interplay produces more context-aware and semantically rich visual outputs. To unveil the potential of this framework, we investigate three candidate strategies, zero-shot prompting, supervised fine-tuning (SFT) on our curated TwiG-50K dataset, and reinforcement learning (RL) via a customized TwiG-GRPO strategy, each offering unique insights into the dynamics of interleaved reasoning. We hope this work inspires further research into interleaving textual reasoning for enhanced visual generation. Code will be released at: https://github.com/ZiyuGuo99/Thinking-while-Generating.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16671v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16671v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeViper: A Hybrid Mamba-Transformer Vision-Language Model for Efficient Long Video Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce TimeViper, a hybrid vision-language model designed to tackle challenges of long video understanding. Processing long videos demands both an efficient model architecture and an effective mechanism for handling extended temporal contexts. To this end, TimeViper adopts a hybrid Mamba-Transformer backbone that combines the efficiency of state-space models with the expressivity of attention mechanisms. Through this hybrid design, we reveal the vision-to-text information aggregation phenomenon, where information progressively flows from vision tokens to text tokens across increasing LLM depth, resulting in severe vision token redundancy. Motivated by this observation, we propose TransV, a token information transfer module that transfers and compresses vision tokens into instruction tokens while maintaining multimodal understanding capabilities. This design enables TimeViper to process hour-long videos exceeding 10,000 frames. Extensive experiments across multiple benchmarks demonstrate that TimeViper competes with state-of-the-art models while extending frame numbers. We further analyze attention behaviors of both Mamba and Transformer layers, offering new insights into hybrid model interpretability. This work represents an initial step towards developing, interpreting, and compressing hybrid Mamba-Transformer architectures.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16595v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16595v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        D-GARA: A Dynamic Benchmarking Framework for GUI Agent Robustness in Real-World Anomalies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Developing intelligent agents capable of operating a wide range of Graphical User Interfaces (GUIs) with human-level proficiency is a key milestone on the path toward Artificial General Intelligence. While most existing datasets and benchmarks for training and evaluating GUI agents are static and idealized, failing to reflect the complexity and unpredictability of real-world environments, particularly the presence of anomalies. To bridge this research gap, we propose D-GARA, a dynamic benchmarking framework, to evaluate Android GUI agent robustness in real-world anomalies. D-GARA introduces a diverse set of real-world anomalies that GUI agents commonly face in practice, including interruptions such as permission dialogs, battery warnings, and update prompts. Based on D-GARA framework, we construct and annotate a benchmark featuring commonly used Android applications with embedded anomalies to support broader community research. Comprehensive experiments and results demonstrate substantial performance degradation in state-of-the-art GUI agents when exposed to anomaly-rich environments, highlighting the need for robustness-aware learning. D-GARA is modular and extensible, supporting the seamless integration of new tasks, anomaly types, and interaction scenarios to meet specific evaluation goals.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16590v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16590v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Integrating Symbolic Natural Language Understanding and Language Models for Word Sense Disambiguation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Word sense disambiguation is a fundamental challenge in natural language understanding. Current methods are primarily aimed at coarse-grained representations (e.g. WordNet synsets or FrameNet frames) and require hand-annotated training data to construct. This makes it difficult to automatically disambiguate richer representations (e.g. built on OpenCyc) that are needed for sophisticated inference. We propose a method that uses statistical language models as oracles for disambiguation that does not require any hand-annotation of training data. Instead, the multiple candidate meanings generated by a symbolic NLU system are converted into distinguishable natural language alternatives, which are used to query an LLM to select appropriate interpretations given the linguistic context. The selected meanings are propagated back to the symbolic NLU system. We evaluate our method against human-annotated gold answers to demonstrate its effectiveness.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16577v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16577v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WER is Unaware: Assessing How ASR Errors Distort Clinical Understanding in Patient Facing Dialogue
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As Automatic Speech Recognition (ASR) is increasingly deployed in clinical dialogue, standard evaluations still rely heavily on Word Error Rate (WER). This paper challenges that standard, investigating whether WER or other common metrics correlate with the clinical impact of transcription errors. We establish a gold-standard benchmark by having expert clinicians compare ground-truth utterances to their ASR-generated counterparts, labeling the clinical impact of any discrepancies found in two distinct doctor-patient dialogue datasets. Our analysis reveals that WER and a comprehensive suite of existing metrics correlate poorly with the clinician-assigned risk labels (No, Minimal, or Significant Impact). To bridge this evaluation gap, we introduce an LLM-as-a-Judge, programmatically optimized using GEPA to replicate expert clinical assessment. The optimized judge (Gemini-2.5-Pro) achieves human-comparable performance, obtaining 90% accuracy and a strong Cohen's $κ$ of 0.816. This work provides a validated, automated framework for moving ASR evaluation beyond simple textual fidelity to a necessary, scalable assessment of safety in clinical dialogue.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16544v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16544v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Oracle and The Prism: A Decoupled and Efficient Framework for Generative Recommendation Explanation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of Large Language Models (LLMs) into explainable recommendation systems often leads to a performance-efficiency trade-off in end-to-end architectures, where joint optimization of ranking and explanation can result in suboptimal compromises. To resolve this, we propose Prism, a novel decoupled framework that rigorously separates the recommendation process into a dedicated ranking stage and an explanation generation stage.
  Inspired by knowledge distillation, Prism leverages a powerful teacher LLM (e.g., FLAN-T5-XXL) as an Oracle to produce high-fidelity explanatory knowledge. A compact, fine-tuned student model (e.g., BART-Base), the Prism, then specializes in synthesizing this knowledge into personalized explanations. This decomposition ensures that each component is optimized for its specific objective, eliminating inherent conflicts in coupled models.
  Extensive experiments on benchmark datasets demonstrate that our 140M-parameter Prism model significantly outperforms its 11B-parameter teacher in human evaluations of faithfulness and personalization, while achieving a 24 times speedup and a 10 times reduction in memory consumption during inference. These results validate that decoupling, coupled with targeted distillation, provides an efficient and effective pathway to high-quality explainable recommendation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16543v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16543v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond Tokens in Language Models: Interpreting Activations through Text Genre Chunks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Understanding Large Language Models (LLMs) is key to ensure their safe and beneficial deployment. This task is complicated by the difficulty of interpretability of LLM structures, and the inability to have all their outputs human-evaluated. In this paper, we present the first step towards a predictive framework, where the genre of a text used to prompt an LLM, is predicted based on its activations. Using Mistral-7B and two datasets, we show that genre can be extracted with F1-scores of up to 98% and 71% using scikit-learn classifiers. Across both datasets, results consistently outperform the control task, providing a proof of concept that text genres can be inferred from LLMs with shallow learning models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16540v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16540v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TurkColBERT: A Benchmark of Dense and Late-Interaction Models for Turkish Information Retrieval
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Neural information retrieval systems excel in high-resource languages but remain underexplored for morphologically rich, lower-resource languages such as Turkish. Dense bi-encoders currently dominate Turkish IR, yet late-interaction models -- which retain token-level representations for fine-grained matching -- have not been systematically evaluated. We introduce TurkColBERT, the first comprehensive benchmark comparing dense encoders and late-interaction models for Turkish retrieval. Our two-stage adaptation pipeline fine-tunes English and multilingual encoders on Turkish NLI/STS tasks, then converts them into ColBERT-style retrievers using PyLate trained on MS MARCO-TR. We evaluate 10 models across five Turkish BEIR datasets covering scientific, financial, and argumentative domains. Results show strong parameter efficiency: the 1.0M-parameter colbert-hash-nano-tr is 600$\times$ smaller than the 600M turkish-e5-large dense encoder while preserving over 71\% of its average mAP. Late-interaction models that are 3--5$\times$ smaller than dense encoders significantly outperform them; ColmmBERT-base-TR yields up to +13.8\% mAP on domain-specific tasks. For production-readiness, we compare indexing algorithms: MUVERA+Rerank is 3.33$\times$ faster than PLAID and offers +1.7\% relative mAP gain. This enables low-latency retrieval, with ColmmBERT-base-TR achieving 0.54 ms query times under MUVERA. We release all checkpoints, configs, and evaluation scripts. Limitations include reliance on moderately sized datasets ($\leq$50K documents) and translated benchmarks, which may not fully reflect real-world Turkish retrieval conditions; larger-scale MUVERA evaluations remain necessary.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16528v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16528v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Anatomy of an Idiom: Tracing Non-Compositionality in Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We investigate the processing of idiomatic expressions in transformer-based language models using a novel set of techniques for circuit discovery and analysis. First discovering circuits via a modified path patching algorithm, we find that idiom processing exhibits distinct computational patterns. We identify and investigate ``Idiom Heads,'' attention heads that frequently activate across different idioms, as well as enhanced attention between idiom tokens due to earlier processing, which we term ``augmented reception.'' We analyze these phenomena and the general features of the discovered circuits as mechanisms by which transformers balance computational efficiency and robustness. Finally, these findings provide insights into how transformers handle non-compositional language and suggest pathways for understanding the processing of more complex grammatical constructions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16467v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16467v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TOFA: Training-Free One-Shot Federated Adaptation for Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Efficient and lightweight adaptation of pre-trained Vision-Language Models (VLMs) to downstream tasks through collaborative interactions between local clients and a central server is a rapidly emerging research topic in federated learning. Existing adaptation algorithms are typically trained iteratively, which incur significant communication costs and increase the susceptibility to potential attacks. Motivated by the one-shot federated training techniques that reduce client-server exchanges to a single round, developing a lightweight one-shot federated VLM adaptation method to alleviate these issues is particularly attractive. However, current one-shot approaches face certain challenges in adapting VLMs within federated settings: (1) insufficient exploitation of the rich multimodal information inherent in VLMs; (2) lack of specialized adaptation strategies to systematically handle the severe data heterogeneity; and (3) requiring additional training resource of clients or server. To bridge these gaps, we propose a novel Training-free One-shot Federated Adaptation framework for VLMs, named TOFA. To fully leverage the generalizable multimodal features in pre-trained VLMs, TOFA employs both visual and textual pipelines to extract task-relevant representations. In the visual pipeline, a hierarchical Bayesian model learns personalized, class-specific prototype distributions. For the textual pipeline, TOFA evaluates and globally aligns the generated local text prompts for robustness. An adaptive weight calibration mechanism is also introduced to combine predictions from both modalities, balancing personalization and robustness to handle data heterogeneity. Our method is training-free, not relying on additional training resources on either the client or server side. Extensive experiments across 9 datasets in various federated settings demonstrate the effectiveness of the proposed TOFA method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16423v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16423v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Classification of worldwide news articles by perceived quality, 2018-2024
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This study explored whether supervised machine learning and deep learning models can effectively distinguish perceived lower-quality news articles from perceived higher-quality news articles. 3 machine learning classifiers and 3 deep learning models were assessed using a newly created dataset of 1,412,272 English news articles from the Common Crawl over 2018-2024. Expert consensus ratings on 579 source websites were split at the median, creating perceived low and high-quality classes of about 706,000 articles each, with 194 linguistic features per website-level labelled article. Traditional machine learning classifiers such as the Random Forest demonstrated capable performance (0.7355 accuracy, 0.8131 ROC AUC). For deep learning, ModernBERT-large (256 context length) achieved the best performance (0.8744 accuracy; 0.9593 ROC-AUC; 0.8739 F1), followed by DistilBERT-base (512 context length) at 0.8685 accuracy and 0.9554 ROC-AUC. DistilBERT-base (256 context length) reached 0.8478 accuracy and 0.9407 ROC-AUC, while ModernBERT-base (256 context length) attained 0.8569 accuracy and 0.9470 ROC-AUC. These results suggest that the perceived quality of worldwide news articles can be effectively differentiated by traditional CPU-based machine learning classifiers and deep learning classifiers.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16416v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16416v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning from Sufficient Rationales: Analysing the Relationship Between Explanation Faithfulness and Token-level Regularisation Strategies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Human explanations of natural language, rationales, form a tool to assess whether models learn a label for the right reasons or rely on dataset-specific shortcuts. Sufficiency is a common metric for estimating the informativeness of rationales, but it provides limited insight into the effects of rationale information on model performance. We address this limitation by relating sufficiency to two modelling paradigms: the ability of models to identify which tokens are part of the rationale (through token classification) and the ability of improving model performance by incorporating rationales in the input (through attention regularisation). We find that highly informative rationales are not likely to help classify the instance correctly. Sufficiency conversely captures the classification impact of the non-rationalised context, which interferes with rationale information in the same input. We also find that incorporating rationale information in model inputs can boost cross-domain classification, but results are inconsistent per task and model type. Finally, sufficiency and token classification appear to be unrelated. These results exemplify the complexity of rationales, showing that metrics capable of systematically capturing this type of information merit further investigation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16353v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16353v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        OpenMMReasoner: Pushing the Frontiers for Multimodal Reasoning with an Open and General Recipe
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large reasoning models have fueled growing interest in extending such capabilities to multimodal domains. However, despite notable progress in visual reasoning, the lack of transparent and reproducible data curation and training strategies remains a major barrier to scalable research. In this work, we introduce OpenMMReasoner, a fully transparent two-stage recipe for multimodal reasoning spanning supervised fine-tuning (SFT) and reinforcement learning (RL). In the SFT stage, we construct an 874K-sample cold-start dataset with rigorous step-by-step validation, providing a strong foundation for reasoning capabilities. The subsequent RL stage leverages a 74K-sample dataset across diverse domains to further sharpen and stabilize these abilities, resulting in a more robust and efficient learning process. Extensive evaluations demonstrate that our training recipe not only surpasses strong baselines but also highlights the critical role of data quality and training design in shaping multimodal reasoning performance. Notably, our method achieves a 11.6% improvement over the Qwen2.5-VL-7B-Instruct baseline across nine multimodal reasoning benchmarks, establishing a solid empirical foundation for future large-scale multimodal reasoning research. We open-sourced all our codes, pipeline, and data at https://github.com/EvolvingLMMs-Lab/OpenMMReasoner.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16334v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16334v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Incorporating Self-Rewriting into Large Language Model Reasoning Reinforcement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Through reinforcement learning (RL) with outcome correctness rewards, large reasoning models (LRMs) with scaled inference computation have demonstrated substantial success on complex reasoning tasks. However, the one-sided reward, focused solely on final correctness, limits its ability to provide detailed supervision over internal reasoning process. This deficiency leads to suboptimal internal reasoning quality, manifesting as issues like over-thinking, under-thinking, redundant-thinking, and disordered-thinking. Inspired by the recent progress in LRM self-rewarding, we introduce self-rewriting framework, where a model rewrites its own reasoning texts, and subsequently learns from the rewritten reasoning to improve the internal thought process quality. For algorithm design, we propose a selective rewriting approach wherein only "simple" samples, defined by the model's consistent correctness, are rewritten, thereby preserving all original reward signals of GRPO. For practical implementation, we compile rewriting and vanilla generation within one single batch, maintaining the scalability of the RL algorithm and introducing only ~10% overhead. Extensive experiments on diverse tasks with different model sizes validate the effectiveness of self-rewriting. In terms of the accuracy-length tradeoff, the self-rewriting approach achieves improved accuracy (+0.6) with substantially shorter reasoning (-46%) even without explicit instructions in rewriting prompts to reduce reasoning length, outperforming existing strong baselines. In terms of internal reasoning quality, self-rewriting achieves significantly higher scores (+7.2) under the LLM-as-a-judge metric, successfully mitigating internal reasoning flaws.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16331v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16331v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SDA: Steering-Driven Distribution Alignment for Open LLMs without Fine-Tuning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the rapid advancement of large language models (LLMs), their deployment in real-world applications has become increasingly widespread. LLMs are expected to deliver robust performance across diverse tasks, user preferences, and practical scenarios. However, as demands grow, ensuring that LLMs produce responses aligned with human intent remains a foundational challenge. In particular, aligning model behavior effectively and efficiently during inference, without costly retraining or extensive supervision, is both a critical requirement and a non-trivial technical endeavor. To address the challenge, we propose SDA (Steering-Driven Distribution Alignment), a training-free and model-agnostic alignment framework designed for open-source LLMs. SDA dynamically redistributes model output probabilities based on user-defined alignment instructions, enhancing alignment between model behavior and human intents without fine-tuning. The method is lightweight, resource-efficient, and compatible with a wide range of open-source LLMs. It can function independently during inference or be integrated with training-based alignment strategies. Moreover, SDA supports personalized preference alignment, enabling flexible control over the model response behavior. Empirical results demonstrate that SDA consistently improves alignment performance across 8 open-source LLMs with varying scales and diverse origins, evaluated on three key alignment dimensions, helpfulness, harmlessness, and honesty (3H). Specifically, SDA achieves average gains of 64.4% in helpfulness, 30% in honesty and 11.5% in harmlessness across the tested models, indicating its effectiveness and generalization across diverse models and application scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16324v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16324v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeSE: A Structural Information-Guided Uncertainty Quantification Framework for Hallucination Detection in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable uncertainty quantification (UQ) is essential for deploying large language models (LLMs) in safety-critical scenarios, as it enables them to abstain from responding when uncertain, thereby avoiding hallucinating falsehoods. However, state-of-the-art UQ methods primarily rely on semantic probability distributions or pairwise distances, overlooking latent semantic structural information that could enable more precise uncertainty estimates. This paper presents Semantic Structural Entropy (SeSE), a principled UQ framework that quantifies the inherent semantic uncertainty of LLMs from a structural information perspective for hallucination detection. Specifically, to effectively model semantic spaces, we first develop an adaptively sparsified directed semantic graph construction algorithm that captures directional semantic dependencies while automatically pruning unnecessary connections that introduce negative interference. We then exploit latent semantic structural information through hierarchical abstraction: SeSE is defined as the structural entropy of the optimal semantic encoding tree, formalizing intrinsic uncertainty within semantic spaces after optimal compression. A higher SeSE value corresponds to greater uncertainty, indicating that LLMs are highly likely to generate hallucinations. In addition, to enhance fine-grained UQ in long-form generation -- where existing methods often rely on heuristic sample-and-count techniques -- we extend SeSE to quantify the uncertainty of individual claims by modeling their random semantic interactions, providing theoretically explicable hallucination detection. Extensive experiments across 29 model-dataset combinations show that SeSE significantly outperforms advanced UQ baselines, including strong supervised methods and the recently proposed KLE.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16275v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16275v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TS-PEFT: Token-Selective Parameter-Efficient Fine-Tuning with Learnable Threshold Gating
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In the field of large models (LMs) for natural language processing (NLP) and computer vision (CV), Parameter-Efficient Fine-Tuning (PEFT) has emerged as a resource-efficient method that modifies a limited number of parameters while keeping the pretrained weights fixed. This paper investigates the traditional PEFT approach, which applies modifications to all position indices, and questions its necessity. We introduce a new paradigm called Token-Selective PEFT (TS-PEFT), in which a function S selectively applies PEFT modifications to a subset of position indices, potentially enhancing performance on downstream tasks. Our experimental results reveal that the indiscriminate application of PEFT to all indices is not only superfluous, but may also be counterproductive. This study offers a fresh perspective on PEFT, advocating for a more targeted approach to modifications and providing a framework for future research to optimize the fine-tuning process for large models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16147v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16147v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ELPO: Ensemble Learning Based Prompt Optimization for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The remarkable performance of Large Language Models (LLMs) highly relies on crafted prompts. However, manual prompt engineering is a laborious process, creating a core bottleneck for practical application of LLMs. This phenomenon has led to the emergence of a new research area known as Automatic Prompt Optimization (APO), which develops rapidly in recent years. Existing APO methods such as those based on evolutionary algorithms or trial-and-error approaches realize an efficient and accurate prompt optimization to some extent. However, those researches focus on a single model or algorithm for the generation strategy and optimization process, which limits their performance when handling complex tasks. To address this, we propose a novel framework called Ensemble Learning based Prompt Optimization (ELPO) to achieve more accurate and robust results. Motivated by the idea of ensemble learning, ELPO conducts voting mechanism and introduces shared generation strategies along with different search methods for searching superior prompts. Moreover, ELPO creatively presents more efficient algorithms for the prompt generation and search process. Experimental results demonstrate that ELPO outperforms state-of-the-art prompt optimization methods across different tasks, e.g., improving F1 score by 7.6 on ArSarcasm dataset.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16122v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16122v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Early science acceleration experiments with GPT-5
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    AI models like GPT-5 are an increasingly valuable tool for scientists, but many remain unaware of the capabilities of frontier AI. We present a collection of short case studies in which GPT-5 produced new, concrete steps in ongoing research across mathematics, physics, astronomy, computer science, biology, and materials science. In these examples, the authors highlight how AI accelerated their work, and where it fell short; where expert time was saved, and where human input was still key. We document the interactions of the human authors with GPT-5, as guiding examples of fruitful collaboration with AI. Of note, this paper includes four new results in mathematics (carefully verified by the human authors), underscoring how GPT-5 can help human mathematicians settle previously unsolved problems. These contributions are modest in scope but profound in implication, given the rate at which frontier AI is progressing.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16072v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16072v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning Tractable Distributions Of Language Model Continuations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Controlled language generation conditions text on sequence-level constraints (for example, syntax, style, or safety). These constraints may depend on future tokens, which makes directly conditioning an autoregressive language model (LM) generally intractable. Prior work uses tractable surrogates such as hidden Markov models (HMMs) to approximate the distribution over continuations and adjust the model's next-token logits at decoding time. However, we find that these surrogates are often weakly context aware, which reduces query quality. We propose Learning to Look Ahead (LTLA), a hybrid approach that pairs the same base language model for rich prefix encoding with a fixed tractable surrogate model that computes exact continuation probabilities. Two efficiency pitfalls arise when adding neural context: (i) naively rescoring the prefix with every candidate next token requires a sweep over the entire vocabulary at each step, and (ii) predicting fresh surrogate parameters for each prefix, although tractable at a single step, forces recomputation of future probabilities for every new prefix and eliminates reuse. LTLA avoids both by using a single batched HMM update to account for all next-token candidates at once, and by conditioning only the surrogate's latent state prior on the LM's hidden representations while keeping the surrogate decoder fixed, so computations can be reused across prefixes. Empirically, LTLA attains higher conditional likelihood than an unconditional HMM, approximates continuation distributions for vision-language models where a standalone HMM cannot encode visual context, and improves constraint satisfaction at comparable fluency on controlled-generation tasks, with minimal inference overhead.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16054v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16054v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Liars' Bench: Evaluating Lie Detectors for Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Prior work has introduced techniques for detecting when large language models (LLMs) lie, that is, generating statements they believe are false. However, these techniques are typically validated in narrow settings that do not capture the diverse lies LLMs can generate. We introduce LIARS' BENCH, a testbed consisting of 72,863 examples of lies and honest responses generated by four open-weight models across seven datasets. Our settings capture qualitatively different types of lies and vary along two dimensions: the model's reason for lying and the object of belief targeted by the lie. Evaluating three black- and white-box lie detection techniques on LIARS' BENCH, we find that existing techniques systematically fail to identify certain types of lies, especially in settings where it's not possible to determine whether the model lied from the transcript alone. Overall, LIARS' BENCH reveals limitations in prior techniques and provides a practical testbed for guiding progress in lie detection.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16035v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16035v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SpellForger: Prompting Custom Spell Properties In-Game using BERT supervised-trained model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Introduction: The application of Artificial Intelligence in games has evolved significantly, allowing for dynamic content generation. However, its use as a core gameplay co-creation tool remains underexplored. Objective: This paper proposes SpellForger, a game where players create custom spells by writing natural language prompts, aiming to provide a unique experience of personalization and creativity. Methodology: The system uses a supervisedtrained BERT model to interpret player prompts. This model maps textual descriptions to one of many spell prefabs and balances their parameters (damage, cost, effects) to ensure competitive integrity. The game is developed in the Unity Game Engine, and the AI backend is in Python. Expected Results: We expect to deliver a functional prototype that demonstrates the generation of spells in real time, applied to an engaging gameplay loop, where player creativity is central to the experience, validating the use of AI as a direct gameplay mechanic.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16018v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16018v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CARE-RAG - Clinical Assessment and Reasoning in RAG
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Access to the right evidence does not guarantee that large language models (LLMs) will reason with it correctly. This gap between retrieval and reasoning is especially concerning in clinical settings, where outputs must align with structured protocols. We study this gap using Written Exposure Therapy (WET) guidelines as a testbed. In evaluating model responses to curated clinician-vetted questions, we find that errors persist even when authoritative passages are provided. To address this, we propose an evaluation framework that measures accuracy, consistency, and fidelity of reasoning. Our results highlight both the potential and the risks: retrieval-augmented generation (RAG) can constrain outputs, but safe deployment requires assessing reasoning as rigorously as retrieval.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15994v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15994v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        JudgeBoard: Benchmarking and Enhancing Small Language Models for Reasoning Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While small language models (SLMs) have shown promise on various reasoning tasks, their ability to judge the correctness of answers remains unclear compared to large language models (LLMs). Prior work on LLM-as-a-judge frameworks typically relies on comparing candidate answers against ground-truth labels or other candidate answers using predefined metrics like entailment. However, this approach is inherently indirect and difficult to fully automate, offering limited support for fine-grained and scalable evaluation of reasoning outputs. In this work, we propose JudgeBoard, a novel evaluation pipeline that directly queries models to assess the correctness of candidate answers without requiring extra answer comparisons. We focus on two core reasoning domains: mathematical reasoning and science/commonsense reasoning, and construct task-specific evaluation leaderboards using both accuracy-based ranking and an Elo-based rating system across five benchmark datasets, enabling consistent model comparison as judges rather than comparators. To improve judgment performance in lightweight models, we propose MAJ (Multi-Agent Judging), a novel multi-agent evaluation framework that leverages multiple interacting SLMs with distinct reasoning profiles to approximate LLM-level judgment accuracy through collaborative deliberation. Experimental results reveal a significant performance gap between SLMs and LLMs in isolated judging tasks. However, our MAJ framework substantially improves the reliability and consistency of SLMs. On the MATH dataset, MAJ using smaller-sized models as backbones performs comparatively well or even better than their larger-sized counterparts. Our findings highlight that multi-agent SLM systems can potentially match or exceed LLM performance in judgment tasks, with implications for scalable and efficient assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_15958v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.15958v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-8" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Vision
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 3 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前视觉技术研究的主要动态集中在多模态模型的应用与优化上。MiMo-Embodied展示了跨领域模型在自主驾驶和具身智能中的卓越表现，体现了多任务学习的潜力；而Arctic-Extract则关注于高效的数据提取，适应资源受限环境，显示出实用性和可部署性。与此同时，针对多方社交互动中的欺骗识别，新的基准测试揭示了现有模型在社会智能方面的不足，强调了未来研究在理解人类交互中的重要性。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiMo-Embodied: X-Embodied Foundation Model Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We open-source MiMo-Embodied, the first cross-embodied foundation model to successfully integrate and achieve state-of-the-art performance in both Autonomous Driving and Embodied AI. MiMo-Embodied sets new records across 17 embodied AI benchmarks in Task Planning, Affordance Prediction and Spatial Understanding, while also excelling in 12 autonomous driving benchmarks across Environmental Perception, Status Prediction, and Driving Planning. Across these tasks, MiMo-Embodied significantly outperforms existing open-source, closed-source, and specialized baselines. Our results indicate that through multi-stage learning, curated data construction, and CoT/RL fine-tuning, these two domains exhibit strong positive transfer and mutually reinforce one another. We provide a detailed analysis of our model design and training methodologies to facilitate further research. Code and models are available at https://github.com/XiaomiMiMo/MiMo-Embodied.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16518v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16518v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Arctic-Extract Technical Report
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Arctic-Extract is a state-of-the-art model designed for extracting structural data (question answering, entities and tables) from scanned or digital-born business documents. Despite its SoTA capabilities, the model is deployable on resource-constrained hardware, weighting only 6.6 GiB, making it suitable for deployment on devices with limited resources, such as A10 GPUs with 24 GB of memory. Arctic-Extract can process up to 125 A4 pages on those GPUs, making suitable for long document processing. This paper highlights Arctic-Extract's training protocols and evaluation results, demonstrating its strong performance in document understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16470v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16470v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Can MLLMs Read the Room? A Multimodal Benchmark for Assessing Deception in Multi-Party Social Interactions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite their advanced reasoning capabilities, state-of-the-art Multimodal Large Language Models (MLLMs) demonstrably lack a core component of human intelligence: the ability to `read the room' and assess deception in complex social interactions. To rigorously quantify this failure, we introduce a new task, Multimodal Interactive Deception Assessment (MIDA), and present a novel multimodal dataset providing synchronized video and text with verifiable ground-truth labels for every statement. We establish a comprehensive benchmark evaluating 12 state-of-the-art open- and closed-source MLLMs, revealing a significant performance gap: even powerful models like GPT-4o struggle to distinguish truth from falsehood reliably. Our analysis of failure modes indicates that these models fail to effectively ground language in multimodal social cues and lack the ability to model what others know, believe, or intend, highlighting the urgent need for novel approaches to building more perceptive and trustworthy AI systems. To take a step forward, we design a Social Chain-of-Thought (SoCoT) reasoning pipeline and a Dynamic Social Epistemic Memory (DSEM) module. Our framework yields performance improvement on this challenging task, demonstrating a promising new path toward building MLLMs capable of genuine human-like social reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-21/2511_16221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.16221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
        </div>
        
        

        <div class="footer">
            <p>生成时间: 2025-11-21 19:12:44</p>
            <p>访问地址: <a href="https://">https://</a></p>
        </div>
    </div>
</body>
</html>