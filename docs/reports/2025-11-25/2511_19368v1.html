<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</title>
    <style>
        :root {
            /* 配色方案：Slate + Indigo */
            --primary-color: #4f46e5;
            --bg-body: #f8fafc;
            --bg-paper: #ffffff;
            --text-main: #1e293b;      /* Slate 800 */
            --text-body: #334155;      /* Slate 700 - 正文颜色略浅，减少视觉疲劳 */
            --text-secondary: #64748b; /* Slate 500 */
            --border-color: #e2e8f0;
            --code-bg: #f1f5f9;
            
            /* 警告色 */
            --warn-bg: #fff7ed;
            --warn-text: #9a3412;
            --warn-border: #fdba74;

            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-body);
            line-height: 1.8; /* 增加行高，适合阅读 */
            padding: 40px 20px;
            min-height: 100vh;
        }

        /* 阅读容器：限制宽度以提升阅读体验 */
        .container {
            max-width: 800px;
            margin: 0 auto;
            background-color: var(--bg-paper);
            border-radius: 16px; /* 更圆润的角 */
            padding: 40px 60px; /* 宽敞的内边距 */
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03);
        }

        /* 顶部导航 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            font-size: 14px;
        }

        .nav-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            transition: color 0.2s;
        }

        .nav-link:hover { color: var(--primary-color); }
        .nav-link::before { content: "←"; margin-right: 5px; }
        
        .arxiv-link {
            background-color: #f1f5f9;
            color: var(--text-main);
            padding: 6px 12px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
        }
        
        .arxiv-link:hover {
            background-color: #e2e8f0;
            color: var(--primary-color);
        }

        /* 论文头部信息 */
        .paper-header {
            margin-bottom: 40px;
        }

        .paper-title {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        /* 标签组 */
        .tags-wrapper {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .tag {
            background-color: #e0e7ff; /* Indigo 100 */
            color: #4338ca;            /* Indigo 700 */
            font-size: 12px;
            padding: 4px 10px;
            border-radius: 99px;
            font-weight: 500;
        }

        /* 元数据栏 */
        .metadata-box {
            background-color: #f8fafc;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            font-size: 14px;
            color: var(--text-secondary);
        }

        .meta-item {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .meta-label {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #94a3b8;
        }

        .meta-value {
            font-weight: 600;
            color: var(--text-main);
        }
        
        .score-badge {
            color: var(--primary-color);
        }

        /* 核心图片展示 */
        .core-image-container {
            margin: 40px 0;
            text-align: center;
            background-color: #f8fafc;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .core-image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        
        .image-caption {
            margin-top: 10px;
            font-size: 13px;
            color: var(--text-secondary);
            font-style: italic;
        }

        /* 警告框 */
        .warning-box {
            background-color: var(--warn-bg);
            border-left: 4px solid var(--warn-border);
            color: var(--warn-text);
            padding: 15px;
            border-radius: 0 6px 6px 0;
            margin: 20px 0;
            font-size: 14px;
        }

        /* 章节标题 */
        .section-header {
            display: flex;
            align-items: center;
            margin-top: 50px;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px dashed var(--border-color);
        }

        .section-header h2 {
            font-size: 24px;
            font-weight: 700;
            color: var(--text-main);
            margin: 0;
            position: relative;
        }
        
        /* 章节前的装饰点 */
        .section-header h2::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: var(--primary-color);
            border-radius: 50%;
            margin-right: 12px;
            vertical-align: middle;
        }

        /* Markdown 内容样式重置 - 极简学术风 */
        .content-body {
            font-size: 17px; /* 略大的字号适合阅读 */
            color: var(--text-body);
        }

        .content-body p {
            margin-bottom: 1.5em;
            text-align: justify;
        }

        .content-body h3 {
            font-size: 20px;
            font-weight: 600;
            color: var(--text-main);
            margin-top: 2em;
            margin-bottom: 1em;
        }
        
        .content-body h4 {
            font-size: 18px;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }

        .content-body ul, .content-body ol {
            margin-bottom: 1.5em;
            padding-left: 1.5em;
        }

        .content-body li {
            margin-bottom: 0.5em;
        }

        .content-body strong {
            color: var(--text-main);
            font-weight: 600;
        }
        
        /* 引用块 - 学术风 */
        .content-body blockquote {
            border-left: 4px solid var(--primary-color);
            background-color: #f8fafc;
            padding: 16px 20px;
            margin: 20px 0;
            font-style: italic;
            color: var(--text-secondary);
            border-radius: 0 8px 8px 0;
        }

        /* 代码块 */
        .content-body pre {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid var(--border-color);
        }

        .content-body code {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
            color: #d63384; /* 类似 GitHub 的代码红 */
        }
        
        .content-body pre code {
            color: inherit;
            padding: 0;
            background-color: transparent;
        }

        /* Footer */
        .footer {
            margin-top: 80px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        /* 移动端适配 */
        @media (max-width: 768px) {
            body { padding: 0; }
            
            .container {
                border-radius: 0;
                padding: 30px 20px;
                box-shadow: none;
            }

            .paper-title { font-size: 26px; }
            
            .metadata-box {
                flex-direction: column;
                gap: 15px;
            }
            
            .content-body { font-size: 16px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="index.html" class="nav-link">返回今日简报</a>
            <a href="http://arxiv.org/abs/2511.19368v1" target="_blank" class="arxiv-link">PDF / arXiv ↗</a>
        </div>

        <div class="paper-header">
            <h1 class="paper-title">LLM-Driven Stationarity-Aware Expert Demonstrations for Multi-Agent Reinforcement Learning in Mobile Systems</h1>
            
            
            <div class="tags-wrapper">
                
                <span class="tag">多智能体强化学习</span>
                
                <span class="tag">非平稳性</span>
                
                <span class="tag">大型语言模型</span>
                
                <span class="tag">专家演示</span>
                
                <span class="tag">策略优化</span>
                
            </div>
            

            <div class="metadata-box">
                
                <div class="meta-item" style="flex: 2; min-width: 200px;">
                    <span class="meta-label">作者单位</span>
                    <span class="meta-value">The University of Hong Kong, Peking University, City University of Hong Kong, Xiamen University of Technology, Sichuan University, Nanyang Technological University, Fudan University</span>
                </div>
                
                
                <div class="meta-item">
                    <span class="meta-label">推荐指数</span>
                    <span class="meta-value score-badge">0.346</span>
                </div>
                
                <div class="meta-item">
                    <span class="meta-label">arXiv ID</span>
                    <span class="meta-value">2511.19368v1</span>
                </div>
            </div>

            
        </div>

        
        <div class="core-image-container">
            
            <img src="../../images/2025-11-25/b84f751a9ab7427b4602d65dae821285d58b297f6a1fa6c2888e909459bef1f2.jpg" alt="核心思路示意图" />
            <div class="image-caption">图 1：论文核心方法/架构示意图</div>
        </div>
        

        <div class="section-header">
            <h2>快速简介</h2>
        </div>
        <div class="content-body">
            <p>本文提出了RELED框架，旨在解决多智能体强化学习中的非平稳性问题。通过结合大型语言模型（LLM）驱动的专家演示与自主探索，RELED利用Stationarity-Aware Expert Demonstration模块生成高质量轨迹，并通过Hybrid Expert-Agent Policy Optimization模块自适应平衡学习，显著加速策略收敛和提高性能。实验结果表明，RELED在复杂城市交通管理任务中优于现有MARL方法。</p>
        </div>

        <div class="section-header">
            <h2>深度解读</h2>
        </div>
        <div class="content-body">
            
                <h3>现有问题</h3>

<p>本文旨在解决多智能体强化学习（MARL）中一个长期存在的关键挑战：<strong>环境的非平稳性</strong>。该问题源于多个智能体同时更新其策略，导致每个智能体所感知的环境动态变化，从而引发训练不稳定、策略收敛困难、样本和时间效率低下等问题。随着MARL在城市交通管理等复杂现实世界场景中的应用日益增多，解决这一问题变得至关重要。现有方法在有效结合专家演示与智能体自主探索以应对此挑战方面存在不足。</p>

<h3>Hypothesis</h3>

<p>本文的核心假设是，通过将<strong>大型语言模型（LLM）驱动的专家演示</strong>与<strong>智能体的自主探索</strong>相结合，可以有效缓解MARL中的非平稳性问题。具体假设如下：
- <strong>关键发现</strong>: 提出的<strong>RELED框架</strong>能够通过理论指导和环境反馈，让LLM生成高质量、稳定的专家演示，并自适应地平衡模仿学习与自主探索。
- <strong>核心假设</strong>: 这种结合能够显著加速策略收敛，提升样本和时间效率，并增强智能体在复杂动态环境中的最终性能和泛化能力。</p>

<h3>相关研究</h3>

<ul>
<li><strong>多智能体强化学习（MARL）方法</strong>: 包括价值基础方法（如QMIX）和策略基础方法（如IPPO, MAPPO）。</li>
<li><strong>专家演示与模仿学习</strong>: 在强化学习中利用专家知识来指导策略学习。</li>
<li><strong>大语言模型（LLM）在决策中的应用</strong>: 利用LLM的推理能力来生成决策或优化策略。</li>
</ul>

<h3>解决方案</h3>

<h3>完整的详细解决方案：RELED框架</h3>

<p>本文提出的核心解决方案是一个名为<strong>RELED (Reinforcement Learning with Expert Demonstration)</strong> 的可扩展多智能体强化学习（MARL）框架。该框架旨在解决MARL中的核心挑战——<strong>非平稳性（non-stationarity）</strong>问题，即由于多个智能体同时学习和更新策略，导致每个智能体所感知的环境动态变化，从而影响训练的稳定性和收敛性。RELED通过创新地整合基于大型语言模型（LLM）的专家演示与智能体的自主探索，显著提升了学习效率、策略性能和泛化能力。</p>

<p>RELED框架主要由两个相互协作的核心模块组成：</p>

<ol>
<li><strong>Stationarity-Aware Expert Demonstration (SED) 模块</strong>：利用LLM生成高质量的专家演示，并主动量化和约束环境的非平稳性。</li>
<li><strong>Hybrid Expert-Agent Policy Optimization (HPO) 模块</strong>：通过一个动态混合的策略优化机制，有效结合专家指导和智能体的自主探索经验。</li>
</ol>

<hr />

<h4>1. Stationarity-Aware Expert Demonstration (SED) 模块：理论驱动的专家指导</h4>

<p>SED模块的目标是生成高质量的专家演示，同时缓解因多智能体策略更新所带来的环境不稳定性。</p>

<h5><strong>理论基础与非平稳性量化</strong></h5>

<p>SED模块的設計基于对MARL非平稳性的深刻理论分析。论文首先推导出了一个性能界限，用以量化当一个智能体子集的策略发生变化时，对其他外部智能体造成的影响。这个界限由两个关键的可计算指标构成：</p>

<ul>
<li><strong>奖励波动指数 (Reward Volatility Index, RVI)</strong>：量化外部智能体在奖励上的最大波动。它识别出那些因专家策略介入而导致当前智能体策略奖励显著下降的状态和指令。</li>
<li><strong>策略散度指数 (Policy Divergence Index, PDI)</strong>：通过计算专家策略与当前智能体策略之间的Kullback-Leibler (KL)散度，量化策略分布的偏差。它帮助识别两种策略在决策上的不一致之处。</li>
</ul>

<p>这两个指标共同构成了对非平稳性的量化度量，为优化专家演示提供了理论依据。</p>

<h5><strong>基于LLM的反馈式演示生成流程</strong></h5>

<p>SED模块利用LLM生成专家演示，其过程是一个迭代优化的闭环：</p>

<ol>
<li><strong>初始指令生成</strong>：首先，向LLM提供一个详细的<strong>初始提示 (Initial Prompt)</strong>，包含环境信息（如城市网络拓扑）、智能体信息（如起点-终点对）以及可用的导航函数接口。LLM基于此生成初步的指令序列作为专家演示。</li>
<li><strong>演示执行与评估</strong>：一组智能体根据LLM生成的指令与环境交互，收集专家轨迹 <code>τ^e</code>。同时，SED模块计算上述的RVI和PDI指标，评估这些演示对系统稳定性的影响。</li>
<li><strong>反馈与迭代优化</strong>：将RVI和PDI的评估结果，连同专家轨迹的观察信息，整合成一个<strong>反馈提示 (Feedback Prompt)</strong>。该提示被送回LLM，指示其修正那些导致奖励波动过大或策略散度过高的指令。</li>
<li><strong>周期性更新</strong>：为了平衡计算开销和演示质量，此反馈优化过程并非每个周期都执行，而是以一个采样间隔 <code>q</code>（例如每10个训练周期）进行一次，确保智能体有足够的时间利用当前的专家演示。</li>
</ol>

<p>通过这种方式，SED模块不仅利用LLM生成了高奖励的轨迹，更重要的是，它确保了这些演示能够增强环境的稳定性，从而为所有智能体的学习创造一个更有利的环境。</p>

<hr />

<h4>2. Hybrid Expert-Agent Policy Optimization (HPO) 模块：从模仿到探索的自适应学习</h4>

<p>单纯依赖专家演示（模仿学习）可能导致样本多样性不足和泛化能力差。HPO模块旨在解决这一问题，它是一个完全去中心化的训练框架，允许每个智能体独立地结合专家演示和自身探索的经验来优化策略。</p>

<h5><strong>混合策略损失函数</strong></h5>

<p>HPO模块的核心是为每个智能体 <code>i</code> 定义一个动态混合的策略损失函数。</p>

<ol>
<li><strong>价值与优势函数</strong>：首先，分别为专家轨迹 <code>τ^e</code> 和智能体自主探索的轨迹 <code>τ^a</code> 定义各自的价值函数 (<code>V^e_i</code>, <code>V^a_i</code>) 和优势函数 (<code>A^e_i</code>, <code>A^a_i</code>)。这使得可以独立评估两种经验来源的质量。</li>
<li><strong>策略损失计算</strong>：基于优势函数，使用裁剪的替代目标（类似于PPO）分别计算专家策略损失 <code>L_e(π_i)</code> 和智能体策略损失 <code>L_a(π_i)</code>。</li>
<li><strong>动态混合损失</strong>：最终的混合损失函数 <code>L_mix</code> 是这两者的加权和：
$$ L<em>{mix}(\pi</em>i) = \alpha L<em>a(\pi</em>i) + (1 - \alpha) L<em>e(\pi</em>i) $$</li>
<li><strong>最大熵正则化</strong>：为了鼓励智能体进行更广泛的探索，最终的策略损失函数还加入了一个最大熵正则化项：
$$ L(\pi<em>i) = L</em>{mix}(\pi<em>i) + \beta H(\pi</em>i) $$</li>
</ol>

<h5><strong>动态权重 α：实现从模仿到探索的平滑过渡</strong></h5>

<p>权重 <code>α</code> 是HPO模块自适应能力的关键。它并非一个固定值，而是根据智能体当前策略与专家演示的相似度动态调整的。</p>

<ul>
<li><strong>相似度度量</strong>：使用<strong>动态时间规整 (Dynamic Time Warping, DTW)</strong> 距离来衡量智能体轨迹与专家轨迹的相似性。</li>
<li><strong>权重调整机制</strong>：
<ul>
<li>在训练初期，智能体策略与专家演示差异较大（DTW距离大），此时 <code>α</code> 值较低。这使得优化过程更侧重于专家损失 <code>L_e</code>，引导智能体快速模仿专家的有效行为。</li>
<li>随着训练的进行，智能体策略逐渐向专家靠拢（DTW距离减小），<code>α</code> 值随之增大。优化重心逐渐转移到智能体自身的探索损失 <code>L_a</code> 上，鼓励其在专家策略的基础上进行自主优化和探索。</li>
</ul></li>
</ul>

<p>这种机制实现了从模仿学习到自主强化学习的平滑过渡，兼顾了学习初期的效率和后期的性能上限。</p>

<hr />

<h4>完整的训练流程与应用</h4>

<p>RELED的整体训练过程是一个循环迭代的算法：</p>

<ol>
<li><strong>初始化</strong>：初始化所有智能体的策略网络和价值函数网络，并加载预训练的LLM。</li>
<li><strong>轨迹收集</strong>：在每个训练周期，所有智能体根据当前策略与环境交互，收集自主探索轨迹 <code>τ_a</code>。</li>
<li><strong>专家演示生成与更新（周期性）</strong>：每隔 <code>q</code> 个周期，SED模块被激活。它利用最新的环境反馈（RVI和PDI）来查询LLM，生成或优化专家演示轨迹 <code>τ_e</code>。</li>
<li><strong>策略优化</strong>：在HPO模块中，每个智能体利用收集到的 <code>τ_a</code> 和最新的 <code>τ_e</code>，计算动态权重 <code>α</code>，并根据混合损失函数 <code>L(π_i)</code> 更新其策略和价值网络。</li>
<li><strong>循环</strong>：重复步骤2至4，直到策略收敛。</li>
</ol>

<h5><strong>实验验证与优势</strong></h5>

<p>该框架在基于<strong>OpenStreetMap</strong>的真实城市交通网络（如奥兰多和香港）上进行了广泛的模拟实验。结果表明：</p>

<ul>
<li><strong>卓越的性能</strong>：RELED在平均奖励和旅行时间等关键指标上，显著优于现有的先进MARL基线方法（如IPPO, MAPPO, QMIX）。</li>
<li><strong>更高的样本和时间效率</strong>：通过专家指导，RELED能够更快地收敛到高性能策略，在更少的训练步骤和更短的时间内达到优越表现。</li>
<li><strong>强大的泛化与可扩展性</strong>：混合优化机制增强了智能体在未见状态下的决策能力。同时，去中心化的设计使其能够平稳地扩展到更多智能体的场景中。</li>
</ul>

<h3>结论</h3>

<p>RELED框架通过其两大核心模块——<strong>SED</strong>和<strong>HPO</strong>——的协同工作，为解决MARL中的非平稳性问题提供了一个强大而新颖的解决方案。它巧妙地将LLM的规划能力与深度强化学习的决策能力相结合，通过理论驱动的反馈机制稳定训练环境，并通过自适应的混合优化策略平衡模仿与探索。这不仅显著提升了多智能体系统的训练效率和最终性能，也为该技术在资源受限的边缘设备和复杂的现实世界应用（如城市交通管理、机器人协作）中落地提供了坚实的基础。</p>

<h3>实验设计</h3>

<ul>
<li><strong>环境</strong>: 实验在基于真实世界城市网络（如奥兰多、香港）的交通模拟环境（使用<strong>OpenStreetMap</strong>数据和<strong>SUMO</strong>模拟器）中进行。</li>
<li><strong>任务</strong>: 多智能体导航和交通管理任务，涵盖不同的交通流量条件（如中等、拥堵）。</li>
<li><strong>基线对比</strong>: 将RELED与多个先进的MARL基线方法（如<strong>IPPO, MAPPO, QMIX</strong>）进行性能比较。</li>
<li><strong>消融研究</strong>: 通过对比完整RELED框架与使用固定权重的变体，验证自适应平衡机制的有效性。</li>
<li><strong>评估指标</strong>: 主要评估指标包括样本效率、时间效率、平均回报和平均旅行时间。</li>
</ul>

<h3>数据集和代码</h3>

<p>实验使用了<strong>OpenStreetMap</strong>的城市网络数据和<strong>SUMO</strong>交通模拟器。然而，在提供的所有片段中，均<strong>未提供</strong>具体的代码库或处理后数据集的公开链接。</p>

<h3>实验结果</h3>

<p>实验结果有力地支持了论文的假设：
- <strong>性能优势</strong>: RELED在所有测试的交通条件和城市网络中，其样本效率、时间效率和最终性能（更高的平均回报、更短的旅行时间）均<strong>持续优于</strong>所有基线方法。
- <strong>收敛速度</strong>: RELED能用更少的训练步骤达到更高的性能水平，展示了其加速策略收敛的能力。
- <strong>模块有效性</strong>: 消融实验证明，自适应调整专家与智能体经验权重的HPO模块至关重要，固定权重的版本性能显著下降。
- <strong>LLM选择</strong>: 对比不同LLM的实验表明，GPT-3.5 Turbo在性能和计算效率之间取得了良好的平衡。</p>

<h3>论文贡献</h3>

<ol>
<li>提出了新颖的<strong>RELED框架</strong>，首次成功地将LLM驱动的专家演示与自主探索相结合，为解决MARL中的非平稳性问题提供了有效方案。</li>
<li>设计并实现了<strong>SED</strong>和<strong>HPO</strong>两个关键模块，前者通过理论指导生成高质量专家演示，后者通过自适应机制高效融合模仿与探索。</li>
<li>提供了量化MARL非平稳性的理论框架，并将其作为反馈来优化LLM的输出。</li>
<li>通过在复杂的城市交通模拟中的大量实验，验证了RELED框架的有效性和优越性，为MARL在现实世界中的应用提供了新的思路。</li>
</ol>

            
        </div>

        <div class="footer">
            <p>Generated by AI Paper Review System at 2025-11-28 13:26:54</p>
            <p style="margin-top: 10px;">
                <a href="https://jycarlos1019.pp.ua">系统首页</a> • 
                <a href="../../search.html">搜索归档</a>
            </p>
        </div>
    </div>
</body>
</html>