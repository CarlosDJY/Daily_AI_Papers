<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>微观深度解读 - 2025-11-09</title>
    <style>
        :root {
            /* 配色：Slate (基调) + Sky (微观主题色) + Indigo (交互) */
            --primary-color: #4f46e5;
            --micro-color: #0ea5e9;    /* Sky 500 - 对应每日简报中的微观色条 */
            --bg-body: #f8fafc;
            --bg-card: #ffffff;
            --text-main: #0f172a;
            --text-secondary: #64748b;
            --text-light: #94a3b8;
            --border-color: #e2e8f0;
            
            --tag-bg: #f0f9ff;
            --tag-text: #0369a1;
            
            --warn-bg: #fff7ed;
            --warn-text: #c2410c;
            
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            --shadow-lg: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -2px rgba(0, 0, 0, 0.05);
            
            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
        }

        /* SVG 图标 */
        .icon {
            width: 1.1em;
            height: 1.1em;
            display: inline-block;
            vertical-align: middle;
            stroke-width: 2;
            stroke: currentColor;
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
            margin-bottom: 2px;
        }

        /* 导航栏 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            font-size: 14px;
        }

        .back-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: flex;
            align-items: center;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--primary-color); }
        .back-link .icon { margin-right: 6px; }

        /* 头部 */
        .header {
            text-align: center;
            margin-bottom: 50px;
        }

        .header h1 {
            color: var(--text-main);
            font-size: 28px;
            font-weight: 700;
            margin-bottom: 10px;
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
        }
        
        .header h1 .icon { color: var(--micro-color); width: 32px; height: 32px; }

        .header .subtitle {
            color: var(--text-secondary);
            font-size: 15px;
        }

        /* 论文卡片列表 */
        .paper-list {
            display: flex;
            flex-direction: column;
            gap: 30px;
        }

        .paper-card {
            background-color: var(--bg-card);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            padding: 30px;
            position: relative;
            transition: all 0.2s ease;
        }

        .paper-card:hover {
            box-shadow: var(--shadow-md);
            border-color: #cbd5e1;
        }

        /* 序号水印 */
        .paper-index {
            position: absolute;
            top: 20px;
            right: 20px;
            font-size: 40px;
            font-weight: 900;
            color: #f1f5f9;
            line-height: 1;
            pointer-events: none;
            z-index: 0;
        }

        /* 卡片头部 */
        .card-header {
            position: relative;
            z-index: 1;
            margin-bottom: 20px;
        }

        .paper-title {
            font-size: 20px;
            font-weight: 700;
            margin-bottom: 12px;
            line-height: 1.4;
            padding-right: 40px; /* 给序号留位 */
        }

        .paper-title a {
            color: var(--text-main);
            text-decoration: none;
            background-image: linear-gradient(transparent 90%, var(--micro-color) 0); /* 极简下划线 */
            background-size: 0 10px;
            background-repeat: no-repeat;
            background-position: bottom;
            transition: background-size 0.3s;
        }

        .paper-card:hover .paper-title a {
            background-size: 100% 10px;
            color: var(--primary-color);
        }

        /* 元数据行 */
        .meta-row {
            display: flex;
            flex-wrap: wrap;
            gap: 16px;
            align-items: center;
            font-size: 13px;
            color: var(--text-secondary);
            margin-bottom: 16px;
        }

        .meta-item {
            display: flex;
            align-items: center;
            gap: 6px;
        }

        .score-badge {
            background-color: #eff6ff;
            color: var(--primary-color);
            font-weight: 700;
            padding: 2px 8px;
            border-radius: 6px;
            font-size: 12px;
        }

        /* 关键词 */
        .keywords-row {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .keyword-tag {
            background-color: var(--tag-bg);
            color: var(--tag-text);
            font-size: 12px;
            padding: 4px 10px;
            border-radius: 99px;
            font-weight: 500;
        }

        /* 摘要内容 */
        .summary-content {
            font-size: 15px;
            color: #334155;
            line-height: 1.7;
            margin-bottom: 20px;
            text-align: justify;
        }
        
        .summary-label {
            font-weight: 700;
            color: var(--text-main);
            margin-right: 4px;
        }

        /* 警告框 */
        .warning-box {
            background-color: var(--warn-bg);
            color: var(--warn-text);
            padding: 12px;
            border-radius: 8px;
            font-size: 13px;
            margin-bottom: 20px;
            display: flex;
            align-items: flex-start;
            gap: 8px;
        }
        
        .warning-box .icon { flex-shrink: 0; margin-top: 2px; }

        /* 图片容器 */
        .image-wrapper {
            margin: 20px 0;
            background-color: #f8fafc;
            border: 1px solid var(--border-color);
            border-radius: 8px;
            padding: 10px;
            text-align: center;
        }

        .image-wrapper img {
            max-width: 100%;
            height: auto;
            border-radius: 4px;
            display: block;
            margin: 0 auto;
        }

        /* 底部操作栏 */
        .card-footer {
            margin-top: 25px;
            padding-top: 20px;
            border-top: 1px dashed var(--border-color);
            display: flex;
            justify-content: flex-end;
        }

        .btn-read {
            display: inline-flex;
            align-items: center;
            gap: 8px;
            background-color: var(--bg-body);
            color: var(--text-main);
            padding: 8px 16px;
            border-radius: 8px;
            text-decoration: none;
            font-size: 14px;
            font-weight: 600;
            transition: all 0.2s;
            border: 1px solid var(--border-color);
        }

        .btn-read:hover {
            background-color: var(--primary-color);
            color: #fff;
            border-color: var(--primary-color);
        }

        .page-footer {
            margin-top: 60px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        /* 移动端适配 */
        @media (max-width: 640px) {
            .paper-card { padding: 20px; }
            .paper-index { font-size: 30px; top: 15px; right: 15px; }
            .paper-title { font-size: 18px; padding-right: 30px; }
            .card-footer { justify-content: stretch; }
            .btn-read { width: 100%; justify-content: center; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="index.html" class="back-link">
                <svg class="icon" viewBox="0 0 24 24"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
                返回每日简报
            </a>
            <a href="../../index.html" class="back-link">返回汇总页</a>
        </div>

        <div class="header">
            <h1>
                <svg class="icon" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                微观深度解读
            </h1>
            <div class="subtitle">为您精选了 6 篇高质量 AI 论文的深度解析</div>
        </div>

        <div class="paper-list">
            
            <div class="paper-card">
                <div class="paper-index">#1</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05784v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05784v1.html">
                            DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.549 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            University of California, Santa Cruz, Center for Advanced AI, Accenture
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">大型语言模型</span>
                        
                        <span class="keyword-tag">去学习</span>
                        
                        <span class="keyword-tag">知识遗忘</span>
                        
                        <span class="keyword-tag">推理</span>
                        
                        <span class="keyword-tag">上下文干预</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本文提出了DRAGON框架，旨在解决大型语言模型（LLM）中有效“去学习”特定知识的挑战。该方法通过引入轻量级检测模块和基于推理的上下文干预，实现在无保留数据的情况下进行知识遗忘，同时保持模型的通用能力。DRAGON在多个实验中表现出色，证明了其高效性和鲁棒性。
                </div>

                
                <div class="image-wrapper">
                    
                    <img src="../../images/2025-11-09/ecb778e9ea694fd9aa2c7f414efd3da86b62a135fa60a80907067f88dfb8632e.jpg" alt="核心思路示意图" loading="lazy" />
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05784v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05784v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
            <div class="paper-card">
                <div class="paper-index">#2</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05919v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05919v1.html">
                            Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.494 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            Technical University of Munich
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">对抗性攻击</span>
                        
                        <span class="keyword-tag">大语言模型</span>
                        
                        <span class="keyword-tag">脆弱性评估</span>
                        
                        <span class="keyword-tag">随机森林分类器</span>
                        
                        <span class="keyword-tag">安全性机制</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本文提出了χmera框架，系统评估大语言模型（LLM）在对抗性中间人攻击下的脆弱性。通过对输入进行微小扰动，显著降低LLM的回答准确性（成功率高达85.3%）。同时，基于响应不确定性训练的随机森林分类器有效区分被攻击和正常查询，平均AUC达到96%，为LLM的安全性提供了初步防护机制。
                </div>

                
                <div class="image-wrapper">
                    
                    <img src="../../images/2025-11-09/af5362ebda53dc23a78750b602a87e2358a912b01315069a9a71012e5bf89fae.jpg" alt="核心思路示意图" loading="lazy" />
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05919v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05919v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
            <div class="paper-card">
                <div class="paper-index">#3</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05850v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05850v1.html">
                            Retrieval Quality at Context Limit
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.478 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            Google LLC
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">大型语言模型</span>
                        
                        <span class="keyword-tag">信息检索</span>
                        
                        <span class="keyword-tag">中间遗忘</span>
                        
                        <span class="keyword-tag">位置编码</span>
                        
                        <span class="keyword-tag">训练策略</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本文探讨了大型语言模型（LLMs）在长上下文中信息检索准确性下降的问题，特别是“中间遗忘”现象。通过实验分析，验证了Gemini 2.5 Flash模型在长上下文中对简单事实问答的卓越性能，显示其能有效克服该现象。研究表明，改进的位置编码和训练策略是提升检索准确性的关键。
                </div>

                
                <div class="warning-box" style="background-color: #fff1f2; color: #be123c;">
                    <svg class="icon" viewBox="0 0 24 24"><line x1="1" y1="1" x2="23" y2="23"></line><path d="M21 21l-2-2m-3.28-3.28A6 6 0 0 0 13 12.72"></path><path d="M21 21h-5.17"></path><path d="M3 3l2 2m3.28 3.28A6 6 0 0 0 11 11.28"></path><path d="M3 3v18h18"></path></svg>
                    <span>图片提取失败：未找到图片</span>
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05850v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05850v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
            <div class="paper-card">
                <div class="paper-index">#4</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05931v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05931v1.html">
                            Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.477 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            Salesforce AI Research
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">自我抽象</span>
                        
                        <span class="keyword-tag">大型语言模型</span>
                        
                        <span class="keyword-tag">软件工程</span>
                        
                        <span class="keyword-tag">计划引导</span>
                        
                        <span class="keyword-tag">政策优化</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本文提出了SAGE（Self-Abstraction from Grounded Experience）框架，旨在解决大型语言模型（LLM）代理在软件工程任务中缺乏自我学习和改进能力的问题。SAGE通过“探索-计划抽象-计划增强执行”的循环，使代理从自身经验中提取高层次计划，显著提升了任务执行的性能，尤其在SWE-Bench基准上实现了7.2%的相对提升。
                </div>

                
                <div class="image-wrapper">
                    
                    <img src="../../images/2025-11-09/ee43db92b3fa08019d4526f00050cfeec575da7aafb33bbad81582c5f52f1e2c.jpg" alt="核心思路示意图" loading="lazy" />
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05931v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05931v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
            <div class="paper-card">
                <div class="paper-index">#5</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05874v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05874v1.html">
                            An Empirical Study of Reasoning Steps in Thinking Code LLMs
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.463 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            York University, Canada
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">大型语言模型</span>
                        
                        <span class="keyword-tag">推理质量</span>
                        
                        <span class="keyword-tag">代码生成</span>
                        
                        <span class="keyword-tag">评估框架</span>
                        
                        <span class="keyword-tag">推理链</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本论文提出了一个系统的评估框架，针对大型语言模型（LLMs）在复杂代码生成任务中的推理质量进行深入分析。通过对六种先进LLMs的推理链进行大规模实证研究，识别出“完整性”是主要缺陷，并建立了评估维度（效率、逻辑一致性、完整性）和推理失败模式分类法，为未来的模型改进提供了重要见解。
                </div>

                
                <div class="image-wrapper">
                    
                    <img src="../../images/2025-11-09/8bb5d10d8863c6e2af408cbf9cf37edaa9b5db81302a2605d309faff0d140e63.jpg" alt="核心思路示意图" loading="lazy" />
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.05874v1&redirect_url=%2Freports%2F2025-11-09%2F2511_05874v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
            <div class="paper-card">
                <div class="paper-index">#6</div>
                
                <div class="card-header">
                    <div class="paper-title">
                        
                        
                        <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.06132v1&redirect_url=%2Freports%2F2025-11-09%2F2511_06132v1.html">
                            On the Convergence and Stability of Distributed Sub-model Training
                        </a>
                    </div>

                    <div class="meta-row">
                        <div class="meta-item">
                            <span class="score-badge">0.317 分</span>
                        </div>
                        
                        <div class="meta-item">
                            <svg class="icon" style="color: var(--text-light);" viewBox="0 0 24 24"><path d="M3 21h18"/><path d="M5 21V7l8-4 8 4v14"/><path d="M17 21v-8H7v8"/></svg>
                            Columbia University, The Pennsylvania State University
                        </div>
                        
                    </div>

                    
                    <div class="keywords-row">
                        
                        <span class="keyword-tag">分布式子模型训练</span>
                        
                        <span class="keyword-tag">联邦学习</span>
                        
                        <span class="keyword-tag">收敛性</span>
                        
                        <span class="keyword-tag">泛化能力</span>
                        
                        <span class="keyword-tag">滚动掩码策略</span>
                        
                    </div>
                    
                </div>

                <div class="summary-content">
                    

                    <span class="summary-label">简介：</span>本文提出了一种名为“Rolling Masked FedAvg”的分布式子模型训练框架，旨在解决联邦学习中子模型训练的收敛性和泛化能力问题。通过引入滚动掩码策略，服务器在每轮通信中对模型进行洗牌并分配给客户端，从而提高了训练的稳定性和准确性。实验结果验证了该方法在高数据异构性环境下的优越性能，并提供了首次严格的收敛性分析。
                </div>

                
                <div class="image-wrapper">
                    
                    <img src="../../images/2025-11-09/bb2c27e3a5adf7ae19c587d956087b6baf2391b573d5be4fc30fa18e0cd3ebb3.jpg" alt="核心思路示意图" loading="lazy" />
                </div>
                

                <div class="card-footer">
                    
                    
                    <a href="https://jycarlos1019.pp.ua/track?paper_id=2511.06132v1&redirect_url=%2Freports%2F2025-11-09%2F2511_06132v1.html" class="btn-read">
                        阅读详细解读
                        <svg class="icon" viewBox="0 0 24 24"><line x1="5" y1="12" x2="19" y2="12"></line><polyline points="12 5 19 12 12 19"></polyline></svg>
                    </a>
                </div>
            </div>
            
        </div>

        <div class="page-footer">
            <p>生成时间: 2025-11-20 13:23:19</p>
            <p>访问地址: <a href="https://jycarlos1019.pp.ua">https://jycarlos1019.pp.ua</a></p>
        </div>
    </div>
</body>
</html>