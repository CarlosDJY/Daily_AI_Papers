<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>主流技术方向 - 2025-11-09</title>
    <style>
        :root {
            /* 系统配色 */
            --primary-color: #4f46e5;   /* Indigo */
            --direction-color: #f97316; /* Orange 500 */
            --direction-dark: #c2410c;  /* Orange 700 */
            --direction-light: #fff7ed; /* Orange 50 */
            --direction-border: #fdba74;/* Orange 300 */
            
            --bg-body: #f8fafc;
            --bg-card: #ffffff;
            --text-main: #0f172a;
            --text-secondary: #64748b;
            --text-light: #94a3b8;
            --border-color: #e2e8f0;
            
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            
            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        /* SVG 图标 */
        .icon {
            width: 1.1em;
            height: 1.1em;
            display: inline-block;
            vertical-align: middle;
            stroke-width: 2;
            stroke: currentColor;
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
        }

        /* 导航栏 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            font-size: 14px;
        }

        .back-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: flex;
            align-items: center;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--primary-color); }
        .back-link .icon { margin-right: 6px; }

        .date-badge {
            background-color: #e0e7ff;
            color: var(--primary-color);
            padding: 4px 12px;
            border-radius: 99px;
            font-weight: 600;
            font-size: 13px;
        }

        /* 头部 */
        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .header h1 {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
            margin-bottom: 12px;
        }
        
        .header h1 .icon { color: var(--direction-color); width: 36px; height: 36px; }
        
        .header .subtitle { 
            color: var(--text-secondary);
            font-size: 16px;
        }

        /* --- Tab 导航栏 --- */
        .tabs-container {
            position: sticky;
            top: 20px;
            z-index: 100;
            margin-bottom: 30px;
            /* 磨砂玻璃效果 */
            background: rgba(248, 250, 252, 0.9);
            backdrop-filter: blur(8px);
            padding: 10px 0;
            border-radius: 16px;
        }

        .tabs-scroll {
            display: flex;
            gap: 12px;
            overflow-x: auto;
            padding: 4px 4px 12px 4px; /* 底部留空间给滚动条或阴影 */
            
            /* 隐藏滚动条但保持可滚动 */
            scrollbar-width: none; /* Firefox */
            -ms-overflow-style: none;  /* IE 10+ */
            
            /* 鼠标交互优化 */
            cursor: grab; /* 提示可拖拽 */
            user-select: none; /* 防止拖拽时选中文字 */
        }
        
        .tabs-scroll::-webkit-scrollbar { 
            display: none; /* Chrome/Safari */
        }

        /* 拖拽时的光标状态 */
        .tabs-scroll.active {
            cursor: grabbing;
        }

        .tab-btn {
            flex-shrink: 0;
            background-color: var(--bg-card);
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
            padding: 10px 20px;
            border-radius: 99px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: var(--shadow-sm);
            display: flex;
            align-items: center;
            gap: 8px;
            /* 防止图片/文字干扰拖拽 */
            pointer-events: auto; 
        }

        .tab-btn:hover {
            border-color: var(--direction-border);
            color: var(--direction-color);
            transform: translateY(-2px);
        }

        .tab-btn.active {
            background-color: var(--direction-color);
            color: white;
            border-color: var(--direction-color);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3); /* Orange glow */
        }

        .tab-count {
            background-color: rgba(0,0,0,0.1);
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 11px;
        }
        
        .tab-btn.active .tab-count {
            background-color: rgba(255,255,255,0.2);
        }

        /* --- 内容区域 --- */
        .tab-pane {
            display: none; /* 默认隐藏 */
            animation: fadeIn 0.3s ease-out;
        }
        
        .tab-pane.active {
            display: block;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .direction-block {
            background-color: var(--bg-card);
            border-radius: 20px;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-md);
            overflow: hidden;
        }

        .direction-header-info {
            background-color: var(--direction-light);
            padding: 24px 30px;
            border-bottom: 1px solid var(--direction-border);
        }
        
        .direction-title-lg {
            font-size: 22px;
            font-weight: 800;
            color: var(--direction-dark);
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .direction-desc-lg {
            font-size: 15px;
            color: #9a3412;
            opacity: 0.9;
        }

        /* --- 手风琴论文列表 --- */
        .paper-list {
            padding: 10px 30px 30px;
        }

        .paper-item {
            border-bottom: 1px solid var(--border-color);
        }
        
        .paper-item:last-child { border-bottom: none; }

        /* 折叠头部 (可点击) */
        .paper-header {
            padding: 20px 0;
            cursor: pointer;
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            gap: 16px;
            group: paper-header;
        }

        .paper-title-row {
            flex-grow: 1;
        }

        .paper-title {
            font-size: 17px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            transition: color 0.2s;
            margin-bottom: 6px;
        }
        
        .paper-header:hover .paper-title {
            color: var(--direction-color);
        }

        .paper-meta-preview {
            font-size: 13px;
            color: var(--text-light);
            display: flex;
            align-items: center;
            gap: 12px;
        }
        
        .score-badge {
            background-color: #f3f4f6;
            color: var(--text-secondary);
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            font-size: 12px;
        }
        
        .toggle-icon {
            color: var(--text-light);
            transition: transform 0.3s ease, color 0.2s;
            flex-shrink: 0;
            margin-top: 4px;
        }
        
        /* 激活状态样式 */
        .paper-item.expanded .toggle-icon {
            transform: rotate(180deg);
            color: var(--direction-color);
        }
        
        .paper-item.expanded .paper-title {
            color: var(--direction-color);
        }

        /* 折叠内容区 */
        .paper-body {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
            padding-left: 4px; /* 微调对齐 */
        }
        
        .paper-content-inner {
            padding-bottom: 24px;
            color: var(--text-secondary);
            font-size: 15px;
            line-height: 1.7;
            text-align: justify;
        }
        
        .paper-links {
            margin-top: 12px;
            padding-top: 12px;
            border-top: 1px dashed var(--border-color);
            display: flex;
            gap: 16px;
            font-size: 13px;
        }
        
        .action-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        
        .action-link:hover { text-decoration: underline; }

        /* 空状态 */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            background-color: var(--bg-card);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .footer {
            margin-top: 60px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }
        
        .footer a { color: var(--text-secondary); text-decoration: none; }

        @media (max-width: 640px) {
            .container { padding: 20px 15px; }
            .direction-header-info { padding: 20px; }
            .paper-list { padding: 0 20px 20px; }
            .paper-title { font-size: 16px; }
        }
    </style>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 1. Tab 栏鼠标拖拽滚动 (Draggable Scroll)
            const slider = document.querySelector('.tabs-scroll');
            let isDown = false;
            let startX;
            let scrollLeft;
            let isDragging = false; // 区分是“点击”还是“拖拽”

            slider.addEventListener('mousedown', (e) => {
                isDown = true;
                isDragging = false;
                slider.classList.add('active');
                startX = e.pageX - slider.offsetLeft;
                scrollLeft = slider.scrollLeft;
            });

            slider.addEventListener('mouseleave', () => {
                isDown = false;
                slider.classList.remove('active');
            });

            slider.addEventListener('mouseup', () => {
                isDown = false;
                slider.classList.remove('active');
                // 如果是拖拽结束，为了防止触发 click，我们在 click 事件中做判断
                setTimeout(() => { isDragging = false; }, 0);
            });

            slider.addEventListener('mousemove', (e) => {
                if (!isDown) return;
                e.preventDefault(); // 防止选中文字
                const x = e.pageX - slider.offsetLeft;
                const walk = (x - startX) * 2; // 滚动速度系数
                slider.scrollLeft = scrollLeft - walk;
                
                // 如果移动距离超过 5px，则视为拖拽，不是点击
                if (Math.abs(walk) > 5) {
                    isDragging = true;
                }
            });

            // 2. Tab 切换逻辑
            const tabs = document.querySelectorAll('.tab-btn');
            const panes = document.querySelectorAll('.tab-pane');

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    // 如果刚才是在拖拽，则拦截点击，不切换 Tab
                    if (isDragging) {
                        e.preventDefault();
                        e.stopPropagation();
                        return;
                    }

                    // 移除所有激活状态
                    tabs.forEach(t => t.classList.remove('active'));
                    panes.forEach(p => p.classList.remove('active'));

                    // 激活当前
                    tab.classList.add('active');
                    const targetId = tab.getAttribute('data-target');
                    document.getElementById(targetId).classList.add('active');
                    
                    // 滚动 Tab 到可见区域 (移动端/拖拽后体验优化)
                    tab.scrollIntoView({ behavior: 'smooth', block: 'nearest', inline: 'center' });
                });
            });

            // 3. 论文折叠/展开逻辑
            const paperHeaders = document.querySelectorAll('.paper-header');

            paperHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const item = this.parentElement;
                    const body = item.querySelector('.paper-body');
                    
                    // 切换状态
                    item.classList.toggle('expanded');
                    
                    if (item.classList.contains('expanded')) {
                        body.style.maxHeight = body.scrollHeight + "px";
                    } else {
                        body.style.maxHeight = null;
                    }
                });
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <!-- 导航 -->
        <div class="nav-bar">
            <a href="index.html" class="back-link">
                <svg class="icon" viewBox="0 0 24 24"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
                返回每日简报
            </a>
            <div class="date-badge">2025-11-09</div>
        </div>

        <!-- 头部 -->
        <div class="header">
            <h1>
                <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="10"></circle><polygon points="16.24 7.76 14.12 14.12 7.76 16.24 9.88 9.88 16.24 7.76"></polygon></svg>
                每日主流技术方向
            </h1>
            <div class="subtitle">聚合 RAG / LLM / Agent 等核心赛道，点击下方标签切换</div>
        </div>

        
        
        
        
        <!-- Tab 导航栏 (可拖拽) -->
        <div class="tabs-container">
            <div class="tabs-scroll">
                
                <button class="tab-btn active" data-target="tab-1">
                    Agent
                    <span class="tab-count">10</span>
                </button>
                
                <button class="tab-btn " data-target="tab-2">
                    Alignment
                    <span class="tab-count">15</span>
                </button>
                
                <button class="tab-btn " data-target="tab-3">
                    LLM
                    <span class="tab-count">58</span>
                </button>
                
                <button class="tab-btn " data-target="tab-4">
                    Multimodal
                    <span class="tab-count">22</span>
                </button>
                
                <button class="tab-btn " data-target="tab-5">
                    Optimization
                    <span class="tab-count">24</span>
                </button>
                
                <button class="tab-btn " data-target="tab-6">
                    RAG
                    <span class="tab-count">2</span>
                </button>
                
                <button class="tab-btn " data-target="tab-7">
                    RL
                    <span class="tab-count">117</span>
                </button>
                
                <button class="tab-btn " data-target="tab-8">
                    Vision
                    <span class="tab-count">13</span>
                </button>
                
            </div>
        </div>

        <!-- 内容区域 -->
        <div class="content-wrapper">
            
            <div id="tab-1" class="tab-pane active">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Agent
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 10 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在智能体（Agent）领域的研究动态主要集中在提高大型语言模型（LLMs）和视觉语言模型（VLMs）的可靠性与安全性，尤其是在复杂任务和多代理系统中的应用。共性趋势包括利用先进的预测和评估方法来解决模型输出的过度自信和幻觉问题，以及通过知识图谱和多代理协作来优化数据整合和处理效率。这些研究不仅提升了智能体在实际应用中的表现，也为未来的技术发展提供了重要的理论基础和潜在的商业价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have recently emerged as planners for
language-instructed agents, generating sequences of actions to accomplish
natural language tasks. However, their reliability remains a challenge,
especially in long-horizon tasks, since they often produce overconfident yet
wrong outputs. Conformal Prediction (CP) has been leveraged to address this
issue by wrapping LLM outputs into prediction sets that contain the correct
action with a user-defined confidence. When the prediction set is a singleton,
the planner executes that action; otherwise, it requests help from a user. This
has led to LLM-based planners that can ensure plan correctness with a
user-defined probability. However, as LLMs are trained in an
uncertainty-agnostic manner, without awareness of prediction sets, they tend to
produce unnecessarily large sets, particularly at higher confidence levels,
resulting in frequent human interventions limiting autonomous deployment. To
address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first
CP-aware finetuning framework for LLM-based planners that explicitly reduces
prediction-set size and, in turn, the need for user interventions. We evaluate
our approach on multiple language-instructed robot planning problems and show
consistent improvements over uncertainty-aware and uncertainty-agnostic
finetuning baselines in terms of prediction-set size, and help rates. Finally,
we demonstrate robustness of our method to out-of-distribution scenarios in
hardware experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06575v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06575v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Vision Language Models (VLMs) are increasingly used in autonomous driving to
help understand traffic scenes, but they sometimes produce hallucinations,
which are false details not grounded in the visual input. Detecting and
mitigating hallucinations is challenging when ground-truth references are
unavailable and model internals are inaccessible. This paper proposes a novel
self-contained low-rank approach to automatically rank multiple candidate
captions generated by multiple VLMs based on their hallucination levels, using
only the captions themselves without requiring external references or model
access. By constructing a sentence-embedding matrix and decomposing it into a
low-rank consensus component and a sparse residual, we use the residual
magnitude to rank captions: selecting the one with the smallest residual as the
most hallucination-free. Experiments on the NuScenes dataset demonstrate that
our approach achieves 87% selection accuracy in identifying hallucination-free
captions, representing a 19% improvement over the unfiltered baseline and a
6-10% improvement over multi-agent debate method. The sorting produced by
sparse error magnitudes shows strong correlation with human judgments of
hallucinations, validating our scoring mechanism. Additionally, our method,
which can be easily parallelized, reduces inference time by 51-67% compared to
debate approaches, making it practical for real-time autonomous driving
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06496v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06496v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Enterprises often maintain multiple databases for storing critical business
data in siloed systems, resulting in inefficiencies and challenges with data
interoperability. A key to overcoming these challenges lies in integrating
disparate data sources, enabling businesses to unlock the full potential of
their data. Our work presents a novel approach for integrating multiple
databases using knowledge graphs, focusing on the application of large language
models as semantic agents for mapping and connecting structured data across
systems by leveraging existing vocabularies. The proposed methodology
introduces a semantic layer above tables in relational databases, utilizing a
system comprising multiple LLM agents that map tables and columns to Schema.org
terms. Our approach achieves a mapping accuracy of over 90% in multiple
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06455v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06455v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this work, we study the risks of collective financial fraud in large-scale
multi-agent systems powered by large language model (LLM) agents. We
investigate whether agents can collaborate in fraudulent behaviors, how such
collaboration amplifies risks, and what factors influence fraud success. To
support this research, we present MultiAgentFraudBench, a large-scale benchmark
for simulating financial fraud scenarios based on realistic online
interactions. The benchmark covers 28 typical online fraud scenarios, spanning
the full fraud lifecycle across both public and private domains. We further
analyze key factors affecting fraud success, including interaction depth,
activity level, and fine-grained collaboration failure modes. Finally, we
propose a series of mitigation strategies, including adding content-level
warnings to fraudulent posts and dialogues, using LLMs as monitors to block
potentially malicious agents, and fostering group resilience through
information sharing at the societal level. Notably, we observe that malicious
agents can adapt to environmental interventions. Our findings highlight the
real-world risks of multi-agent financial fraud and suggest practical measures
for mitigating them. Code is available at
https://github.com/zheng977/MutiAgent4Fraud.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06448v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06448v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient LLM Safety Evaluation through Multi-Agent Debate
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Safety evaluation of large language models (LLMs) increasingly relies on
LLM-as-a-Judge frameworks, but the high cost of frontier models limits
scalability. We propose a cost-efficient multi-agent judging framework that
employs Small Language Models (SLMs) through structured debates among critic,
defender, and judge agents. To rigorously assess safety judgments, we construct
HAJailBench, a large-scale human-annotated jailbreak benchmark comprising
12,000 adversarial interactions across diverse attack methods and target
models. The dataset provides fine-grained, expert-labeled ground truth for
evaluating both safety robustness and judge reliability. Our SLM-based
framework achieves agreement comparable to GPT-4o judges on HAJailBench while
substantially reducing inference cost. Ablation results show that three rounds
of debate yield the optimal balance between accuracy and efficiency. These
findings demonstrate that structured, value-aligned debate enables SLMs to
capture semantic nuances of jailbreak attacks and that HAJailBench offers a
reliable foundation for scalable LLM safety evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06396v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06396v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Designing high-performance kernels requires expert-level tuning and a deep
understanding of hardware characteristics. Recent advances in large language
models (LLMs) have enabled automated kernel generation, yet most existing
systems rely solely on correctness or execution time feedback, lacking the
ability to reason about low-level performance bottlenecks. In this paper, we
introduce PRAGMA, a profile-guided AI kernel generation framework that
integrates execution feedback and fine-grained hardware profiling into the
reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks,
preserve historical best versions, and iteratively refine code quality. We
evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show
that PRAGMA consistently outperforms baseline AIKG without profiling enabled
and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on
CPU and GPU platforms, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06345v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06345v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Station: An Open-World Environment for AI-Driven Discovery
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce the STATION, an open-world multi-agent environment that models a
miniature scientific ecosystem. Leveraging their extended context windows,
agents in the Station can engage in long scientific journeys that include
reading papers from peers, formulating hypotheses, submitting code, performing
analyses, and publishing results. Importantly, there is no centralized system
coordinating their activities - agents are free to choose their own actions and
develop their own narratives within the Station. Experiments demonstrate that
AI agents in the Station achieve new state-of-the-art performance on a wide
range of benchmarks, spanning from mathematics to computational biology to
machine learning, notably surpassing AlphaEvolve in circle packing. A rich
tapestry of narratives emerges as agents pursue independent research, interact
with peers, and build upon a cumulative history. From these emergent
narratives, novel methods arise organically, such as a new density-adaptive
algorithm for scRNA-seq batch integration. The Station marks a first step
towards autonomous scientific discovery driven by emergent behavior in an
open-world environment, representing a new paradigm that moves beyond rigid
optimization.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06309v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06309v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    User interface (UI) development requires translating design mockups into
functional code, a process that remains repetitive and labor-intensive. While
recent Vision-Language Models (VLMs) automate UI-to-Code generation, they
generate only static HTML/CSS/JavaScript layouts lacking interactivity. To
address this, we propose WebVIA, the first agentic framework for interactive
UI-to-Code generation and validation. The framework comprises three components:
1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code
model that generates executable interactive code; 3) a validation module that
verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves
more stable and accurate UI exploration than general-purpose agents (e.g.,
Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit
substantial improvements in generating executable and interactive
HTML/CSS/JavaScript code, outperforming their base counterparts across both
interactive and static UI2Code benchmarks. Our code and models are available at
\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06251v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06251v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In open-vocabulary mobile manipulation (OVMM), task success often hinges on
the selection of an appropriate base placement for the robot. Existing
approaches typically navigate to proximity-based regions without considering
affordances, resulting in frequent manipulation failures. We propose
Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base
placement that integrates semantic understanding from vision-language models
(VLMs) with geometric feasibility through an iterative optimization process.
Our method constructs cross-modal representations, namely Affordance RGB and
Obstacle Map+, to align semantics with spatial context. This enables reasoning
that extends beyond the egocentric limitations of RGB perception. To ensure
interaction is guided by task-relevant affordances, we leverage coarse semantic
priors from VLMs to guide the search toward task-relevant regions and refine
placements with geometric constraints, thereby reducing the risk of convergence
to local optima. Evaluated on five diverse open-vocabulary mobile manipulation
tasks, our system achieves an 85% success rate, significantly outperforming
classical geometric planners and VLM-based methods. This demonstrates the
promise of affordance-aware and multimodal reasoning for generalizable,
instruction-conditioned planning in OVMM.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06240v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06240v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As AI moves beyond text, large language models (LLMs) increasingly power
vision, audio, and document understanding; however, their high inference costs
hinder real-time, scalable deployment. Conversely, smaller open-source models
offer cost advantages but struggle with complex or multimodal queries. We
introduce a unified, modular framework that intelligently routes each query -
textual, multimodal, or complex - to the most fitting expert model, using a
learned routing network that balances cost and quality. For vision tasks, we
employ a two-stage open-source pipeline optimized for efficiency and reviving
efficient classical vision components where they remain SOTA for sub-tasks. On
benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual
Question Answering (VQA), we match or exceed the performance of always-premium
LLM (monolithic systems with one model serving all query types) performance,
yet reduce the reliance on costly models by over 67%. With its extensible,
multi-agent orchestration, we deliver high-quality, resource-efficient AI at
scale.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06441v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06441v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-2" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Alignment
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 15 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在对齐（Alignment）技术方向的研究动态主要集中在提升模型的推理能力和跨语言适应性上。研究者们通过引入人类认知行为、优化多语言模型的重对齐策略以及探索大型语言模型在程序安全性评估中的应用，旨在增强系统的泛化能力和用户体验。此外，针对低资源语言和复杂环境中的数据不足问题，研究者们也在积极寻求新的解决方案，以提高模型的实用性和可靠性。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM For Loop Invariant Generation and Fixing: How Far Are We?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A loop invariant is a property of a loop that remains true before and after
each execution of the loop. The identification of loop invariants is a critical
step to support automated program safety assessment. Recent advancements in
Large Language Models (LLMs) have demonstrated potential in diverse software
engineering (SE) and formal verification tasks. However, we are not aware of
the performance of LLMs to infer loop invariants. We report an empirical study
of both open-source and closed-source LLMs of varying sizes to assess their
proficiency in inferring inductive loop invariants for programs and in fixing
incorrect invariants. Our findings reveal that while LLMs exhibit some utility
in inferring and repairing loop invariants, their performance is substantially
enhanced when supplemented with auxiliary information such as domain knowledge
and illustrative examples. LLMs achieve a maximum success rate of 78\% in
generating, but are limited to 16\% in repairing the invariant.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06552v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06552v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Realignment is a promising strategy to improve cross-lingual transfer in
multilingual language models. However, empirical results are mixed and often
unreliable, particularly for typologically distant or low-resource languages
(LRLs) compared to English. Moreover, word realignment tools often rely on
high-quality parallel data, which can be scarce or noisy for many LRLs. In this
work, we conduct an extensive empirical study to investigate whether
realignment truly benefits from using all available languages, or if
strategically selected subsets can offer comparable or even improved
cross-lingual transfer, and study the impact on LRLs. Our controlled
experiments show that realignment can be particularly effective for LRLs and
that using carefully selected, linguistically diverse subsets can match full
multilingual alignment, and even outperform it for unseen LRLs. This indicates
that effective realignment does not require exhaustive language coverage and
can reduce data collection overhead, while remaining both efficient and robust
when guided by informed language selection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06497v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06497v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Brain-Inspired Planning for Better Generalization in Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Existing Reinforcement Learning (RL) systems encounter significant challenges
when applied to real-world scenarios, primarily due to poor generalization
across environments that differ from their training conditions. This thesis
explores the direction of enhancing agents' zero-shot systematic generalization
abilities by granting RL agents reasoning behaviors that are found to help
systematic generalization in the human brain. Inspired by human conscious
planning behaviors, we first introduced a top-down attention mechanism, which
allows a decision-time planning agent to dynamically focus its reasoning on the
most relevant aspects of the environmental state given its instantaneous
intentions, a process we call "spatial abstraction". This approach
significantly improves systematic generalization outside the training tasks.
Subsequently, building on spatial abstraction, we developed the Skipper
framework to automatically decompose complex tasks into simpler, more
manageable sub-tasks. Skipper provides robustness against distributional shifts
and efficacy in long-term, compositional planning by focusing on pertinent
spatial and temporal elements of the environment. Finally, we identified a
common failure mode and safety risk in planning agents that rely on generative
models to generate state targets during planning. It is revealed that most
agents blindly trust the targets they hallucinate, resulting in delusional
planning behaviors. Inspired by how the human brain rejects delusional
intentions, we propose learning a feasibility evaluator to enable rejecting
hallucinated infeasible targets, which led to significant performance
improvements in various kinds of planning agents. Finally, we suggest
directions for future research, aimed at achieving general task abstraction and
fully enabling abstract planning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06470v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06470v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Conversational search interfaces, like ChatGPT, offer an interactive,
personalized, and engaging user experience compared to traditional search. On
the downside, they are prone to cause overtrust issues where users rely on
their responses even when they are incorrect. What aspects of the
conversational interaction paradigm drive people to adopt it, and how it
creates personalized experiences that lead to overtrust, is not clear. To
understand the factors influencing the adoption of conversational interfaces,
we conducted a survey with 173 participants. We examined user perceptions
regarding trust, human-likeness (anthropomorphism), and design preferences
between ChatGPT and Google. To better understand the overtrust phenomenon, we
asked users about their willingness to trade off factuality for constructs like
ease of use or human-likeness. Our analysis identified two distinct user
groups: those who use both ChatGPT and Google daily (DUB), and those who
primarily rely on Google (DUG). The DUB group exhibited higher trust in
ChatGPT, perceiving it as more human-like, and expressed greater willingness to
trade factual accuracy for enhanced personalization and conversational flow.
Conversely, the DUG group showed lower trust toward ChatGPT but still
appreciated aspects like ad-free experiences and responsive interactions.
Demographic analysis further revealed nuanced patterns, with middle-aged adults
using ChatGPT less frequently yet trusting it more, suggesting potential
vulnerability to misinformation. Our findings contribute to understanding user
segmentation, emphasizing the critical roles of personalization and
human-likeness in conversational IR systems, and reveal important implications
regarding users' willingness to compromise factual accuracy for more engaging
interactions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06447v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06447v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable geospatial information on road accidents is vital for safety
analysis and infrastructure planning, yet most low- and middle-income countries
continue to face a critical shortage of accurate, location-specific crash data.
Existing text-based geocoding tools perform poorly in multilingual and
unstructured news environments, where incomplete place descriptions and mixed
Bangla-English scripts obscure spatial context. To address these limitations,
this study introduces ALIGN (Accident Location Inference through Geo-Spatial
Neural Reasoning)- a vision-language framework that emulates human spatial
reasoning to infer accident coordinates directly from textual and map-based
cues. ALIGN integrates large language and vision-language models within a
multi-stage pipeline that performs optical character recognition, linguistic
reasoning, and map-level verification through grid-based spatial scanning. The
framework systematically evaluates each predicted location against contextual
and visual evidence, ensuring interpretable, fine-grained geolocation outcomes
without requiring model retraining. Applied to Bangla-language news data, ALIGN
demonstrates consistent improvements over traditional geoparsing methods,
accurately identifying district and sub-district-level crash sites. Beyond its
technical contribution, the framework establishes a high accuracy foundation
for automated crash mapping in data-scarce regions, supporting evidence-driven
road-safety policymaking and the broader integration of multimodal artificial
intelligence in transportation analytics. The code for this paper is
open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06316v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06316v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Exploiting Inter-Session Information with Frequency-enhanced Dual-Path Networks for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sequential recommendation (SR) aims to predict a user's next item preference
by modeling historical interaction sequences. Recent advances often integrate
frequency-domain modules to compensate for self-attention's low-pass nature by
restoring the high-frequency signals critical for personalized recommendations.
Nevertheless, existing frequency-aware solutions process each session in
isolation and optimize exclusively with time-domain objectives. Consequently,
they overlook cross-session spectral dependencies and fail to enforce alignment
between predicted and actual spectral signatures, leaving valuable frequency
information under-exploited. To this end, we propose FreqRec, a
Frequency-Enhanced Dual-Path Network for sequential Recommendation that jointly
captures inter-session and intra-session behaviors via a learnable
Frequency-domain Multi-layer Perceptrons. Moreover, FreqRec is optimized under
a composite objective that combines cross entropy with a frequency-domain
consistency loss, explicitly aligning predicted and true spectral signatures.
Extensive experiments on three benchmarks show that FreqRec surpasses strong
baselines and remains robust under data sparsity and noisy-log conditions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06285v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06285v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Organizations are increasingly exploring delegation of screening and
negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is
constrained by governance: preventing unauthorized commitments, ensuring
sufficient information before bargaining, and maintaining effective human
oversight and auditability. Prior work on large language model negotiation
largely emphasizes autonomous bargaining between agents and omits practical
needs such as staged information gathering, explicit authorization boundaries,
and systematic feedback integration. We propose GAIA, a governance-first
framework for LLM-human agency in B2B negotiation and screening. GAIA defines
three essential roles - Principal (human), Delegate (LLM agent), and
Counterparty - with an optional Critic to enhance performance, and organizes
interactions through three mechanisms: information-gated progression that
separates screening from negotiation; dual feedback integration that combines
AI critique with lightweight human corrections; and authorization boundaries
with explicit escalation paths. Our contributions are fourfold: (1) a formal
governance framework with three coordinated mechanisms and four safety
invariants for delegation with bounded authorization; (2) information-gated
progression via task-completeness tracking (TCI) and explicit state transitions
that separate screening from commitment; (3) dual feedback integration that
blends Critic suggestions with human oversight through parallel learning
channels; and (4) a hybrid validation blueprint that combines automated
protocol metrics with human judgment of outcomes and safety. By bridging theory
and practice, GAIA offers a reproducible specification for safe, efficient, and
accountable AI delegation that can be instantiated across procurement, real
estate, and staffing workflows.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06262v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06262v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieving molecular structures from tandem mass spectra is a crucial step in
rapid compound identification. Existing retrieval methods, such as traditional
mass spectral library matching, suffer from limited spectral library coverage,
while recent cross-modal representation learning frameworks often encounter
modality misalignment, resulting in suboptimal retrieval accuracy and
generalization. To address these limitations, we propose GLMR, a Generative
Language Model-based Retrieval framework that mitigates the cross-modal
misalignment through a two-stage process. In the pre-retrieval stage, a
contrastive learning-based model identifies top candidate molecules as
contextual priors for the input mass spectrum. In the generative retrieval
stage, these candidate molecules are integrated with the input mass spectrum to
guide a generative model in producing refined molecular structures, which are
then used to re-rank the candidates based on molecular similarity. Experiments
on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR
significantly outperforms existing methods, achieving over 40% improvement in
top-1 accuracy and exhibiting strong generalizability.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06259v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06259v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Assertion-Aware Test Code Summarization with Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unit tests often lack concise summaries that convey test intent, especially
in auto-generated or poorly documented codebases. Large Language Models (LLMs)
offer a promising solution, but their effectiveness depends heavily on how they
are prompted. Unlike generic code summarization, test-code summarization poses
distinct challenges because test methods validate expected behavior through
assertions rather than im- plementing functionality. This paper presents a new
benchmark of 91 real-world Java test cases paired with developer-written
summaries and conducts a controlled ablation study to investigate how test
code-related components-such as the method under test (MUT), assertion
messages, and assertion semantics-affect the performance of LLM-generated test
summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and
Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU,
ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation.
Results show that prompting with as- sertion semantics improves summary quality
by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while
requiring fewer input tokens. Codex and Qwen-Coder achieve the highest
alignment with human-written summaries, while DeepSeek underperforms despite
high lexical overlap. The replication package is publicly available at
https://doi.org/10. 5281/zenodo.17067550
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06227v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06227v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate accident anticipation is essential for enhancing the safety of
autonomous vehicles (AVs). However, existing methods often assume ideal
conditions, overlooking challenges such as sensor failures, environmental
disturbances, and data imperfections, which can significantly degrade
prediction accuracy. Additionally, previous models have not adequately
addressed the considerable variability in driver behavior and accident rates
across different vehicle types. To overcome these limitations, this study
introduces ROAR, a novel approach for accident detection and prediction. ROAR
combines Discrete Wavelet Transform (DWT), a self adaptive object aware module,
and dynamic focal loss to tackle these challenges. The DWT effectively extracts
features from noisy and incomplete data, while the object aware module improves
accident prediction by focusing on high-risk vehicles and modeling the spatial
temporal relationships among traffic agents. Moreover, dynamic focal loss
mitigates the impact of class imbalance between positive and negative samples.
Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car
Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently
outperforms existing baselines in key metrics such as Average Precision (AP)
and mean Time to Accident (mTTA). These results demonstrate the model's
robustness in real-world conditions, particularly in handling sensor
degradation, environmental noise, and imbalanced data distributions. This work
offers a promising solution for reliable and accurate accident anticipation in
complex traffic environments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06226v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06226v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Detecting Alzheimer's Disease (AD) from narrative transcripts remains a
challenging task for large language models (LLMs), particularly under
out-of-distribution (OOD) and data-scarce conditions. While in-context learning
(ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL
approaches often suffer from task recognition failure, suboptimal demonstration
selection, and misalignment between label words and task objectives, issues
that are amplified in clinical domains like AD detection. We propose Explicit
Knowledge In-Context Learners (EK-ICL), a novel framework that integrates
structured explicit knowledge to enhance reasoning stability and task alignment
in ICL. EK-ICL incorporates three knowledge components: confidence scores
derived from small language models (SLMs) to ground predictions in
task-relevant patterns, parsing feature scores to capture structural
differences and improve demo selection, and label word replacement to resolve
semantic misalignment with LLM priors. In addition, EK-ICL employs a
parsing-based retrieval strategy and ensemble prediction to mitigate the
effects of semantic homogeneity in AD transcripts. Extensive experiments across
three AD datasets demonstrate that EK-ICL significantly outperforms
state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that
ICL performance in AD detection is highly sensitive to the alignment of label
semantics and task-specific context, underscoring the importance of explicit
knowledge in clinical reasoning under low-resource conditions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06215v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06215v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EASE: Practical and Efficient Safety Alignment for Small Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Small language models (SLMs) are increasingly deployed on edge devices,
making their safety alignment crucial yet challenging. Current shallow
alignment methods that rely on direct refusal of malicious queries fail to
provide robust protection, particularly against adversarial jailbreaks. While
deliberative safety reasoning alignment offers deeper alignment for defending
against sophisticated attacks, effectively implanting such reasoning capability
in SLMs with limited capabilities remains an open challenge. Moreover, safety
reasoning incurs significant computational overhead as models apply reasoning
to nearly all queries, making it impractical for resource-constrained edge
deployment scenarios that demand rapid responses. We propose EASE, a novel
framework that enables practical and Efficient safety Alignment for Small
languagE models. Our approach first identifies the optimal safety reasoning
teacher that can effectively distill safety reasoning capabilities to SLMs. We
then align models to selectively activate safety reasoning for dangerous
adversarial jailbreak queries while providing direct responses to
straightforward malicious queries and general helpful tasks. This selective
mechanism enables small models to maintain robust safety guarantees against
sophisticated attacks while preserving computational efficiency for benign
interactions. Experimental results demonstrate that EASE reduces jailbreak
attack success rates by up to 17% compared to shallow alignment methods while
reducing inference overhead by up to 90% compared to deliberative safety
reasoning alignment, making it practical for SLMs real-world edge deployments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06512v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06512v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Control barrier functions (CBFs) are a popular tool for safety certification
of nonlinear dynamical control systems. Recently, CBFs represented as neural
networks have shown great promise due to their expressiveness and applicability
to a broad class of dynamics and safety constraints. However, verifying that a
trained neural network is indeed a valid CBF is a computational bottleneck that
limits the size of the networks that can be used. To overcome this limitation,
we present a novel framework for verifying neural CBFs based on piecewise
linear upper and lower bounds on the conditions required for a neural network
to be a CBF. Our approach is rooted in linear bound propagation (LBP) for
neural networks, which we extend to compute bounds on the gradients of the
network. Combined with McCormick relaxation, we derive linear upper and lower
bounds on the CBF conditions, thereby eliminating the need for computationally
expensive verification procedures. Our approach applies to arbitrary
control-affine systems and a broad range of nonlinear activation functions. To
reduce conservatism, we develop a parallelizable refinement strategy that
adaptively refines the regions over which these bounds are computed. Our
approach scales to larger neural networks than state-of-the-art verification
procedures for CBFs, as demonstrated by our numerical experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06341v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06341v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    High-quality Question-Answer (QA) datasets are foundational for reliable
Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit
persistent gaps in domain coverage, misaligned difficulty distributions, and
factual inconsistencies. The recent surge in generative model-powered datasets
has compounded these quality challenges. In this work, we introduce RefineLab,
the first LLM-driven framework that automatically refines raw QA textual data
into high-quality datasets under a controllable token-budget constraint.
RefineLab takes a set of target quality attributes (such as coverage and
difficulty balance) as refinement objectives, and performs selective edits
within a predefined token budget to ensure practicality and efficiency. In
essence, RefineLab addresses a constrained optimization problem: improving the
quality of QA samples as much as possible while respecting resource
limitations. With a set of available refinement operations (e.g., rephrasing,
distractor replacement), RefineLab takes as input the original dataset, a
specified set of target quality dimensions, and a token budget, and determines
which refinement operations should be applied to each QA sample. This process
is guided by an assignment module that selects optimal refinement strategies to
maximize overall dataset quality while adhering to the budget constraint.
Experiments demonstrate that RefineLab consistently narrows divergence from
expert datasets across coverage, difficulty alignment, factual fidelity, and
distractor quality. RefineLab pioneers a scalable, customizable path to
reproducible dataset design, with broad implications for LLM evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06530v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06530v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs
must be both trustworthy and helpful. However, these goals often conflict. We
propose priority alignment, a new alignment paradigm that enforces a strict
"trustworthy-before-helpful" ordering: optimization of helpfulness is
conditioned on first meeting trustworthy thresholds (e.g., harmlessness or
honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully
unsupervised framework that generates diverse responses, self-evaluates them
and refines them by the model itself, and applies dual-criterion denoising to
remove inconsistency and control variance. From this, SPA constructs
lexicographically ordered preference pairs and fine-tunes the model using an
uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap
decisions. Experiments across multiple benchmarks show that SPA improves
helpfulness without compromising safety, outperforming strong baselines while
preserving general capabilities. Our results demonstrate that SPA provides a
scalable and interpretable alignment strategy for critical LLM applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06222v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06222v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-3" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            LLM
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 58 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前大语言模型（LLM）研究的主要动态体现在几个方面：首先，针对LLM在长时间任务中的可靠性问题，研究者们开始引入符合预测（Conformal Prediction）方法来提高输出的可信度；其次，关于LLM内部机制的透明性，新的解码框架（Rep2Text）被提出，以从最后一个标记的表示中恢复完整文本；此外，LLM在软件工程中的应用，如循环不变式的生成和修复，显示出其在程序安全评估中的潜力；最后，针对语言多样性的研究也在不断推进，尤其是对尼日利亚少数语言的自然语言处理的关注。这些趋势表明，LLM的应用正在向更广泛的领域扩展，同时也在努力提升模型的可靠性和透明度。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have recently emerged as planners for
language-instructed agents, generating sequences of actions to accomplish
natural language tasks. However, their reliability remains a challenge,
especially in long-horizon tasks, since they often produce overconfident yet
wrong outputs. Conformal Prediction (CP) has been leveraged to address this
issue by wrapping LLM outputs into prediction sets that contain the correct
action with a user-defined confidence. When the prediction set is a singleton,
the planner executes that action; otherwise, it requests help from a user. This
has led to LLM-based planners that can ensure plan correctness with a
user-defined probability. However, as LLMs are trained in an
uncertainty-agnostic manner, without awareness of prediction sets, they tend to
produce unnecessarily large sets, particularly at higher confidence levels,
resulting in frequent human interventions limiting autonomous deployment. To
address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first
CP-aware finetuning framework for LLM-based planners that explicitly reduces
prediction-set size and, in turn, the need for user interventions. We evaluate
our approach on multiple language-instructed robot planning problems and show
consistent improvements over uncertainty-aware and uncertainty-agnostic
finetuning baselines in terms of prediction-set size, and help rates. Finally,
we demonstrate robustness of our method to out-of-distribution scenarios in
hardware experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06575v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06575v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rep2Text: Decoding Full Text from a Single LLM Token Representation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) have achieved remarkable progress across diverse
tasks, yet their internal mechanisms remain largely opaque. In this work, we
address a fundamental question: to what extent can the original input text be
recovered from a single last-token representation within an LLM? We propose
Rep2Text, a novel framework for decoding full text from last-token
representations. Rep2Text employs a trainable adapter that projects a target
model's internal representations into the embedding space of a decoding
language model, which then autoregressively reconstructs the input text.
Experiments on various model combinations (Llama-3.1-8B, Gemma-7B,
Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the
information in 16-token sequences can be recovered from this compressed
representation while maintaining strong semantic integrity and coherence.
Furthermore, our analysis reveals an information bottleneck effect: longer
sequences exhibit decreased token-level recovery while preserving strong
semantic integrity. Besides, our framework also demonstrates robust
generalization to out-of-distribution medical data.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06571v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06571v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM For Loop Invariant Generation and Fixing: How Far Are We?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A loop invariant is a property of a loop that remains true before and after
each execution of the loop. The identification of loop invariants is a critical
step to support automated program safety assessment. Recent advancements in
Large Language Models (LLMs) have demonstrated potential in diverse software
engineering (SE) and formal verification tasks. However, we are not aware of
the performance of LLMs to infer loop invariants. We report an empirical study
of both open-source and closed-source LLMs of varying sizes to assess their
proficiency in inferring inductive loop invariants for programs and in fixing
incorrect invariants. Our findings reveal that while LLMs exhibit some utility
in inferring and repairing loop invariants, their performance is substantially
enhanced when supplemented with auxiliary information such as domain knowledge
and illustrative examples. LLMs achieve a maximum success rate of 78\% in
generating, but are limited to 16\% in repairing the invariant.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06552v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06552v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Nigeria is the most populous country in Africa with a population of more than
200 million people. More than 500 languages are spoken in Nigeria and it is one
of the most linguistically diverse countries in the world. Despite this,
natural language processing (NLP) research has mostly focused on the following
four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the
languages spoken in Nigeria). This is in part due to the unavailability of
textual data in these languages to train and apply NLP algorithms. In this
work, we introduce ibom -- a dataset for machine translation and topic
classification in four Coastal Nigerian languages from the Akwa Ibom State
region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in
Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus
on extending Flores-200 benchmark to these languages, and further align the
translated texts with topic labels based on SIB-200 classification dataset. Our
evaluation shows that current LLMs perform poorly on machine translation for
these languages in both zero-and-few shot settings. However, we find the
few-shot samples to steadily improve topic classification with more shots.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06531v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06531v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mathematical reasoning requires abstracting symbolic rules from visual
patterns -- inferring the infinite from the finite. We investigate whether
multimodal AI systems possess this capability through FractalBench, a benchmark
evaluating fractal program synthesis from images. Fractals provide ideal test
cases: Iterated Function Systems with only a few contraction maps generate
complex self-similar patterns through simple recursive rules, requiring models
to bridge visual perception with mathematical abstraction. We evaluate four
leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL
-- on 12 canonical fractals. Models must generate executable Python code
reproducing the fractal, enabling objective evaluation. Results reveal a
striking disconnect: 76% generate syntactically valid code but only 4% capture
mathematical structure. Success varies systematically -- models handle
geometric transformations (Koch curves: 17-21%) but fail at branching recursion
(trees: <2%), revealing fundamental gaps in mathematical abstraction.
FractalBench provides a contamination-resistant diagnostic for
visual-mathematical reasoning and is available at
https://github.com/NaiveNeuron/FractalBench
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06522v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06522v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Artificial Neural Networks, the building blocks of AI, were inspired by the
human brain's network of neurons. Over the years, these networks have evolved
to replicate the complex capabilities of the brain, allowing them to handle
tasks such as image and language processing. In the realm of Large Language
Models, there has been a keen interest in making the language learning process
more akin to that of humans. While neuroscientific research has shown that
different grammatical categories are processed by different neurons in the
brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we
identify the most important neurons associated with the prediction of words
belonging to different part-of-speech tags. Using the achieved knowledge, we
train a classifier on a dataset, which shows that the activation patterns of
these key neurons can reliably predict part-of-speech tags on fresh data. The
results suggest the presence of a subspace in LLMs focused on capturing
part-of-speech tag concepts, resembling patterns observed in lesion studies of
the brain in neuroscience.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06519v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06519v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Realignment is a promising strategy to improve cross-lingual transfer in
multilingual language models. However, empirical results are mixed and often
unreliable, particularly for typologically distant or low-resource languages
(LRLs) compared to English. Moreover, word realignment tools often rely on
high-quality parallel data, which can be scarce or noisy for many LRLs. In this
work, we conduct an extensive empirical study to investigate whether
realignment truly benefits from using all available languages, or if
strategically selected subsets can offer comparable or even improved
cross-lingual transfer, and study the impact on LRLs. Our controlled
experiments show that realignment can be particularly effective for LRLs and
that using carefully selected, linguistically diverse subsets can match full
multilingual alignment, and even outperform it for unseen LRLs. This indicates
that effective realignment does not require exhaustive language coverage and
can reduce data collection overhead, while remaining both efficient and robust
when guided by informed language selection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06497v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06497v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Route Experts by Sequence, not by Token
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by
activating only a subset of experts per token, but the standard TopK routing
assigns the same fixed number of experts to all tokens, ignoring their varying
complexity. Prior adaptive routing methods introduce additional modules and
hyperparameters, often requiring costly retraining from scratch. We propose
Sequence-level TopK (SeqTopK), a minimal modification that shifts the expert
budget from the token level to the sequence level. By selecting the top $T
\cdot K$ experts across all $T$ tokens, SeqTopK enables end-to-end learned
dynamic allocation -- assigning more experts to difficult tokens and fewer to
easy ones -- while preserving the same overall budget. SeqTopK requires only a
few lines of code, adds less than 1% overhead, and remains fully compatible
with pretrained MoE models. Experiments across math, coding, law, and writing
show consistent improvements over TopK and prior parameter-free adaptive
methods, with gains that become substantially larger under higher sparsity (up
to 16.9%). These results highlight SeqTopK as a simple, efficient, and scalable
routing strategy, particularly well-suited for the extreme sparsity regimes of
next-generation LLMs. Code is available at
https://github.com/Y-Research-SBU/SeqTopK.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06494v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06494v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Enterprises often maintain multiple databases for storing critical business
data in siloed systems, resulting in inefficiencies and challenges with data
interoperability. A key to overcoming these challenges lies in integrating
disparate data sources, enabling businesses to unlock the full potential of
their data. Our work presents a novel approach for integrating multiple
databases using knowledge graphs, focusing on the application of large language
models as semantic agents for mapping and connecting structured data across
systems by leveraging existing vocabularies. The proposed methodology
introduces a semantic layer above tables in relational databases, utilizing a
system comprising multiple LLM agents that map tables and columns to Schema.org
terms. Our approach achieves a mapping accuracy of over 90% in multiple
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06455v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06455v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FLEX: Continuous Agent Evolution via Forward Learning from Experience
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Autonomous agents driven by Large Language Models (LLMs) have revolutionized
reasoning and problem-solving but remain static after training, unable to grow
with experience as intelligent beings do during deployment. We introduce
Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that
enables LLM agents to continuously evolve through accumulated experience.
Specifically, FLEX cultivates scalable and inheritable evolution by
constructing a structured experience library through continual reflection on
successes and failures during interaction with the environment. FLEX delivers
substantial improvements on mathematical reasoning, chemical retrosynthesis,
and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14%
on ProteinGym). We further identify a clear scaling law of experiential growth
and the phenomenon of experience inheritance across agents, marking a step
toward scalable and inheritable continuous agent evolution. Project Page:
https://flex-gensi-thuair.github.io.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06449v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06449v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this work, we study the risks of collective financial fraud in large-scale
multi-agent systems powered by large language model (LLM) agents. We
investigate whether agents can collaborate in fraudulent behaviors, how such
collaboration amplifies risks, and what factors influence fraud success. To
support this research, we present MultiAgentFraudBench, a large-scale benchmark
for simulating financial fraud scenarios based on realistic online
interactions. The benchmark covers 28 typical online fraud scenarios, spanning
the full fraud lifecycle across both public and private domains. We further
analyze key factors affecting fraud success, including interaction depth,
activity level, and fine-grained collaboration failure modes. Finally, we
propose a series of mitigation strategies, including adding content-level
warnings to fraudulent posts and dialogues, using LLMs as monitors to block
potentially malicious agents, and fostering group resilience through
information sharing at the societal level. Notably, we observe that malicious
agents can adapt to environmental interventions. Our findings highlight the
real-world risks of multi-agent financial fraud and suggest practical measures
for mitigating them. Code is available at
https://github.com/zheng977/MutiAgent4Fraud.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06448v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06448v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper proposes SR-KI, a novel approach for integrating real-time and
large-scale structured knowledge bases (KBs) into large language models (LLMs).
SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder,
and injects them into LLMs' KV cache. Building on this representation, we
employ a two-stage training paradigm: first locating a dedicated retrieval
layer within the LLM, and then applying an attention-based loss at this layer
to explicitly supervise attention toward relevant KB entries. Unlike
traditional retrieval-augmented generation methods that rely heavily on the
performance of external retrievers and multi-stage pipelines, SR-KI supports
end-to-end inference by performing retrieval entirely within the models latent
space. This design enables efficient compression of injected knowledge and
facilitates dynamic knowledge updates. Comprehensive experiments demonstrate
that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single
A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98%
Recall@10 on the best-performing task and exceeding 88% on average across all
tasks. Task performance on question answering and KB ID generation also
demonstrates that SR-KI maintains strong performance while achieving up to
99.75% compression of the injected KBs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06446v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06446v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chain-of-thought (CoT) prompting enables Large Language Models to solve
complex problems, but deploying these models safely requires reliable
confidence estimates, a capability where existing methods suffer from poor
calibration and severe overconfidence on incorrect predictions. We propose
Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that
combines topological analysis with Dirichlet-based uncertainty quantification
to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT
as a vector in high-dimensional space and extracts eight topological risk
features capturing the geometric structure of reasoning distributions: tighter,
more coherent clusters indicate higher confidence while dispersed, inconsistent
paths signal uncertainty. We evaluate EDTR against three state-of-the-art
calibration methods across four diverse reasoning benchmarks spanning
olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense
reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training,
talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better
calibration than competing methods with an average ECE of 0.287 and the best
overall composite score of 0.672, while notably achieving perfect accuracy on
AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where
baselines exhibit severe overconfidence. Our work provides a geometric
framework for understanding and quantifying uncertainty in multi-step LLM
reasoning, enabling more reliable deployment where calibrated confidence
estimates are essential.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06437v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06437v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Background: Large Language Models emerged with the potential of provoking a
revolution in software development (e.g., automating processes, workforce
transformation). Although studies have started to investigate the perceived
impact of LLMs for software development, there is a need for empirical studies
to comprehend how to balance forward and backward effects of using LLMs.
Objective: We investigated how LLMs impact software development and how to
manage the impact from a software developer's perspective. Method: We conducted
22 interviews with software practitioners across 3 rounds of data collection
and analysis, between October (2024) and September (2025). We employed
socio-technical grounded theory (STGT) for data analysis to rigorously analyse
interview participants' responses. Results: We identified the benefits (e.g.,
maintain software development flow, improve developers' mental model, and
foster entrepreneurship) and disadvantages (e.g., negative impact on
developers' personality and damage to developers' reputation) of using LLMs at
individual, team, organisation, and society levels; as well as best practices
on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that
software practitioners, teams, and organisations face in working with LLMs. Our
findings are particularly useful for software team leaders and IT managers to
assess the viability of LLMs within their specific context.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06428v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06428v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models
tend to agree with users' incorrect beliefs and follow misinformation rather
than maintain independent reasoning. This behavior undermines model reliability
and poses societal risks. Mitigating LRM sycophancy requires monitoring how
this sycophancy emerges during the reasoning trajectory; however, current
methods mainly focus on judging based on final answers and correcting them,
without understanding how sycophancy develops during reasoning processes. To
address this limitation, we propose MONICA, a novel Monitor-guided Calibration
framework that monitors and mitigates sycophancy during model inference at the
level of reasoning steps, without requiring the model to finish generating its
complete answer. MONICA integrates a sycophantic monitor that provides
real-time monitoring of sycophantic drift scores during response generation
with a calibrator that dynamically suppresses sycophantic behavior when scores
exceed predefined thresholds. Extensive experiments across 12 datasets and 3
LRMs demonstrate that our method effectively reduces sycophantic behavior in
both intermediate reasoning steps and final answers, yielding robust
performance improvements.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06419v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06419v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AUTO-Explorer: Automated Data Collection for GUI Agent
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in GUI agents have significantly expanded their ability
to interpret natural language commands to manage software interfaces. However,
acquiring GUI data remains a significant challenge. Existing methods often
involve designing automated agents that browse URLs from the Common Crawl,
using webpage HTML to collect screenshots and corresponding annotations,
including the names and bounding boxes of UI elements. However, this method is
difficult to apply to desktop software or some newly launched websites not
included in the Common Crawl. While we expect the model to possess strong
generalization capabilities to handle this, it is still crucial for
personalized scenarios that require rapid and perfect adaptation to new
software or websites. To address this, we propose an automated data collection
method with minimal annotation costs, named Auto-Explorer. It incorporates a
simple yet effective exploration mechanism that autonomously parses and
explores GUI environments, gathering data efficiently. Additionally, to assess
the quality of exploration, we have developed the UIXplore benchmark. This
benchmark creates environments for explorer agents to discover and save
software states. Using the data gathered, we fine-tune a multimodal large
language model (MLLM) and establish a GUI element grounding testing set to
evaluate the effectiveness of the exploration strategies. Our experiments
demonstrate the superior performance of Auto-Explorer, showing that our method
can quickly enhance the capabilities of an MLLM in explored software.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06417v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06417v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The soft-thinking paradigm for Large Language Model (LLM) reasoning can
outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in
some scenarios, underscoring its research and application value. However, while
the discrete-token CoT reasoning pattern can be reinforced through policy
optimization algorithms such as group relative policy optimization (GRPO),
extending the soft-thinking pattern with Reinforcement Learning (RL) remains
challenging. This difficulty stems from the complexities of injecting
stochasticity into soft-thinking tokens and updating soft-thinking policies
accordingly. As a result, previous attempts to combine soft-thinking with GRPO
typically underperform their discrete-token GRPO counterparts. To fully unlock
the potential of soft-thinking, this paper presents a novel policy optimization
algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning
pattern. SofT-GRPO injects the Gumbel noise into logits, employs the
Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained
embedding space, and leverages the reparameterization trick in policy gradient.
We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and
results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly
outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while
exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes
and weights are available on https://github.com/zz1358m/SofT-GRPO-master
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06411v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06411v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient LLM Safety Evaluation through Multi-Agent Debate
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Safety evaluation of large language models (LLMs) increasingly relies on
LLM-as-a-Judge frameworks, but the high cost of frontier models limits
scalability. We propose a cost-efficient multi-agent judging framework that
employs Small Language Models (SLMs) through structured debates among critic,
defender, and judge agents. To rigorously assess safety judgments, we construct
HAJailBench, a large-scale human-annotated jailbreak benchmark comprising
12,000 adversarial interactions across diverse attack methods and target
models. The dataset provides fine-grained, expert-labeled ground truth for
evaluating both safety robustness and judge reliability. Our SLM-based
framework achieves agreement comparable to GPT-4o judges on HAJailBench while
substantially reducing inference cost. Ablation results show that three rounds
of debate yield the optimal balance between accuracy and efficiency. These
findings demonstrate that structured, value-aligned debate enables SLMs to
capture semantic nuances of jailbreak attacks and that HAJailBench offers a
reliable foundation for scalable LLM safety evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06396v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06396v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Optimization of offensive content moderation models for different types of
hateful messages is typically achieved through continued pre-training or
fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly
address explicit hate toward protected groups and often overlook implicit or
indirect hate, such as demeaning comparisons, calls for exclusion or violence,
and subtle discriminatory language that still causes harm. While explicit hate
can often be captured through surface features, implicit hate requires deeper,
full-model semantic processing. In this work, we question the need for repeated
fine-tuning and analyze the role of HatePrototypes, class-level vector
representations derived from language models optimized for hate speech
detection and safety moderation. We find that these prototypes, built from as
few as 50 examples per class, enable cross-task transfer between explicit and
implicit hate, with interchangeable prototypes across benchmarks. Moreover, we
show that parameter-free early exiting with prototypes is effective for both
hate types. We release the code, prototype resources, and evaluation scripts to
support future research on efficient and transferable hate speech detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06391v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06391v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have rapidly advanced and are widely adopted
across diverse fields. Due to the substantial computational cost and data
requirements of training from scratch, many developers choose to fine-tune or
modify existing open-source models. While most adhere to open-source licenses,
some falsely claim original training despite clear derivation from public
models. This raises pressing concerns about intellectual property protection
and highlights the need for reliable methods to verify model provenance. In
this paper, we propose GhostSpec, a lightweight yet effective method for
verifying LLM lineage without access to training data or modification of model
behavior. Our approach constructs compact and robust fingerprints by applying
singular value decomposition (SVD) to invariant products of internal attention
weight matrices, effectively capturing the structural identity of a model.
Unlike watermarking or output-based methods, GhostSpec is fully data-free,
non-invasive, and computationally efficient. It demonstrates strong robustness
to sequential fine-tuning, pruning, block expansion, and even adversarial
transformations. Extensive experiments show that GhostSpec can reliably trace
the lineage of transformed models with minimal overhead. By offering a
practical solution for model verification and reuse tracking, our method
contributes to the protection of intellectual property and fosters a
transparent, trustworthy ecosystem for large-scale language models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06390v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06390v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of reasoning tasks. Recent methods have further improved LLM
performance in complex mathematical reasoning. However, when extending these
methods beyond the domain of mathematical reasoning to tasks involving complex
domain-specific knowledge, we observe a consistent failure of LLMs to generate
novel insights during the reflection stage. Instead of conducting genuine
cognitive refinement, the model tends to mechanically reiterate earlier
reasoning steps without introducing new information or perspectives, a
phenomenon referred to as "Echo Reflection". We attribute this behavior to two
key defects: (1) Uncontrollable information flow during response generation,
which allows premature intermediate thoughts to propagate unchecked and distort
final decisions; (2) Insufficient exploration of internal knowledge during
reflection, leading to repeating earlier findings rather than generating new
cognitive insights. Building on these findings, we proposed a novel
reinforcement learning method termed Adaptive Entropy Policy Optimization
(AEPO). Specifically, the AEPO framework consists of two major components: (1)
Reflection-aware Information Filtration, which quantifies the cognitive
information flow and prevents the final answer from being affected by earlier
bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically
balances exploration and exploitation across different reasoning stages,
promoting both reflective diversity and answer correctness. Extensive
experiments demonstrate that AEPO consistently achieves state-of-the-art
performance over mainstream reinforcement learning baselines across diverse
benchmarks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06380v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06380v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have made rapid progress in reasoning, question
answering, and professional applications; however, their true capabilities
remain difficult to evaluate using existing benchmarks. Current datasets often
focus on simplified tasks or artificial scenarios, overlooking long-tail
knowledge and the complexities of real-world applications. To bridge this gap,
we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic
professional forums across 20 academic and industrial fields, covering 502
tasks grounded in practical expertise. LPFQA introduces four key innovations:
fine-grained evaluation dimensions that target knowledge depth, reasoning,
terminology comprehension, and contextual analysis; a hierarchical difficulty
structure that ensures semantic clarity and unique answers; authentic
professional scenario modeling with realistic user personas; and
interdisciplinary knowledge integration across diverse domains. We evaluated 12
mainstream LLMs on LPFQA and observed significant performance disparities,
especially in specialized reasoning tasks. LPFQA provides a robust, authentic,
and discriminative benchmark for advancing LLM evaluation and guiding future
model development.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06346v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06346v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Designing high-performance kernels requires expert-level tuning and a deep
understanding of hardware characteristics. Recent advances in large language
models (LLMs) have enabled automated kernel generation, yet most existing
systems rely solely on correctness or execution time feedback, lacking the
ability to reason about low-level performance bottlenecks. In this paper, we
introduce PRAGMA, a profile-guided AI kernel generation framework that
integrates execution feedback and fine-grained hardware profiling into the
reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks,
preserve historical best versions, and iteratively refine code quality. We
evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show
that PRAGMA consistently outperforms baseline AIKG without profiling enabled
and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on
CPU and GPU platforms, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06345v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06345v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeSense:Making Large Language Models Proficient in Time-Series Analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In the time-series domain, an increasing number of works combine text with
temporal data to leverage the reasoning capabilities of large language models
(LLMs) for various downstream time-series understanding tasks. This enables a
single model to flexibly perform tasks that previously required specialized
models for each domain. However, these methods typically rely on text labels
for supervision during training, biasing the model toward textual cues while
potentially neglecting the full temporal features. Such a bias can lead to
outputs that contradict the underlying time-series context. To address this
issue, we construct the EvalTS benchmark, comprising 10 tasks across three
difficulty levels, from fundamental temporal pattern recognition to complex
real-world reasoning, to evaluate models under more challenging and realistic
scenarios. We also propose TimeSense, a multimodal framework that makes LLMs
proficient in time-series analysis by balancing textual reasoning with a
preserved temporal sense. TimeSense incorporates a Temporal Sense module that
reconstructs the input time-series within the model's context, ensuring that
textual reasoning is grounded in the time-series dynamics. Moreover, to enhance
spatial understanding of time-series data, we explicitly incorporate
coordinate-based positional embeddings, which provide each time point with
spatial context and enable the model to capture structural dependencies more
effectively. Experimental results demonstrate that TimeSense achieves
state-of-the-art performance across multiple tasks, and it particularly
outperforms existing methods on complex multi-dimensional time-series reasoning
tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06344v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06344v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Evaluating semantic tables interpretation (STI) systems, (particularly, those
based on Large Language Models- LLMs) especially in domain-specific contexts
such as the security domain, depends heavily on the dataset. However, in the
security domain, tabular datasets for state-of-the-art are not publicly
available. In this paper, we introduce Secu-Table dataset, composed of more
than 1500 tables with more than 15k entities constructed using security data
extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness
Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic
Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES
CSKG). Along with the dataset, all the code is publicly released. This dataset
is made available to the research community in the context of the SemTab
challenge on Tabular to Knowledge Graph Matching. This challenge aims to
evaluate the performance of several STI based on open source LLMs. Preliminary
evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and
Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source
LLM.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06301v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06301v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Financial documents like earning reports or balance sheets often involve long
tables and multi-page reports. Large language models have become a new tool to
help numerical reasoning and understanding these documents. However, prompt
quality can have a major effect on how well LLMs perform these financial
reasoning tasks. Most current methods tune prompts on fixed datasets of
financial text or tabular data, which limits their ability to adapt to new
question types or document structures, or they involve costly and manually
labeled/curated dataset to help build the prompts. We introduce a
self-improving prompt framework driven by data-augmented optimization. In this
closed-loop process, we generate synthetic financial tables and document
excerpts, verify their correctness and robustness, and then update the prompt
based on the results. Specifically, our framework combines a synthetic data
generator with verifiers and a prompt optimizer, where the generator produces
new examples that exposes weaknesses in the current prompt, the verifiers check
the validity and robustness of the produced examples, and the optimizer
incrementally refines the prompt in response. By iterating these steps in a
feedback cycle, our method steadily improves prompt accuracy on financial
reasoning tasks without needing external labels. Evaluation on DocMath-Eval
benchmark demonstrates that our system achieves higher performance in both
accuracy and robustness than standard prompt methods, underscoring the value of
incorporating synthetic data generation into prompt learning for financial
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06292v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06292v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Organizations are increasingly exploring delegation of screening and
negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is
constrained by governance: preventing unauthorized commitments, ensuring
sufficient information before bargaining, and maintaining effective human
oversight and auditability. Prior work on large language model negotiation
largely emphasizes autonomous bargaining between agents and omits practical
needs such as staged information gathering, explicit authorization boundaries,
and systematic feedback integration. We propose GAIA, a governance-first
framework for LLM-human agency in B2B negotiation and screening. GAIA defines
three essential roles - Principal (human), Delegate (LLM agent), and
Counterparty - with an optional Critic to enhance performance, and organizes
interactions through three mechanisms: information-gated progression that
separates screening from negotiation; dual feedback integration that combines
AI critique with lightweight human corrections; and authorization boundaries
with explicit escalation paths. Our contributions are fourfold: (1) a formal
governance framework with three coordinated mechanisms and four safety
invariants for delegation with bounded authorization; (2) information-gated
progression via task-completeness tracking (TCI) and explicit state transitions
that separate screening from commitment; (3) dual feedback integration that
blends Critic suggestions with human oversight through parallel learning
channels; and (4) a hybrid validation blueprint that combines automated
protocol metrics with human judgment of outcomes and safety. By bridging theory
and practice, GAIA offers a reproducible specification for safe, efficient, and
accountable AI delegation that can be instantiated across procurement, real
estate, and staffing workflows.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06262v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06262v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) are increasingly used as behavioral proxies for
self-interested travelers in agent-based traffic models. Although more flexible
and generalizable than conventional models, the practical use of these
approaches remains limited by scalability due to the cost of calling one LLM
for every traveler. Moreover, it has been found that LLM agents often make
opaque choices and produce unstable day-to-day dynamics. To address these
challenges, we propose to model each homogeneous traveler group facing the same
decision context with a single representative LLM agent who behaves like the
population's average, maintaining and updating a mixed strategy over routes
that coincides with the group's aggregate flow proportions. Each day, the LLM
reviews the travel experience and flags routes with positive reinforcement that
they hope to use more often, and an interpretable update rule then converts
this judgment into strategy adjustments using a tunable (progressively
decaying) step size. The representative-agent design improves scalability,
while the separation of reasoning from updating clarifies the decision logic
while stabilizing learning. In classic traffic assignment settings, we find
that the proposed approach converges rapidly to the user equilibrium. In richer
settings with income heterogeneity, multi-criteria costs, and multi-modal
choices, the generated dynamics remain stable and interpretable, reproducing
plausible behavioral patterns well-documented in psychology and economics, for
example, the decoy effect in toll versus non-toll road selection, and higher
willingness-to-pay for convenience among higher-income travelers when choosing
between driving, transit, and park-and-ride options.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06260v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06260v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Mixtures of SubExperts for Large Language Continual Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Adapting Large Language Models (LLMs) to a continuous stream of tasks is a
critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT)
methods have become a standard for this, they face a fundamental dilemma in
continual learning. Reusing a single set of PEFT parameters for new tasks often
leads to catastrophic forgetting of prior knowledge. Conversely, allocating
distinct parameters for each task prevents forgetting but results in a linear
growth of the model's size and fails to facilitate knowledge transfer between
related tasks. To overcome these limitations, we propose a novel adaptive PEFT
method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel
continual learning framework designed for minimal forgetting and efficient
scalability. MoSEs integrate a sparse Mixture of SubExperts into the
transformer layers, governed by a task-specific routing mechanism. This
architecture allows the model to isolate and protect knowledge within dedicated
SubExperts, thereby minimizing parameter interference and catastrophic
forgetting. Crucially, the router can adaptively select and combine previously
learned sparse parameters for new tasks, enabling effective knowledge transfer
while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs
on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that
MoSEs significantly outperform conventional continual learning approaches in
both knowledge retention and scalability to new tasks, achieving
state-of-the-art performance with substantial memory and computational savings.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06237v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06237v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Pre-trained models for natural language inference (NLI) often achieve high
performance on benchmark datasets by using spurious correlations, or dataset
artifacts, rather than understanding language touches such as negation. In this
project, we investigate the performance of an ELECTRA-small model fine-tuned on
the Stanford Natural Language Inference (SNLI) dataset, focusing on its
handling of negation. Through analysis, we identify that the model struggles
with correctly classifying examples containing negation. To address this, we
augment the training data with contrast sets and adversarial examples
emphasizing negation. Our results demonstrate that this targeted data
augmentation improves the model's accuracy on negation-containing examples
without adversely affecting overall performance, therefore mitigating the
identified dataset artifact.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06234v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06234v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scaling Laws and In-Context Learning: A Unified Theoretical Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In-context learning (ICL) enables large language models to adapt to new tasks
from demonstrations without parameter updates. Despite extensive empirical
studies, a principled understanding of ICL emergence at scale remains more
elusive. We present a unified theoretical framework connecting scaling laws to
ICL emergence in transformers. Our analysis establishes that ICL performance
follows power-law relationships with model depth $L$, width $d$, context length
$k$, and training data $D$, with exponents determined by task structure. We
show that under specific conditions, transformers implement gradient-based
metalearning in their forward pass, with an effective learning rate
$\eta_{\text{eff}} = \Theta(1/\sqrt{Ld})$. We demonstrate sharp phase
transitions at critical scales and derive optimal depth-width allocations
favoring $L^* \propto N^{2/3}$, $d^* \propto N^{1/3}$ for the fixed parameter
budget $N = Ld$. Systematic experiments on synthetic tasks validate our
predictions, with measured scaling exponents closely matching theory. This work
provides both necessary and sufficient conditions for the emergence of ICLs and
establishes fundamental computational limits on what transformers can learn
in-context.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06232v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06232v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Discharge medication recommendation plays a critical role in ensuring
treatment continuity, preventing readmission, and improving long-term
management for patients with chronic metabolic diseases. This paper present an
overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop
state-of-the-art approaches for automatically recommending appro-priate
discharge medications using real-world Chinese EHR data. For this task, we
constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified
hospitalization records from 3,190 patients in China. This task is challenging
due to multi-label nature of medication recommendation, het-erogeneous clinical
text, and patient-specific variability in treatment plans. A total of 526 teams
registered, with 167 and 95 teams submitting valid results to the Phase A and
Phase B leaderboards, respectively. The top-performing team achieved the
highest overall performance on the final test set, with a Jaccard score of
0.5102, F1 score of 0.6267, demonstrating the potential of advanced large
language model (LLM)-based ensemble systems. These re-sults highlight both the
promise and remaining challenges of applying LLMs to medication recommendation
in Chinese EHRs. The post-evaluation phase remains open at
https://tianchi.aliyun.com/competition/entrance/532411/.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06230v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06230v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Assertion-Aware Test Code Summarization with Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unit tests often lack concise summaries that convey test intent, especially
in auto-generated or poorly documented codebases. Large Language Models (LLMs)
offer a promising solution, but their effectiveness depends heavily on how they
are prompted. Unlike generic code summarization, test-code summarization poses
distinct challenges because test methods validate expected behavior through
assertions rather than im- plementing functionality. This paper presents a new
benchmark of 91 real-world Java test cases paired with developer-written
summaries and conducts a controlled ablation study to investigate how test
code-related components-such as the method under test (MUT), assertion
messages, and assertion semantics-affect the performance of LLM-generated test
summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and
Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU,
ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation.
Results show that prompting with as- sertion semantics improves summary quality
by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while
requiring fewer input tokens. Codex and Qwen-Coder achieve the highest
alignment with human-written summaries, while DeepSeek underperforms despite
high lexical overlap. The replication package is publicly available at
https://doi.org/10. 5281/zenodo.17067550
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06227v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06227v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Detecting Alzheimer's Disease (AD) from narrative transcripts remains a
challenging task for large language models (LLMs), particularly under
out-of-distribution (OOD) and data-scarce conditions. While in-context learning
(ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL
approaches often suffer from task recognition failure, suboptimal demonstration
selection, and misalignment between label words and task objectives, issues
that are amplified in clinical domains like AD detection. We propose Explicit
Knowledge In-Context Learners (EK-ICL), a novel framework that integrates
structured explicit knowledge to enhance reasoning stability and task alignment
in ICL. EK-ICL incorporates three knowledge components: confidence scores
derived from small language models (SLMs) to ground predictions in
task-relevant patterns, parsing feature scores to capture structural
differences and improve demo selection, and label word replacement to resolve
semantic misalignment with LLM priors. In addition, EK-ICL employs a
parsing-based retrieval strategy and ensemble prediction to mitigate the
effects of semantic homogeneity in AD transcripts. Extensive experiments across
three AD datasets demonstrate that EK-ICL significantly outperforms
state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that
ICL performance in AD detection is highly sensitive to the alignment of label
semantics and task-specific context, underscoring the importance of explicit
knowledge in clinical reasoning under low-resource conditions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06215v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06215v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid expansion of the Internet of Things (IoT) is reshaping
communication and operational practices across industries, but it also broadens
the attack surface and increases susceptibility to security breaches.
Artificial Intelligence has become a valuable solution in securing IoT
networks, with Large Language Models (LLMs) enabling automated attack behavior
analysis and mitigation suggestion in Network Intrusion Detection Systems
(NIDS). Despite advancements, the use of LLMs in such systems further expands
the attack surface, putting entire networks at risk by introducing
vulnerabilities such as prompt injection and data poisoning. In this work, we
attack an LLM-based IoT attack analysis and mitigation framework to test its
adversarial robustness. We construct an attack description dataset and use it
in a targeted data poisoning attack that applies word-level, meaning-preserving
perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge
base of the framework. We then compare pre-attack and post-attack mitigation
responses from the target model, ChatGPT-5 Thinking, to measure the impact of
the attack on model performance, using an established evaluation rubric
designed for human experts and judge LLMs. Our results show that small
perturbations degrade LLM performance by weakening the linkage between observed
network traffic features and attack behavior, and by reducing the specificity
and practicality of recommended mitigations for resource-constrained devices.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06212v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06212v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Solving complex tasks usually requires LLMs to generate long multi-step
reasoning chains. Previous work has shown that verifying the correctness of
individual reasoning steps can further improve the performance and efficiency
of LLMs on such tasks and enhance solution interpretability. However, existing
verification approaches, such as Process Reward Models (PRMs), are either
computationally expensive, limited to specific domains, or require large-scale
human or model-generated annotations. Thus, we propose a lightweight
alternative for step-level reasoning verification based on data-driven
uncertainty scores. We train transformer-based uncertainty quantification heads
(UHeads) that use the internal states of a frozen LLM to estimate the
uncertainty of its reasoning steps during generation. The approach is fully
automatic: target labels are generated either by another larger LLM (e.g.,
DeepSeek R1) or in a self-supervised manner by the original model itself.
UHeads are both effective and lightweight, containing less than 10M parameters.
Across multiple domains, including mathematics, planning, and general knowledge
question answering, they match or even surpass the performance of PRMs that are
up to 810x larger. Our findings suggest that the internal states of LLMs encode
their uncertainty and can serve as reliable signals for reasoning verification,
offering a promising direction toward scalable and generalizable introspective
LLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06209v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06209v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid proliferation of Internet of Things (IoT) devices has transformed
numerous industries by enabling seamless connectivity and data-driven
automation. However, this expansion has also exposed IoT networks to
increasingly sophisticated security threats, including adversarial attacks
targeting artificial intelligence (AI) and machine learning (ML)-based
intrusion detection systems (IDS) to deliberately evade detection, induce
misclassification, and systematically undermine the reliability and integrity
of security defenses. To address these challenges, we propose a novel
adversarial detection model that enhances the robustness of IoT IDS against
adversarial attacks through SHapley Additive exPlanations (SHAP)-based
fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints
from network traffic features, enabling the IDS to reliably distinguish between
clean and adversarially perturbed inputs. By capturing subtle attribution
patterns, the model becomes more resilient to evasion attempts and adversarial
manipulations. We evaluated the model on a standard IoT benchmark dataset,
where it significantly outperformed a state-of-the-art method in detecting
adversarial attacks. In addition to enhanced robustness, this approach improves
model transparency and interpretability, thereby increasing trust in the IDS
through explainable AI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06197v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06197v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Dataforge: A Data Agent Platform for Autonomous Data Engineering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The growing demand for AI applications in fields such as materials discovery,
molecular modeling, and climate science has made data preparation an important
but labor-intensive step. Raw data from diverse sources must be cleaned,
normalized, and transformed to become AI-ready, while effective feature
transformation and selection are essential for efficient training and
inference. To address the challenges of scalability and expertise dependence,
we present Data Agent, a fully autonomous system specialized for tabular data.
Leveraging large language model (LLM) reasoning and grounded validation, Data
Agent automatically performs data cleaning, hierarchical routing, and
feature-level optimization through dual feedback loops. It embodies three core
principles: automatic, safe, and non-expert friendly, which ensure end-to-end
reliability without human supervision. This demo showcases the first practical
realization of an autonomous Data Agent, illustrating how raw data can be
transformed "From Data to Better Data."
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06185v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06185v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players
conceal their identities and deliberately mislead others, making hidden-role
inference a central and demanding task. Accurate role identification, which
forms the basis of an agent's belief state, is therefore the keystone for both
human and AI performance. We introduce CSP4SDG, a probabilistic,
constraint-satisfaction framework that analyses gameplay objectively. Game
events and dialogue are mapped to four linguistically-agnostic constraint
classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune
impossible role assignments, while weighted soft constraints score the
remainder; information-gain weighting links each hypothesis to its expected
value under entropy reduction, and a simple closed-form scoring rule guarantees
that truthful assertions converge to classical hard logic with minimum error.
The resulting posterior over roles is fully interpretable and updates in real
time. Experiments on three public datasets show that CSP4SDG (i) outperforms
LLM-based baselines in every inference scenario, and (ii) boosts LLMs when
supplied as an auxiliary "reasoning tool." Our study validates that principled
probabilistic reasoning with information theory is a scalable alternative-or
complement-to heavy-weight neural models for SDGs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06175v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06175v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid progress of large language models (LLMs) has advanced numerous
applications, yet efficient single-batch inference remains vital for on-device
intelligence. While FPGAs offer fine-grained data control and high energy
efficiency, recent GPU optimizations have narrowed their advantage, especially
under arithmetic-based computation. To overcome this, we leverage FPGAs'
abundant on-chip memory to shift LLM inference from arithmetic- to memory-based
computation through table lookups. We present LUT-LLM, the first FPGA
accelerator enabling 1B+ LLM inference via vector-quantized memory operations.
Our analysis identifies activation-weight co-quantization as the most effective
scheme, supported by (1) bandwidth-aware parallel centroid search, (2)
efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing
data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B
model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher
energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency
gain over A100.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06174v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06174v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper presents a framework for evaluating and optimizing reasoning
consistency in Large Language Models (LLMs) via a new metric, the Alignment
Score, which quantifies the semantic alignment between model-generated
reasoning chains and human-written reference chains in Chain-of-Thought (CoT)
reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest
Alignment Score. To explain this phenomenon, we define four key error types:
logical disconnection, thematic shift, redundant reasoning, and causal
reversal, and show how each contributes to the degradation of the Alignment
Score. Building on this analysis, we further propose Semantic Consistency
Optimization Sampling (SCOS), a method that samples and favors chains with
minimal alignment errors, significantly improving Alignment Scores by an
average of 29.84% with longer reasoning chains, such as in 3-hop tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06168v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06168v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, optimizers that explicitly treat weights as matrices, rather than
flattened vectors, have demonstrated their effectiveness. This perspective
naturally leads to structured approximations of the Fisher matrix as
preconditioners, where the matrix view induces a Kronecker-factorized form that
enables memory-efficient representation. However, constructing such
approximations both efficiently and accurately remains an open challenge, since
obtaining the optimal factorization is resource-intensive and practical methods
therefore rely on heuristic design choices. In this work, we introduce a novel
approach that leverages projector-splitting integrators to construct effective
preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the
Fisher Matrix), consistently improves the Fisher matrix approximation quality.
Experiments on large language model pre-training and fine-tuning demonstrate
that DyKAF outperforms existing optimizers across a range of evaluation
metrics.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06477v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06477v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As AI moves beyond text, large language models (LLMs) increasingly power
vision, audio, and document understanding; however, their high inference costs
hinder real-time, scalable deployment. Conversely, smaller open-source models
offer cost advantages but struggle with complex or multimodal queries. We
introduce a unified, modular framework that intelligently routes each query -
textual, multimodal, or complex - to the most fitting expert model, using a
learned routing network that balances cost and quality. For vision tasks, we
employ a two-stage open-source pipeline optimized for efficiency and reviving
efficient classical vision components where they remain SOTA for sub-tasks. On
benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual
Question Answering (VQA), we match or exceed the performance of always-premium
LLM (monolithic systems with one model serving all query types) performance,
yet reduce the reliance on costly models by over 67%. With its extensible,
multi-agent orchestration, we deliver high-quality, resource-efficient AI at
scale.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06441v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06441v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Test-time Reinforcement Learning (TTRL) has shown promise in adapting
foundation models for complex tasks at test-time, resulting in large
performance improvements. TTRL leverages an elegant two-phase sampling
strategy: first, multi-sampling derives a pseudo-label via majority voting,
while subsequent downsampling and reward-based fine-tuning encourages the model
to explore and learn diverse valid solutions, with the pseudo-label modulating
the reward signal. Meanwhile, in-context learning has been widely explored at
inference time and demonstrated the ability to enhance model performance
without weight updates. However, TTRL's two-phase sampling strategy
under-utilizes contextual guidance, which can potentially improve pseudo-label
accuracy in the initial exploitation phase while regulating exploration in the
second. To address this, we propose context-guided TTRL (CG-TTRL), integrating
context dynamically into both sampling phases and propose a method for
efficient context selection for on-device applications. Our evaluations on
mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g.
additional 7% relative accuracy improvement over TTRL), while boosting
efficiency by obtaining strong performance after only a few steps of test-time
training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06430v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06430v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Numerous studies have demonstrated that the Transformer architecture
possesses the capability for in-context learning (ICL). In scenarios involving
function approximation, context can serve as a control parameter for the model,
endowing it with the universal approximation property (UAP). In practice,
context is represented by tokens from a finite set, referred to as a
vocabulary, which is the case considered in this paper, \emph{i.e.}, vocabulary
in-context learning (VICL). We demonstrate that VICL in single-layer
Transformers, without positional encoding, does not possess the UAP; however,
it is possible to achieve the UAP when positional encoding is included. Several
sufficient conditions for the positional encoding are provided. Our findings
reveal the benefits of positional encoding from an approximation theory
perspective in the context of ICL.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06376v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06376v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Drug-target interaction (DTI) prediction is of great significance for drug
discovery and drug repurposing. With the accumulation of a large volume of
valuable data, data-driven methods have been increasingly harnessed to predict
DTIs, reducing costs across various dimensions. Therefore, this paper proposes
a $\textbf{L}$arge $\textbf{L}$anguage $\textbf{M}$odel and
$\textbf{M}$ulti-$\textbf{M}$odel data co-powered $\textbf{D}$rug
$\textbf{T}$arget $\textbf{I}$nteraction prediction framework, named
LLM$^3$-DTI. LLM$^3$-DTI constructs multi-modal data embedding to enhance DTI
prediction performance. In this framework, the text semantic embeddings of
drugs and targets are encoded by a domain-specific LLM. To effectively align
and fuse multi-modal embedding. We propose the dual cross-attention mechanism
and the TSFusion module. Finally, these multi-modal data are utilized for the
DTI task through an output network. The experimental results indicate that
LLM$^3$-DTI can proficiently identify validated DTIs, surpassing the
performance of the models employed for comparison across diverse scenarios.
Consequently, LLM$^3$-DTI is adept at fulfilling the task of DTI prediction
with excellence. The data and code are available at
https://github.com/chaser-gua/LLM3DTI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06269v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06269v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FPGA or GPU? Analyzing comparative research for application-specific guidance
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The growing complexity of computational workloads has amplified the need for
efficient and specialized hardware accelerators. Field Programmable Gate Arrays
(FPGAs) and Graphics Processing Units (GPUs) have emerged as prominent
solutions, each excelling in specific domains. Although there is substantial
research comparing FPGAs and GPUs, most of the work focuses primarily on
performance metrics, offering limited insight into the specific types of
applications that each accelerator benefits the most. This paper aims to bridge
this gap by synthesizing insights from various research articles to guide users
in selecting the appropriate accelerator for domain-specific applications. By
categorizing the reviewed studies and analyzing key performance metrics, this
work highlights the strengths, limitations, and ideal use cases for FPGAs and
GPUs. The findings offer actionable recommendations, helping researchers and
practitioners navigate trade-offs in performance, energy efficiency, and
programmability.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06565v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06565v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    High-quality Question-Answer (QA) datasets are foundational for reliable
Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit
persistent gaps in domain coverage, misaligned difficulty distributions, and
factual inconsistencies. The recent surge in generative model-powered datasets
has compounded these quality challenges. In this work, we introduce RefineLab,
the first LLM-driven framework that automatically refines raw QA textual data
into high-quality datasets under a controllable token-budget constraint.
RefineLab takes a set of target quality attributes (such as coverage and
difficulty balance) as refinement objectives, and performs selective edits
within a predefined token budget to ensure practicality and efficiency. In
essence, RefineLab addresses a constrained optimization problem: improving the
quality of QA samples as much as possible while respecting resource
limitations. With a set of available refinement operations (e.g., rephrasing,
distractor replacement), RefineLab takes as input the original dataset, a
specified set of target quality dimensions, and a token budget, and determines
which refinement operations should be applied to each QA sample. This process
is guided by an assignment module that selects optimal refinement strategies to
maximize overall dataset quality while adhering to the budget constraint.
Experiments demonstrate that RefineLab consistently narrows divergence from
expert datasets across coverage, difficulty alignment, factual fidelity, and
distractor quality. RefineLab pioneers a scalable, customizable path to
reproducible dataset design, with broad implications for LLM evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06530v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06530v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        You Had One Job: Per-Task Quantization Using LLMs' Hidden Representations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) excel across diverse tasks, yet many
applications require only limited capabilities, making large variants
inefficient in memory and latency. Existing approaches often combine
distillation and quantization, but most post-training quantization (PTQ)
methods are task-agnostic, ignoring how task-specific signals are distributed
across layers. In this work, we propose to use hidden representations that
encode task-salient signals as a guideline for quantization. In order to fully
utilize our innovative idea, this paper compares two new task-aware PTQ
methods: Task-Aware Quantization (TAQ), which allocates bitwidths using
task-conditioned statistics from hidden activations, and TAQO, which allocates
precision based on direct layer sensitivity tests. From a small calibration
set, these approaches identify task-relevant layers, preserving their precision
while aggressively quantizing the rest. This yields stable task sensitivity
profiles and efficient task-specialized models. Across models, TAQ and TAQO
outperform the baselines; TAQ leads on Phi-4, while TAQO leads on Llama-3.1,
Qwen3, and Qwen2.5. For instances, on Phi-4 it achieves 42.33 EM / 50.81 F1,
far surpassing Activation-aware Weight Quantization (AWQ) (2.25 / 7.07), while
remaining within < 1.0% of the original accuracy at lower average precision.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06516v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06516v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Dutch Metaphor Extraction from Cancer Patients' Interviews and Forum Data using LLMs and Human in the Loop
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Metaphors and metaphorical language (MLs) play an important role in
healthcare communication between clinicians, patients, and patients' family
members. In this work, we focus on Dutch language data from cancer patients. We
extract metaphors used by patients using two data sources: (1) cancer patient
storytelling interview data and (2) online forum data, including patients'
posts, comments, and questions to professionals. We investigate how current
state-of-the-art large language models (LLMs) perform on this task by exploring
different prompting strategies such as chain of thought reasoning, few-shot
learning, and self-prompting. With a human-in-the-loop setup, we verify the
extracted metaphors and compile the outputs into a corpus named HealthQuote.NL.
We believe the extracted metaphors can support better patient care, for example
shared decision making, improved communication between patients and clinicians,
and enhanced patient health literacy. They can also inform the design of
personalized care pathways. We share prompts and related resources at
https://github.com/aaronlifenghan/HealthQuote.NL
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06427v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06427v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        How Well Do LLMs Understand Drug Mechanisms? A Knowledge + Reasoning Evaluation Dataset
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Two scientific fields showing increasing interest in pre-trained large
language models (LLMs) are drug development / repurposing, and personalized
medicine. For both, LLMs have to demonstrate factual knowledge as well as a
deep understanding of drug mechanisms, so they can recall and reason about
relevant knowledge in novel situations. Drug mechanisms of action are described
as a series of interactions between biomedical entities, which interlink into
one or more chains directed from the drug to the targeted disease. Composing
the effects of the interactions in a candidate chain leads to an inference
about whether the drug might be useful or not for that disease. We introduce a
dataset that evaluates LLMs on both factual knowledge of known mechanisms, and
their ability to reason about them under novel situations, presented as
counterfactuals that the models are unlikely to have seen during training.
Using this dataset, we show that o4-mini outperforms the 4o, o3, and o3-mini
models from OpenAI, and the recent small Qwen3-4B-thinking model closely
matches o4-mini's performance, even outperforming it in some cases. We
demonstrate that the open world setting for reasoning tasks, which requires the
model to recall relevant knowledge, is more challenging than the closed world
setting where the needed factual knowledge is provided. We also show that
counterfactuals affecting internal links in the reasoning chain present a much
harder task than those affecting a link from the drug mentioned in the prompt.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06418v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06418v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SugarTextNet: A Transformer-Based Framework for Detecting Sugar Dating-Related Content on Social Media with Context-Aware Focal Loss
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sugar dating-related content has rapidly proliferated on mainstream social
media platforms, giving rise to serious societal and regulatory concerns,
including commercialization of intimate relationships and the normalization of
transactional relationships.~Detecting such content is highly challenging due
to the prevalence of subtle euphemisms, ambiguous linguistic cues, and extreme
class imbalance in real-world data.~In this work, we present SugarTextNet, a
novel transformer-based framework specifically designed to identify sugar
dating-related posts on social media.~SugarTextNet integrates a pretrained
transformer encoder, an attention-based cue extractor, and a contextual phrase
encoder to capture both salient and nuanced features in user-generated text.~To
address class imbalance and enhance minority-class detection, we introduce
Context-Aware Focal Loss, a tailored loss function that combines focal loss
scaling with contextual weighting.~We evaluate SugarTextNet on a newly curated,
manually annotated dataset of 3,067 Chinese social media posts from Sina Weibo,
demonstrating that our approach substantially outperforms traditional machine
learning models, deep learning baselines, and large language models across
multiple metrics.~Comprehensive ablation studies confirm the indispensable role
of each component.~Our findings highlight the importance of domain-specific,
context-aware modeling for sensitive content detection, and provide a robust
solution for content moderation in complex, real-world scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06402v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06402v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Audio-visual target speaker extraction (AV-TSE) models primarily rely on
visual cues from the target speaker. However, humans also leverage linguistic
knowledge, such as syntactic constraints, next word prediction, and prior
knowledge of conversation, to extract target speech. Inspired by this
observation, we propose ELEGANCE, a novel framework that incorporates
linguistic knowledge from large language models (LLMs) into AV-TSE models
through three distinct guidance strategies: output linguistic constraints,
intermediate linguistic prediction, and input linguistic prior. Comprehensive
experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones
demon- strate the effectiveness of our approach. Significant improvements are
observed in challenging scenarios, including visual cue impaired, unseen
languages, target speaker switches, increased interfering speakers, and
out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06288v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06288v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multimodal Misinformation Detection (MMD) refers to the task of detecting
social media posts involving misinformation, where the post often contains text
and image modalities. However, by observing the MMD posts, we hold that the
text modality may be much more informative than the image modality because the
text generally describes the whole event/story of the current post but the
image often presents partial scenes only. Our preliminary empirical results
indicate that the image modality exactly contributes less to MMD. Upon this
idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that
each text can be divided into several segments, and each text segment describes
a partial scene that can be presented by an image. Accordingly, we split the
text into a sequence of segments, and feed these segments into a pre-trained
text-to-image generator to augment a sequence of images. We further incorporate
two auxiliary objectives concerning text-image and image-label mutual
information, and further post-train the generator over an auxiliary
text-to-image generation benchmark dataset. Additionally, we propose a graph
structure by defining three heuristic relationships between images, and use a
graph neural network to generate the fused features. Extensive empirical
results validate the effectiveness of RETSIMD.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06284v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06284v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs
must be both trustworthy and helpful. However, these goals often conflict. We
propose priority alignment, a new alignment paradigm that enforces a strict
"trustworthy-before-helpful" ordering: optimization of helpfulness is
conditioned on first meeting trustworthy thresholds (e.g., harmlessness or
honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully
unsupervised framework that generates diverse responses, self-evaluates them
and refines them by the model itself, and applies dual-criterion denoising to
remove inconsistency and control variance. From this, SPA constructs
lexicographically ordered preference pairs and fine-tunes the model using an
uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap
decisions. Experiments across multiple benchmarks show that SPA improves
helpfulness without compromising safety, outperforming strong baselines while
preserving general capabilities. Our results demonstrate that SPA provides a
scalable and interpretable alignment strategy for critical LLM applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06222v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06222v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Confidence-Guided Stepwise Model Routing for Cost-Efficient Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in Large Language Models (LLMs) - particularly model scaling
and test-time techniques - have greatly enhanced the reasoning capabilities of
language models at the expense of higher inference costs. To lower inference
costs, prior works train router models or deferral mechanisms that allocate
easy queries to a small, efficient model, while forwarding harder queries to
larger, more expensive models. However, these trained router models often lack
robustness under domain shifts and require expensive data synthesis techniques
such as Monte Carlo rollouts to obtain sufficient ground-truth routing labels
for training. In this work, we propose Confidence-Guided Stepwise Model Routing
for Cost-Efficient Reasoning (STEER), a domain-agnostic framework that performs
fine-grained, step-level routing between smaller and larger LLMs without
utilizing external models. STEER leverages confidence scores from the smaller
model's logits prior to generating a reasoning step, so that the large model is
invoked only when necessary. Extensive evaluations using different LLMs on a
diverse set of challenging benchmarks across multiple domains such as
Mathematical Reasoning, Multi-Hop QA, and Planning tasks indicate that STEER
achieves competitive or enhanced accuracy while reducing inference costs (up to
+20% accuracy with 48% less FLOPs compared to solely using the larger model on
AIME), outperforming baselines that rely on trained external modules. Our
results establish model-internal confidence as a robust, domain-agnostic signal
for model routing, offering a scalable pathway for efficient LLM deployment.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06190v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06190v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        BookAsSumQA: An Evaluation Framework for Aspect-Based Book Summarization via Question Answering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Aspect-based summarization aims to generate summaries that highlight specific
aspects of a text, enabling more personalized and targeted summaries. However,
its application to books remains unexplored due to the difficulty of
constructing reference summaries for long text. To address this challenge, we
propose BookAsSumQA, a QA-based evaluation framework for aspect-based book
summarization. BookAsSumQA automatically generates aspect-specific QA pairs
from a narrative knowledge graph to evaluate summary quality based on its
question-answering performance. Our experiments using BookAsSumQA revealed that
while LLM-based approaches showed higher accuracy on shorter texts, RAG-based
methods become more effective as document length increases, making them more
efficient and practical for aspect-based book summarization.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06183v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06183v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-4" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Multimodal
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 22 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前多模态技术研究呈现出几个主要趋势：一方面，研究者们致力于提升模型在复杂场景中的理解能力，如通过FractalBench和AI4VA-FG等基准，评估模型在视觉与数学推理、漫画理解等任务中的表现；另一方面，针对模型的局限性，如幻觉现象和模态不完整问题，提出了新的解决方案，如低秩方法和架构兼容性模块。这些研究不仅推动了多模态模型的性能提升，也为实际应用提供了更可靠的技术支持。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mathematical reasoning requires abstracting symbolic rules from visual
patterns -- inferring the infinite from the finite. We investigate whether
multimodal AI systems possess this capability through FractalBench, a benchmark
evaluating fractal program synthesis from images. Fractals provide ideal test
cases: Iterated Function Systems with only a few contraction maps generate
complex self-similar patterns through simple recursive rules, requiring models
to bridge visual perception with mathematical abstraction. We evaluate four
leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL
-- on 12 canonical fractals. Models must generate executable Python code
reproducing the fractal, enabling objective evaluation. Results reveal a
striking disconnect: 76% generate syntactically valid code but only 4% capture
mathematical structure. Success varies systematically -- models handle
geometric transformations (Koch curves: 17-21%) but fail at branching recursion
(trees: <2%), revealing fundamental gaps in mathematical abstraction.
FractalBench provides a contamination-resistant diagnostic for
visual-mathematical reasoning and is available at
https://github.com/NaiveNeuron/FractalBench
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06522v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06522v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Vision Language Models (VLMs) are increasingly used in autonomous driving to
help understand traffic scenes, but they sometimes produce hallucinations,
which are false details not grounded in the visual input. Detecting and
mitigating hallucinations is challenging when ground-truth references are
unavailable and model internals are inaccessible. This paper proposes a novel
self-contained low-rank approach to automatically rank multiple candidate
captions generated by multiple VLMs based on their hallucination levels, using
only the captions themselves without requiring external references or model
access. By constructing a sentence-embedding matrix and decomposing it into a
low-rank consensus component and a sparse residual, we use the residual
magnitude to rank captions: selecting the one with the smallest residual as the
most hallucination-free. Experiments on the NuScenes dataset demonstrate that
our approach achieves 87% selection accuracy in identifying hallucination-free
captions, representing a 19% improvement over the unfiltered baseline and a
6-10% improvement over multi-agent debate method. The sorting produced by
sparse error magnitudes shows strong correlation with human judgments of
hallucinations, validating our scoring mechanism. Additionally, our method,
which can be easily parallelized, reduces inference time by 51-67% compared to
debate approaches, making it practical for real-time autonomous driving
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06496v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06496v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06490v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06490v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While zero-shot diffusion-based compression methods have seen significant
progress in recent years, they remain notoriously slow and computationally
demanding. This paper presents an efficient zero-shot diffusion-based
compression method that runs substantially faster than existing methods, while
maintaining performance that is on par with the state-of-the-art techniques.
Our method builds upon the recently proposed Denoising Diffusion Codebook
Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by
sequentially choosing the diffusion noise vectors from reproducible random
codebooks, guiding the denoiser's output to reconstruct the target image. We
modify this framework with Turbo-DDCM, which efficiently combines a large
number of noise vectors at each denoising step, thereby significantly reducing
the number of required denoising operations. This modification is also coupled
with an improved encoding protocol. Furthermore, we introduce two flexible
variants of Turbo-DDCM, a priority-aware variant that prioritizes
user-specified regions and a distortion-controlled variant that compresses an
image based on a target PSNR rather than a target BPP. Comprehensive
experiments position Turbo-DDCM as a compelling, practical, and flexible image
compression scheme.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06424v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06424v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AUTO-Explorer: Automated Data Collection for GUI Agent
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in GUI agents have significantly expanded their ability
to interpret natural language commands to manage software interfaces. However,
acquiring GUI data remains a significant challenge. Existing methods often
involve designing automated agents that browse URLs from the Common Crawl,
using webpage HTML to collect screenshots and corresponding annotations,
including the names and bounding boxes of UI elements. However, this method is
difficult to apply to desktop software or some newly launched websites not
included in the Common Crawl. While we expect the model to possess strong
generalization capabilities to handle this, it is still crucial for
personalized scenarios that require rapid and perfect adaptation to new
software or websites. To address this, we propose an automated data collection
method with minimal annotation costs, named Auto-Explorer. It incorporates a
simple yet effective exploration mechanism that autonomously parses and
explores GUI environments, gathering data efficiently. Additionally, to assess
the quality of exploration, we have developed the UIXplore benchmark. This
benchmark creates environments for explorer agents to discover and save
software states. Using the data gathered, we fine-tune a multimodal large
language model (MLLM) and establish a GUI element grounding testing set to
evaluate the effectiveness of the exploration strategies. Our experiments
demonstrate the superior performance of Auto-Explorer, showing that our method
can quickly enhance the capabilities of an MLLM in explored software.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06417v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06417v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06406v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06406v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06348v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06348v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeSense:Making Large Language Models Proficient in Time-Series Analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In the time-series domain, an increasing number of works combine text with
temporal data to leverage the reasoning capabilities of large language models
(LLMs) for various downstream time-series understanding tasks. This enables a
single model to flexibly perform tasks that previously required specialized
models for each domain. However, these methods typically rely on text labels
for supervision during training, biasing the model toward textual cues while
potentially neglecting the full temporal features. Such a bias can lead to
outputs that contradict the underlying time-series context. To address this
issue, we construct the EvalTS benchmark, comprising 10 tasks across three
difficulty levels, from fundamental temporal pattern recognition to complex
real-world reasoning, to evaluate models under more challenging and realistic
scenarios. We also propose TimeSense, a multimodal framework that makes LLMs
proficient in time-series analysis by balancing textual reasoning with a
preserved temporal sense. TimeSense incorporates a Temporal Sense module that
reconstructs the input time-series within the model's context, ensuring that
textual reasoning is grounded in the time-series dynamics. Moreover, to enhance
spatial understanding of time-series data, we explicitly incorporate
coordinate-based positional embeddings, which provide each time point with
spatial context and enable the model to capture structural dependencies more
effectively. Experimental results demonstrate that TimeSense achieves
state-of-the-art performance across multiple tasks, and it particularly
outperforms existing methods on complex multi-dimensional time-series reasoning
tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06344v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06344v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06325v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06325v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable geospatial information on road accidents is vital for safety
analysis and infrastructure planning, yet most low- and middle-income countries
continue to face a critical shortage of accurate, location-specific crash data.
Existing text-based geocoding tools perform poorly in multilingual and
unstructured news environments, where incomplete place descriptions and mixed
Bangla-English scripts obscure spatial context. To address these limitations,
this study introduces ALIGN (Accident Location Inference through Geo-Spatial
Neural Reasoning)- a vision-language framework that emulates human spatial
reasoning to infer accident coordinates directly from textual and map-based
cues. ALIGN integrates large language and vision-language models within a
multi-stage pipeline that performs optical character recognition, linguistic
reasoning, and map-level verification through grid-based spatial scanning. The
framework systematically evaluates each predicted location against contextual
and visual evidence, ensuring interpretable, fine-grained geolocation outcomes
without requiring model retraining. Applied to Bangla-language news data, ALIGN
demonstrates consistent improvements over traditional geoparsing methods,
accurately identifying district and sub-district-level crash sites. Beyond its
technical contribution, the framework establishes a high accuracy foundation
for automated crash mapping in data-scarce regions, supporting evidence-driven
road-safety policymaking and the broader integration of multimodal artificial
intelligence in transportation analytics. The code for this paper is
open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06316v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06316v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Decomate: Leveraging Generative Models for Co-Creative SVG Animation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Designers often encounter friction when animating static SVG graphics,
especially when the visual structure does not match the desired level of motion
detail. Existing tools typically depend on predefined groupings or require
technical expertise, which limits designers' ability to experiment and iterate
independently. We present Decomate, a system that enables intuitive SVG
animation through natural language. Decomate leverages a multimodal large
language model to restructure raw SVGs into semantically meaningful,
animation-ready components. Designers can then specify motions for each
component via text prompts, after which the system generates corresponding
HTML/CSS/JS animations. By supporting iterative refinement through natural
language interaction, Decomate integrates generative AI into creative
workflows, allowing animation outcomes to be directly shaped by user intent.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06297v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06297v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06272v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06272v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    User interface (UI) development requires translating design mockups into
functional code, a process that remains repetitive and labor-intensive. While
recent Vision-Language Models (VLMs) automate UI-to-Code generation, they
generate only static HTML/CSS/JavaScript layouts lacking interactivity. To
address this, we propose WebVIA, the first agentic framework for interactive
UI-to-Code generation and validation. The framework comprises three components:
1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code
model that generates executable interactive code; 3) a validation module that
verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves
more stable and accurate UI exploration than general-purpose agents (e.g.,
Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit
substantial improvements in generating executable and interactive
HTML/CSS/JavaScript code, outperforming their base counterparts across both
interactive and static UI2Code benchmarks. Our code and models are available at
\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06251v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06251v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In open-vocabulary mobile manipulation (OVMM), task success often hinges on
the selection of an appropriate base placement for the robot. Existing
approaches typically navigate to proximity-based regions without considering
affordances, resulting in frequent manipulation failures. We propose
Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base
placement that integrates semantic understanding from vision-language models
(VLMs) with geometric feasibility through an iterative optimization process.
Our method constructs cross-modal representations, namely Affordance RGB and
Obstacle Map+, to align semantics with spatial context. This enables reasoning
that extends beyond the egocentric limitations of RGB perception. To ensure
interaction is guided by task-relevant affordances, we leverage coarse semantic
priors from VLMs to guide the search toward task-relevant regions and refine
placements with geometric constraints, thereby reducing the risk of convergence
to local optima. Evaluated on five diverse open-vocabulary mobile manipulation
tasks, our system achieves an 85% success rate, significantly outperforming
classical geometric planners and VLM-based methods. This demonstrates the
promise of affordance-aware and multimodal reasoning for generalizable,
instruction-conditioned planning in OVMM.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06240v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06240v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chinese opera is celebrated for preserving classical art. However, early
filming equipment limitations have degraded videos of last-century performances
by renowned artists (e.g., low frame rates and resolution), hindering archival
efforts. Although space-time video super-resolution (STVSR) has advanced
significantly, applying it directly to opera videos remains challenging. The
scarcity of datasets impedes the recovery of high frequency details, and
existing STVSR methods lack global modeling capabilities, compromising visual
quality when handling opera's characteristic large motions. To address these
challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset
and propose the Mamba-based multiscale fusion network for space-time Opera
Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three
novel components: the Global Fusion Module (GFM) for motion modeling through a
multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba
Module (MSMM) for alignment across different sequence lengths. Additionally,
our MambaVR block resolves feature artifacts and positional information loss
during alignment. Experimental results on the COVC dataset show that MambaOVSR
significantly outperforms the SOTA STVSR method by an average of 1.86 dB in
terms of PSNR. Dataset and Code will be publicly released.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06172v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06172v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Although multimodal fusion has made significant progress, its advancement is
severely hindered by the lack of adequate evaluation benchmarks. Current fusion
methods are typically evaluated on a small selection of public datasets, a
limited scope that inadequately represents the complexity and diversity of
real-world scenarios, potentially leading to biased evaluations. This issue
presents a twofold challenge. On one hand, models may overfit to the biases of
specific datasets, hindering their generalization to broader practical
applications. On the other hand, the absence of a unified evaluation standard
makes fair and objective comparisons between different fusion methods
difficult. Consequently, a truly universal and high-performance fusion model
has yet to emerge. To address these challenges, we have developed a
large-scale, domain-adaptive benchmark for multimodal evaluation. This
benchmark integrates over 30 datasets, encompassing 15 modalities and 20
predictive tasks across key application domains. To complement this, we have
also developed an open-source, unified, and automated evaluation pipeline that
includes standardized implementations of state-of-the-art models and diverse
fusion paradigms. Leveraging this platform, we have conducted large-scale
experiments, successfully establishing new performance baselines across
multiple tasks. This work provides the academic community with a crucial
platform for rigorous and reproducible assessment of multimodal models, aiming
to propel the field of multimodal artificial intelligence to new heights.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06452v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06452v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Countering Multi-modal Representation Collapse through Rank-targeted Fusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06450v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06450v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Interpretable representation learning is a central challenge in modern
machine learning, particularly in high-dimensional settings such as
neuroimaging, genomics, and text analysis. Current methods often struggle to
balance the competing demands of interpretability and model flexibility,
limiting their effectiveness in extracting meaningful insights from complex
data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a
general-purpose matrix estimation framework that unifies ideas from sparse
matrix factorization, orthogonalization, and constrained manifold learning.
NSA-Flow enforces structured sparsity through a continuous balance between
reconstruction fidelity and column-wise decorrelation, parameterized by a
single tunable weight. The method operates as a smooth flow near the Stiefel
manifold with proximal updates for non-negativity and adaptive gradient
control, yielding representations that are simultaneously sparse, stable, and
interpretable. Unlike classical regularization schemes, NSA-Flow provides an
intuitive geometric mechanism for manipulating sparsity at the level of global
structure while simplifying latent features. We demonstrate that the NSA-Flow
objective can be optimized smoothly and integrates seamlessly with existing
pipelines for dimensionality reduction while improving interpretability and
generalization in both simulated and real biomedical data. Empirical validation
on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the
NSA-Flow constraints can maintain or improve performance over related methods
with little additional methodological effort. NSA-Flow offers a scalable,
general-purpose tool for interpretable ML, applicable across data science
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06425v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06425v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CAMP-HiVe: Cyclic Pair Merging based Efficient DNN Pruning with Hessian-Vector Approximation for Resource-Constrained Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep learning algorithms are becoming an essential component of many
artificial intelligence (AI) driven applications, many of which run on
resource-constrained and energy-constrained systems. For efficient deployment
of these algorithms, although different techniques for the compression of
neural network models are proposed, neural pruning is one of the fastest and
effective methods, which can provide a high compression gain with minimal cost.
To harness enhanced performance gain with respect to model complexity, we
propose a novel neural network pruning approach utilizing Hessian-vector
products that approximate crucial curvature information in the loss function,
which significantly reduces the computation demands. By employing a power
iteration method, our algorithm effectively identifies and preserves the
essential information, ensuring a balanced trade-off between model accuracy and
computational efficiency. Herein, we introduce CAMP-HiVe, a cyclic pair
merging-based pruning with Hessian Vector approximation by iteratively
consolidating weight pairs, combining significant and less significant weights,
thus effectively streamlining the model while preserving its performance. This
dynamic, adaptive framework allows for real-time adjustment of weight
significance, ensuring that only the most critical parameters are retained. Our
experimental results demonstrate that our proposed method achieves significant
reductions in computational requirements while maintaining high performance
across different neural network architectures, e.g., ResNet18, ResNet56, and
MobileNetv2, on standard benchmark datasets, e.g., CIFAR10, CIFAR-100, and
ImageNet, and it outperforms the existing state-of-the-art neural pruning
methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06265v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06265v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Test-Time Iterative Error Correction for Efficient Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the growing demand for high-quality image generation on
resource-constrained devices, efficient diffusion models have received
increasing attention. However, such models suffer from approximation errors
introduced by efficiency techniques, which significantly degrade generation
quality. Once deployed, these errors are difficult to correct, as modifying the
model is typically infeasible in deployment environments. Through an analysis
of error propagation across diffusion timesteps, we reveal that these
approximation errors can accumulate exponentially, severely impairing output
quality. Motivated by this insight, we propose Iterative Error Correction
(IEC), a novel test-time method that mitigates inference-time errors by
iteratively refining the model's output. IEC is theoretically proven to reduce
error propagation from exponential to linear growth, without requiring any
retraining or architectural changes. IEC can seamlessly integrate into the
inference process of existing diffusion models, enabling a flexible trade-off
between performance and efficiency. Extensive experiments show that IEC
consistently improves generation quality across various datasets, efficiency
techniques, and model architectures, establishing it as a practical and
generalizable solution for test-time enhancement of efficient diffusion models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06250v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06250v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ELEGANCE: Efficient LLM Guidance for Audio-Visual Target Speech Extraction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Audio-visual target speaker extraction (AV-TSE) models primarily rely on
visual cues from the target speaker. However, humans also leverage linguistic
knowledge, such as syntactic constraints, next word prediction, and prior
knowledge of conversation, to extract target speech. Inspired by this
observation, we propose ELEGANCE, a novel framework that incorporates
linguistic knowledge from large language models (LLMs) into AV-TSE models
through three distinct guidance strategies: output linguistic constraints,
intermediate linguistic prediction, and input linguistic prior. Comprehensive
experiments with RoBERTa, Qwen3-0.6B, and Qwen3-4B on two AV-TSE backbones
demon- strate the effectiveness of our approach. Significant improvements are
observed in challenging scenarios, including visual cue impaired, unseen
languages, target speaker switches, increased interfering speakers, and
out-of-domain test set. Demo page: https://alexwxwu.github.io/ELEGANCE/.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06288v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06288v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multimodal Misinformation Detection (MMD) refers to the task of detecting
social media posts involving misinformation, where the post often contains text
and image modalities. However, by observing the MMD posts, we hold that the
text modality may be much more informative than the image modality because the
text generally describes the whole event/story of the current post but the
image often presents partial scenes only. Our preliminary empirical results
indicate that the image modality exactly contributes less to MMD. Upon this
idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that
each text can be divided into several segments, and each text segment describes
a partial scene that can be presented by an image. Accordingly, we split the
text into a sequence of segments, and feed these segments into a pre-trained
text-to-image generator to augment a sequence of images. We further incorporate
two auxiliary objectives concerning text-image and image-label mutual
information, and further post-train the generator over an auxiliary
text-to-image generation benchmark dataset. Additionally, we propose a graph
structure by defining three heuristic relationships between images, and use a
graph neural network to generate the fused features. Extensive empirical
results validate the effectiveness of RETSIMD.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06284v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06284v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-5" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Optimization
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 24 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前优化技术研究的主要动态集中在多领域的应用与创新方法上。首先，针对旅行商问题的新变体及其在轨迹规划中的应用显示出对复杂环境的适应性；其次，强化学习在大语言模型推理中的应用正在突破传统方法的限制；此外，针对仇恨言论检测的可解释性优化，以及城市交通优化中的隐私保护与公平性问题，均表明研究者在提升模型性能的同时，越来越重视社会伦理和实际应用的复杂性。这些趋势不仅推动了优化技术的进步，也为多样化的实际问题提供了潜在解决方案。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP)
defined over a Graph of Convex Sets (GCS) -- a powerful representation for
trajectory planning that decomposes the configuration space into convex regions
connected by a sparse graph. In this setting, edge costs are not fixed but
depend on the specific trajectory selected through each convex region, making
classical TSP methods inapplicable. We introduce GHOST, a hierarchical
framework that optimally solves the GCS-TSP by combining combinatorial tour
search with convex trajectory optimization. GHOST systematically explores tours
on a complete graph induced by the GCS, using a novel abstract-path-unfolding
algorithm to compute admissible lower bounds that guide best-first search at
both the high level (over tours) and the low level (over feasible GCS paths
realizing the tour). These bounds provide strong pruning power, enabling
efficient search while avoiding unnecessary convex optimization calls. We prove
that GHOST guarantees optimality and present a bounded-suboptimal variant for
time-critical scenarios. Experiments show that GHOST is orders-of-magnitude
faster than unified mixed-integer convex programming baselines for simple cases
and uniquely handles complex trajectory planning problems involving high-order
continuity constraints and an incomplete GCS.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06471v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06471v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The soft-thinking paradigm for Large Language Model (LLM) reasoning can
outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in
some scenarios, underscoring its research and application value. However, while
the discrete-token CoT reasoning pattern can be reinforced through policy
optimization algorithms such as group relative policy optimization (GRPO),
extending the soft-thinking pattern with Reinforcement Learning (RL) remains
challenging. This difficulty stems from the complexities of injecting
stochasticity into soft-thinking tokens and updating soft-thinking policies
accordingly. As a result, previous attempts to combine soft-thinking with GRPO
typically underperform their discrete-token GRPO counterparts. To fully unlock
the potential of soft-thinking, this paper presents a novel policy optimization
algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning
pattern. SofT-GRPO injects the Gumbel noise into logits, employs the
Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained
embedding space, and leverages the reparameterization trick in policy gradient.
We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and
results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly
outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while
exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes
and weights are available on https://github.com/zz1358m/SofT-GRPO-master
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06411v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06411v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Optimization of offensive content moderation models for different types of
hateful messages is typically achieved through continued pre-training or
fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly
address explicit hate toward protected groups and often overlook implicit or
indirect hate, such as demeaning comparisons, calls for exclusion or violence,
and subtle discriminatory language that still causes harm. While explicit hate
can often be captured through surface features, implicit hate requires deeper,
full-model semantic processing. In this work, we question the need for repeated
fine-tuning and analyze the role of HatePrototypes, class-level vector
representations derived from language models optimized for hate speech
detection and safety moderation. We find that these prototypes, built from as
few as 50 examples per class, enable cross-task transfer between explicit and
implicit hate, with interchangeable prototypes across benchmarks. Moreover, we
show that parameter-free early exiting with prototypes is effective for both
hate types. We release the code, prototype resources, and evaluation scripts to
support future research on efficient and transferable hate speech detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06391v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06391v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of reasoning tasks. Recent methods have further improved LLM
performance in complex mathematical reasoning. However, when extending these
methods beyond the domain of mathematical reasoning to tasks involving complex
domain-specific knowledge, we observe a consistent failure of LLMs to generate
novel insights during the reflection stage. Instead of conducting genuine
cognitive refinement, the model tends to mechanically reiterate earlier
reasoning steps without introducing new information or perspectives, a
phenomenon referred to as "Echo Reflection". We attribute this behavior to two
key defects: (1) Uncontrollable information flow during response generation,
which allows premature intermediate thoughts to propagate unchecked and distort
final decisions; (2) Insufficient exploration of internal knowledge during
reflection, leading to repeating earlier findings rather than generating new
cognitive insights. Building on these findings, we proposed a novel
reinforcement learning method termed Adaptive Entropy Policy Optimization
(AEPO). Specifically, the AEPO framework consists of two major components: (1)
Reflection-aware Information Filtration, which quantifies the cognitive
information flow and prevents the final answer from being affected by earlier
bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically
balances exploration and exploitation across different reasoning stages,
promoting both reflective diversity and answer correctness. Extensive
experiments demonstrate that AEPO consistently achieves state-of-the-art
performance over mainstream reinforcement learning baselines across diverse
benchmarks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06380v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06380v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The optimization of urban traffic is threatened by the complexity of
achieving a balance between transport efficiency and the maintenance of
privacy, as well as the equitable distribution of traffic based on
socioeconomically diverse neighborhoods. Current centralized traffic management
schemes invade user location privacy and further entrench traffic disparity by
offering disadvantaged route suggestions, whereas current federated learning
frameworks do not consider fairness constraints in multi-objective traffic
settings. This study presents a privacy-preserving federated learning
framework, termed FedFair-Traffic, that jointly and simultaneously optimizes
travel efficiency, traffic fairness, and differential privacy protection. This
is the first attempt to integrate three conflicting objectives to improve urban
transportation systems. The proposed methodology enables collaborative learning
between related vehicles with data locality by integrating Graph Neural
Networks with differential privacy mechanisms ($\epsilon$-privacy guarantees)
and Gini coefficient-based fair constraints using multi-objective optimization.
The framework uses federated aggregation methods of gradient clipping and noise
injection to provide differential privacy and optimize Pareto-efficient
solutions for the efficiency-fairness tradeoff. Real-world comprehensive
experiments on the METR-LA traffic dataset showed that FedFair-Traffic can
reduce the average travel time by 7\% (14.2 minutes) compared with their
centralized baselines, promote traffic fairness by 73\% (Gini coefficient,
0.78), and offer high privacy protection (privacy score, 0.8) with an 89\%
reduction in communication overhead. These outcomes demonstrate that
FedFair-Traffic is a scalable privacy-aware smart city infrastructure with
possible use-cases in metropolitan traffic flow control and federated
transportation networks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06363v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06363v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Station: An Open-World Environment for AI-Driven Discovery
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce the STATION, an open-world multi-agent environment that models a
miniature scientific ecosystem. Leveraging their extended context windows,
agents in the Station can engage in long scientific journeys that include
reading papers from peers, formulating hypotheses, submitting code, performing
analyses, and publishing results. Importantly, there is no centralized system
coordinating their activities - agents are free to choose their own actions and
develop their own narratives within the Station. Experiments demonstrate that
AI agents in the Station achieve new state-of-the-art performance on a wide
range of benchmarks, spanning from mathematics to computational biology to
machine learning, notably surpassing AlphaEvolve in circle packing. A rich
tapestry of narratives emerges as agents pursue independent research, interact
with peers, and build upon a cumulative history. From these emergent
narratives, novel methods arise organically, such as a new density-adaptive
algorithm for scRNA-seq batch integration. The Station marks a first step
towards autonomous scientific discovery driven by emergent behavior in an
open-world environment, representing a new paradigm that moves beyond rigid
optimization.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06309v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06309v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Financial documents like earning reports or balance sheets often involve long
tables and multi-page reports. Large language models have become a new tool to
help numerical reasoning and understanding these documents. However, prompt
quality can have a major effect on how well LLMs perform these financial
reasoning tasks. Most current methods tune prompts on fixed datasets of
financial text or tabular data, which limits their ability to adapt to new
question types or document structures, or they involve costly and manually
labeled/curated dataset to help build the prompts. We introduce a
self-improving prompt framework driven by data-augmented optimization. In this
closed-loop process, we generate synthetic financial tables and document
excerpts, verify their correctness and robustness, and then update the prompt
based on the results. Specifically, our framework combines a synthetic data
generator with verifiers and a prompt optimizer, where the generator produces
new examples that exposes weaknesses in the current prompt, the verifiers check
the validity and robustness of the produced examples, and the optimizer
incrementally refines the prompt in response. By iterating these steps in a
feedback cycle, our method steadily improves prompt accuracy on financial
reasoning tasks without needing external labels. Evaluation on DocMath-Eval
benchmark demonstrates that our system achieves higher performance in both
accuracy and robustness than standard prompt methods, underscoring the value of
incorporating synthetic data generation into prompt learning for financial
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06292v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06292v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Model-based reinforcement learning (MBRL) is a crucial approach to enhance
the generalization capabilities and improve the sample efficiency of RL
algorithms. However, current MBRL methods focus primarily on building world
models for single tasks and rarely address generalization across different
scenarios. Building on the insight that dynamics within the same simulation
engine share inherent properties, we attempt to construct a unified world model
capable of generalizing across different scenarios, named Meta-Regularized
Contextual World-Model (MrCoM). This method first decomposes the latent state
space into various components based on the dynamic characteristics, thereby
enhancing the accuracy of world-model prediction. Further, MrCoM adopts
meta-state regularization to extract unified representation of
scenario-relevant information, and meta-value regularization to align
world-model optimization with policy learning across diverse scenario
objectives. We theoretically analyze the generalization error upper bound of
MrCoM in multi-scenario settings. We systematically evaluate our algorithm's
generalization ability across diverse scenarios, demonstrating significantly
better performance than previous state-of-the-art methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06252v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06252v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Constraint-Informed Active Learning for End-to-End ACOPF Optimization Proxies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper studies optimization proxies, machine learning (ML) models trained
to efficiently predict optimal solutions for AC Optimal Power Flow (ACOPF)
problems. While promising, optimization proxy performance heavily depends on
training data quality. To address this limitation, this paper introduces a
novel active sampling framework for ACOPF optimization proxies designed to
generate realistic and diverse training data. The framework actively explores
varied, flexible problem specifications reflecting plausible operational
realities. More importantly, the approach uses optimization-specific quantities
(active constraint sets) that better capture the salient features of an ACOPF
that lead to the optimal solution. Numerical results show superior
generalization over existing sampling methods with an equivalent training
budget, significantly advancing the state-of-practice for trustworthy ACOPF
optimization proxies.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06248v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06248v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Dataforge: A Data Agent Platform for Autonomous Data Engineering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The growing demand for AI applications in fields such as materials discovery,
molecular modeling, and climate science has made data preparation an important
but labor-intensive step. Raw data from diverse sources must be cleaned,
normalized, and transformed to become AI-ready, while effective feature
transformation and selection are essential for efficient training and
inference. To address the challenges of scalability and expertise dependence,
we present Data Agent, a fully autonomous system specialized for tabular data.
Leveraging large language model (LLM) reasoning and grounded validation, Data
Agent automatically performs data cleaning, hierarchical routing, and
feature-level optimization through dual feedback loops. It embodies three core
principles: automatic, safe, and non-expert friendly, which ensure end-to-end
reliability without human supervision. This demo showcases the first practical
realization of an autonomous Data Agent, illustrating how raw data can be
transformed "From Data to Better Data."
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06185v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06185v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid progress of large language models (LLMs) has advanced numerous
applications, yet efficient single-batch inference remains vital for on-device
intelligence. While FPGAs offer fine-grained data control and high energy
efficiency, recent GPU optimizations have narrowed their advantage, especially
under arithmetic-based computation. To overcome this, we leverage FPGAs'
abundant on-chip memory to shift LLM inference from arithmetic- to memory-based
computation through table lookups. We present LUT-LLM, the first FPGA
accelerator enabling 1B+ LLM inference via vector-quantized memory operations.
Our analysis identifies activation-weight co-quantization as the most effective
scheme, supported by (1) bandwidth-aware parallel centroid search, (2)
efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing
data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B
model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher
energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency
gain over A100.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06174v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06174v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper presents a framework for evaluating and optimizing reasoning
consistency in Large Language Models (LLMs) via a new metric, the Alignment
Score, which quantifies the semantic alignment between model-generated
reasoning chains and human-written reference chains in Chain-of-Thought (CoT)
reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest
Alignment Score. To explain this phenomenon, we define four key error types:
logical disconnection, thematic shift, redundant reasoning, and causal
reversal, and show how each contributes to the degradation of the Alignment
Score. Building on this analysis, we further propose Semantic Consistency
Optimization Sampling (SCOS), a method that samples and favors chains with
minimal alignment errors, significantly improving Alignment Scores by an
average of 29.84% with longer reasoning chains, such as in 3-hop tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06168v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06168v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient Approximation of Volterra Series for High-Dimensional Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The identification of high-dimensional nonlinear dynamical systems via the
Volterra series has significant potential, but has been severely hindered by
the curse of dimensionality. Tensor Network (TN) methods such as the Modified
Alternating Linear Scheme (MVMALS) have been a breakthrough for the field,
offering a tractable approach by exploiting the low-rank structure in Volterra
kernels. However, these techniques still encounter prohibitive computational
and memory bottlenecks due to high-order polynomial scaling with respect to
input dimension. To overcome this barrier, we introduce the Tensor Head
Averaging (THA) algorithm, which significantly reduces complexity by
constructing an ensemble of localized MVMALS models trained on small subsets of
the input space. In this paper, we present a theoretical foundation for the THA
algorithm. We establish observable, finite-sample bounds on the error between
the THA ensemble and a full MVMALS model, and we derive an exact decomposition
of the squared error. This decomposition is used to analyze the manner in which
subset models implicitly compensate for omitted dynamics. We quantify this
effect, and prove that correlation between the included and omitted dynamics
creates an optimization incentive which drives THA's performance toward
accuracy superior to a simple truncation of a full MVMALS model. THA thus
offers a scalable and theoretically grounded approach for identifying
previously intractable high-dimensional systems.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06527v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06527v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Supply chain disruptions and volatile demand pose significant challenges to
the UK automotive industry, which relies heavily on Just-In-Time (JIT)
manufacturing. While qualitative studies highlight the potential of integrating
Artificial Intelligence (AI) with traditional optimization, a formal,
quantitative demonstration of this synergy is lacking. This paper introduces a
novel stochastic learning-optimization framework that integrates Bayesian
inference with inventory optimization for supply chain management (SCM). We
model a two-echelon inventory system subject to stochastic demand and supply
disruptions, comparing a traditional static optimization policy against an
adaptive policy where Bayesian learning continuously updates parameter
estimates to inform stochastic optimization. Our simulations over 365 periods
across three operational scenarios demonstrate that the integrated approach
achieves 7.4\% cost reduction in stable environments and 5.7\% improvement
during supply disruptions, while revealing important limitations during sudden
demand shocks due to the inherent conservatism of Bayesian updating. This work
provides mathematical validation for practitioner observations and establishes
a formal framework for understanding AI-driven supply chain resilience, while
identifying critical boundary conditions for successful implementation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06479v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06479v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, optimizers that explicitly treat weights as matrices, rather than
flattened vectors, have demonstrated their effectiveness. This perspective
naturally leads to structured approximations of the Fisher matrix as
preconditioners, where the matrix view induces a Kronecker-factorized form that
enables memory-efficient representation. However, constructing such
approximations both efficiently and accurately remains an open challenge, since
obtaining the optimal factorization is resource-intensive and practical methods
therefore rely on heuristic design choices. In this work, we introduce a novel
approach that leverages projector-splitting integrators to construct effective
preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the
Fisher Matrix), consistently improves the Fisher matrix approximation quality.
Experiments on large language model pre-training and fine-tuning demonstrate
that DyKAF outperforms existing optimizers across a range of evaluation
metrics.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06477v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06477v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reconstruction and Secrecy under Approximate Distance Queries
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Consider the task of locating an unknown target point using approximate
distance queries: in each round, a reconstructor selects a query point and
receives a noisy version of its distance to the target. This problem arises
naturally in various contexts ranging from localization in GPS and sensor
networks to privacy-aware data access, and spans a wide variety of metric
spaces. It is relevant from the perspective of both the reconstructor (seeking
accurate recovery) and the responder (aiming to limit information disclosure,
e.g., for privacy or security reasons). We study this reconstruction game
through a learning-theoretic lens, focusing on the rate and limits of the best
possible reconstruction error. Our first result provides a tight geometric
characterization of the optimal error in terms of the Chebyshev radius, a
classical concept from geometry. This characterization applies to all compact
metric spaces (in fact, even to all totally bounded spaces) and yields explicit
formulas for natural metric spaces. Our second result addresses the asymptotic
behavior of reconstruction, distinguishing between pseudo-finite spaces --
where the optimal error is attained after finitely many queries -- and spaces
where the approximation curve exhibits nontrivial decay. We characterize
pseudo-finiteness for convex Euclidean spaces.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06461v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06461v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Control barrier functions (CBFs) are a popular tool for safety certification
of nonlinear dynamical control systems. Recently, CBFs represented as neural
networks have shown great promise due to their expressiveness and applicability
to a broad class of dynamics and safety constraints. However, verifying that a
trained neural network is indeed a valid CBF is a computational bottleneck that
limits the size of the networks that can be used. To overcome this limitation,
we present a novel framework for verifying neural CBFs based on piecewise
linear upper and lower bounds on the conditions required for a neural network
to be a CBF. Our approach is rooted in linear bound propagation (LBP) for
neural networks, which we extend to compute bounds on the gradients of the
network. Combined with McCormick relaxation, we derive linear upper and lower
bounds on the CBF conditions, thereby eliminating the need for computationally
expensive verification procedures. Our approach applies to arbitrary
control-affine systems and a broad range of nonlinear activation functions. To
reduce conservatism, we develop a parallelizable refinement strategy that
adaptively refines the regions over which these bounds are computed. Our
approach scales to larger neural networks than state-of-the-art verification
procedures for CBFs, as demonstrated by our numerical experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06341v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06341v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a
resurgence of interest in RLVR. Nevertheless, advances are dominated by
mathematics (e.g., AIME), with competitive-programming code generation
underexplored and data curation receiving less attention than RL algorithm
design. We investigate how to construct RLVR datasets (i.e., RL prompts) and
present practical training techniques that yield strong performance on
competitive-programming code generation. Our pipeline begins with supervised
fine-tuning (SFT) distilled from strong open-source models, augmented with
general-purpose and reasoning-intensive data. RL then follows a two-stage
process with executable, testcase-driven rewards: first, training on a large,
uniformly distributed set of competitive-programming problems using Group
Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively
short response-generation window (e.g., 32k during SFT and 24k in this stage)
to expand entropy and mitigate repetition and truncation; second, we perform
\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging
problems with a large rollout budget (64 rollouts per prompt) under a
hard-focus curriculum that continuously retains the most difficult instances
throughout training. We implement our method on Qwen2.5-32B and evaluate on
LeetCode and Codeforces weekly contests to avoid data leakage. The resulting
model achieves state-of-the-art performance among models of similar scale and
is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.
We also examine scaling trends and observe strong RL scaling on an internal
large-scale MoE model. Our study distills concise best practices for data
curation, entropy expansion, and curriculum design in RLVR for
competitive-programming code generation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06307v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06307v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper presents a comprehensive analysis of hyperparameter estimation
within the empirical Bayes framework (EBF) for sparse learning. By studying the
influence of hyperpriors on the solution of EBF, we establish a theoretical
connection between the choice of the hyperprior and the sparsity as well as the
local optimality of the resulting solutions. We show that some strictly
increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with
the power in $(0,1)$, effectively promote sparsity and improve solution
stability with respect to measurement noise. Based on this analysis, we adopt a
proximal alternating linearized minimization (PALM) algorithm with convergence
guaranties for both convex and concave hyperpriors. Extensive numerical tests
on two-dimensional image deblurring problems demonstrate that introducing
appropriate hyperpriors significantly promotes the sparsity of the solution and
enhances restoration accuracy. Furthermore, we illustrate the influence of the
noise level and the ill-posedness of inverse problems to EBF solutions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06235v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06235v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Human-computer interaction increasingly demands systems that recognize not
only explicit user inputs but also implicit emotional states. While substantial
progress has been made in affective computing, most emotion recognition systems
rely on cloud-based inference, introducing privacy vulnerabilities and latency
constraints unsuitable for real-time applications. This work presents a
comprehensive evaluation of machine learning architectures for on-device
emotion recognition from wrist-based photoplethysmography (PPG), systematically
comparing different models spanning classical ensemble methods, deep neural
networks, and transformers on the WESAD stress detection dataset. Results
demonstrate that classical ensemble methods substantially outperform deep
learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826
on combined features and F1 = 0.623 on wrist-only features, compared to
transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only
ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint,
0.05 ms inference latency, and 152x speedup over the original implementation.
Furthermore, ONNX optimization yields a 30.5% average storage reduction and
40.1x inference speedup, highlighting the feasibility of privacy-preserving
on-device emotion recognition for real-world wearables.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06231v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06231v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sparse Linear Regression is Easy on Random Supports
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sparse linear regression is one of the most basic questions in machine
learning and statistics. Here, we are given as input a design matrix $X \in
\mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$
where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements.
Importantly, we have the additional constraint that the unknown signal vector
${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than
the ambient dimension. Our goal is to output a prediction vector
$\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^*
- {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of
measurements: under most natural noise distributions, we can get prediction
error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples.
Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately,
with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap
(in the dependence on $d$) between the two and we do not know if it is possible
to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices
${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at
random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d,
1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for
any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for
random design matrices from well-behaved families, matrices that have a very
low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed
sensing), or those with special structural properties.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06211v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06211v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Better Datasets Start From RefineLab: Automatic Optimization for High-Quality Dataset Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    High-quality Question-Answer (QA) datasets are foundational for reliable
Large Language Model (LLM) evaluation, yet even expert-crafted datasets exhibit
persistent gaps in domain coverage, misaligned difficulty distributions, and
factual inconsistencies. The recent surge in generative model-powered datasets
has compounded these quality challenges. In this work, we introduce RefineLab,
the first LLM-driven framework that automatically refines raw QA textual data
into high-quality datasets under a controllable token-budget constraint.
RefineLab takes a set of target quality attributes (such as coverage and
difficulty balance) as refinement objectives, and performs selective edits
within a predefined token budget to ensure practicality and efficiency. In
essence, RefineLab addresses a constrained optimization problem: improving the
quality of QA samples as much as possible while respecting resource
limitations. With a set of available refinement operations (e.g., rephrasing,
distractor replacement), RefineLab takes as input the original dataset, a
specified set of target quality dimensions, and a token budget, and determines
which refinement operations should be applied to each QA sample. This process
is guided by an assignment module that selects optimal refinement strategies to
maximize overall dataset quality while adhering to the budget constraint.
Experiments demonstrate that RefineLab consistently narrows divergence from
expert datasets across coverage, difficulty alignment, factual fidelity, and
distractor quality. RefineLab pioneers a scalable, customizable path to
reproducible dataset design, with broad implications for LLM evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06530v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06530v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SPA: Achieving Consensus in LLM Alignment via Self-Priority Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In high-stakes scenarios-such as self-harm, legal, or medical queries-LLMs
must be both trustworthy and helpful. However, these goals often conflict. We
propose priority alignment, a new alignment paradigm that enforces a strict
"trustworthy-before-helpful" ordering: optimization of helpfulness is
conditioned on first meeting trustworthy thresholds (e.g., harmlessness or
honesty). To realize this, we introduce Self-Priority Alignment (SPA)-a fully
unsupervised framework that generates diverse responses, self-evaluates them
and refines them by the model itself, and applies dual-criterion denoising to
remove inconsistency and control variance. From this, SPA constructs
lexicographically ordered preference pairs and fine-tunes the model using an
uncertainty-weighted alignment loss that emphasizes high-confidence, high-gap
decisions. Experiments across multiple benchmarks show that SPA improves
helpfulness without compromising safety, outperforming strong baselines while
preserving general capabilities. Our results demonstrate that SPA provides a
scalable and interpretable alignment strategy for critical LLM applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06222v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06222v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-6" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RAG
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 2 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前RAG技术方向的研究动态主要集中在知识集成和安全防护两个方面。首先，SR-KI提出了一种将实时大规模知识库集成到大语言模型中的新方法，强调了通过监督注意力机制提升模型的知识利用效率。其次，针对物联网环境中的安全挑战，研究者们探讨了基于RAG的对抗攻击策略，以增强基于LLM的威胁检测和缓解框架的有效性。这些研究共同体现了RAG在提升模型智能和安全性方面的潜在价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper proposes SR-KI, a novel approach for integrating real-time and
large-scale structured knowledge bases (KBs) into large language models (LLMs).
SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder,
and injects them into LLMs' KV cache. Building on this representation, we
employ a two-stage training paradigm: first locating a dedicated retrieval
layer within the LLM, and then applying an attention-based loss at this layer
to explicitly supervise attention toward relevant KB entries. Unlike
traditional retrieval-augmented generation methods that rely heavily on the
performance of external retrievers and multi-stage pipelines, SR-KI supports
end-to-end inference by performing retrieval entirely within the models latent
space. This design enables efficient compression of injected knowledge and
facilitates dynamic knowledge updates. Comprehensive experiments demonstrate
that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single
A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98%
Recall@10 on the best-performing task and exceeding 88% on average across all
tasks. Task performance on question answering and KB ID generation also
demonstrates that SR-KI maintains strong performance while achieving up to
99.75% compression of the injected KBs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06446v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06446v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid expansion of the Internet of Things (IoT) is reshaping
communication and operational practices across industries, but it also broadens
the attack surface and increases susceptibility to security breaches.
Artificial Intelligence has become a valuable solution in securing IoT
networks, with Large Language Models (LLMs) enabling automated attack behavior
analysis and mitigation suggestion in Network Intrusion Detection Systems
(NIDS). Despite advancements, the use of LLMs in such systems further expands
the attack surface, putting entire networks at risk by introducing
vulnerabilities such as prompt injection and data poisoning. In this work, we
attack an LLM-based IoT attack analysis and mitigation framework to test its
adversarial robustness. We construct an attack description dataset and use it
in a targeted data poisoning attack that applies word-level, meaning-preserving
perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge
base of the framework. We then compare pre-attack and post-attack mitigation
responses from the target model, ChatGPT-5 Thinking, to measure the impact of
the attack on model performance, using an established evaluation rubric
designed for human experts and judge LLMs. Our results show that small
perturbations degrade LLM performance by weakening the linkage between observed
network traffic features and attack behavior, and by reducing the specificity
and practicality of recommended mitigations for resource-constrained devices.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06212v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06212v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-7" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RL
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 117 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在强化学习（RL）领域的研究动态主要集中在提升模型的可靠性与安全性、增强对多样化任务的适应能力以及推动公平性和包容性。诸如CoFineLLM和LLM For Loop Invariant Generation等研究表明，利用大型语言模型（LLMs）进行任务规划和程序验证的潜力正在被深入挖掘。同时，针对数据隐私和公平性问题的研究也在不断发展，强调了在算法设计中考虑社会影响的重要性。这些趋势不仅推动了技术的进步，也为解决现实世界中的复杂问题提供了新的思路与方法。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoFineLLM: Conformal Finetuning of LLMs for Language-Instructed Robot Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have recently emerged as planners for
language-instructed agents, generating sequences of actions to accomplish
natural language tasks. However, their reliability remains a challenge,
especially in long-horizon tasks, since they often produce overconfident yet
wrong outputs. Conformal Prediction (CP) has been leveraged to address this
issue by wrapping LLM outputs into prediction sets that contain the correct
action with a user-defined confidence. When the prediction set is a singleton,
the planner executes that action; otherwise, it requests help from a user. This
has led to LLM-based planners that can ensure plan correctness with a
user-defined probability. However, as LLMs are trained in an
uncertainty-agnostic manner, without awareness of prediction sets, they tend to
produce unnecessarily large sets, particularly at higher confidence levels,
resulting in frequent human interventions limiting autonomous deployment. To
address this, we introduce CoFineLLM (Conformal Finetuning for LLMs), the first
CP-aware finetuning framework for LLM-based planners that explicitly reduces
prediction-set size and, in turn, the need for user interventions. We evaluate
our approach on multiple language-instructed robot planning problems and show
consistent improvements over uncertainty-aware and uncertainty-agnostic
finetuning baselines in terms of prediction-set size, and help rates. Finally,
we demonstrate robustness of our method to out-of-distribution scenarios in
hardware experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06575v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06575v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SteganoSNN: SNN-Based Audio-in-Image Steganography with Encryption
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Secure data hiding remains a fundamental challenge in digital communication,
requiring a careful balance between computational efficiency and perceptual
transparency. The balance between security and performance is increasingly
fragile with the emergence of generative AI systems capable of autonomously
generating and optimising sophisticated cryptanalysis and steganalysis
algorithms, thereby accelerating the exposure of vulnerabilities in
conventional data-hiding schemes.
  This work introduces SteganoSNN, a neuromorphic steganographic framework that
exploits spiking neural networks (SNNs) to achieve secure, low-power, and
high-capacity multimedia data hiding. Digitised audio samples are converted
into spike trains using leaky integrate-and-fire (LIF) neurons, encrypted via a
modulo-based mapping scheme, and embedded into the least significant bits of
RGBA image channels using a dithering mechanism to minimise perceptual
distortion. Implemented in Python using NEST and realised on a PYNQ-Z2 FPGA,
SteganoSNN attains real-time operation with an embedding capacity of 8 bits per
pixel. Experimental evaluations on the DIV2K 2017 dataset demonstrate image
fidelity between 40.4 dB and 41.35 dB in PSNR and SSIM values consistently
above 0.97, surpassing SteganoGAN in computational efficiency and robustness.
SteganoSNN establishes a foundation for neuromorphic steganography, enabling
secure, energy-efficient communication for Edge-AI, IoT, and biomedical
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06573v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06573v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rep2Text: Decoding Full Text from a Single LLM Token Representation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) have achieved remarkable progress across diverse
tasks, yet their internal mechanisms remain largely opaque. In this work, we
address a fundamental question: to what extent can the original input text be
recovered from a single last-token representation within an LLM? We propose
Rep2Text, a novel framework for decoding full text from last-token
representations. Rep2Text employs a trainable adapter that projects a target
model's internal representations into the embedding space of a decoding
language model, which then autoregressively reconstructs the input text.
Experiments on various model combinations (Llama-3.1-8B, Gemma-7B,
Mistral-7B-v0.1, Llama-3.2-3B) demonstrate that, on average, over half of the
information in 16-token sequences can be recovered from this compressed
representation while maintaining strong semantic integrity and coherence.
Furthermore, our analysis reveals an information bottleneck effect: longer
sequences exhibit decreased token-level recovery while preserving strong
semantic integrity. Besides, our framework also demonstrates robust
generalization to out-of-distribution medical data.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06571v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06571v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Breaking the Dyadic Barrier: Rethinking Fairness in Link Prediction Beyond Demographic Parity
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Link prediction is a fundamental task in graph machine learning with
applications, ranging from social recommendation to knowledge graph completion.
Fairness in this setting is critical, as biased predictions can exacerbate
societal inequalities. Prior work adopts a dyadic definition of fairness,
enforcing fairness through demographic parity between intra-group and
inter-group link predictions. However, we show that this dyadic framing can
obscure underlying disparities across subgroups, allowing systemic biases to go
undetected. Moreover, we argue that demographic parity does not meet desired
properties for fairness assessment in ranking-based tasks such as link
prediction. We formalize the limitations of existing fairness evaluations and
propose a framework that enables a more expressive assessment. Additionally, we
propose a lightweight post-processing method combined with decoupled link
predictors that effectively mitigates bias and achieves state-of-the-art
fairness-utility trade-offs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06568v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06568v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM For Loop Invariant Generation and Fixing: How Far Are We?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A loop invariant is a property of a loop that remains true before and after
each execution of the loop. The identification of loop invariants is a critical
step to support automated program safety assessment. Recent advancements in
Large Language Models (LLMs) have demonstrated potential in diverse software
engineering (SE) and formal verification tasks. However, we are not aware of
the performance of LLMs to infer loop invariants. We report an empirical study
of both open-source and closed-source LLMs of varying sizes to assess their
proficiency in inferring inductive loop invariants for programs and in fixing
incorrect invariants. Our findings reveal that while LLMs exhibit some utility
in inferring and repairing loop invariants, their performance is substantially
enhanced when supplemented with auxiliary information such as domain knowledge
and illustrative examples. LLMs achieve a maximum success rate of 78\% in
generating, but are limited to 16\% in repairing the invariant.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06552v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06552v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ibom NLP: A Step Toward Inclusive Natural Language Processing for Nigeria's Minority Languages
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Nigeria is the most populous country in Africa with a population of more than
200 million people. More than 500 languages are spoken in Nigeria and it is one
of the most linguistically diverse countries in the world. Despite this,
natural language processing (NLP) research has mostly focused on the following
four languages: Hausa, Igbo, Nigerian-Pidgin, and Yoruba (i.e <1% of the
languages spoken in Nigeria). This is in part due to the unavailability of
textual data in these languages to train and apply NLP algorithms. In this
work, we introduce ibom -- a dataset for machine translation and topic
classification in four Coastal Nigerian languages from the Akwa Ibom State
region: Anaang, Efik, Ibibio, and Oro. These languages are not represented in
Google Translate or in major benchmarks such as Flores-200 or SIB-200. We focus
on extending Flores-200 benchmark to these languages, and further align the
translated texts with topic labels based on SIB-200 classification dataset. Our
evaluation shows that current LLMs perform poorly on machine translation for
these languages in both zero-and-few shot settings. However, we find the
few-shot samples to steadily improve topic classification with more shots.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06531v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06531v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TriShGAN: Enhancing Sparsity and Robustness in Multivariate Time Series Counterfactuals Explanation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In decision-making processes, stakeholders often rely on counterfactual
explanations, which provide suggestions about what should be changed in the
queried instance to alter the outcome of an AI system. However, generating
these explanations for multivariate time series presents challenges due to
their complex, multi-dimensional nature. Traditional Nearest Unlike
Neighbor-based methods typically substitute subsequences in a queried time
series with influential subsequences from an NUN, which is not always realistic
in real-world scenarios due to the rigid direct substitution. Counterfactual
with Residual Generative Adversarial Networks-based methods aim to address this
by learning from the distribution of observed data to generate synthetic
counterfactual explanations. However, these methods primarily focus on
minimizing the cost from the queried time series to the counterfactual
explanations and often neglect the importance of distancing the counterfactual
explanation from the decision boundary. This oversight can result in
explanations that no longer qualify as counterfactual if minor changes occur
within the model. To generate a more robust counterfactual explanation, we
introduce TriShGAN, under the CounteRGAN framework enhanced by the
incorporation of triplet loss. This unsupervised learning approach uses
distance metric learning to encourage the counterfactual explanations not only
to remain close to the queried time series but also to capture the feature
distribution of the instance with the desired outcome, thereby achieving a
better balance between minimal cost and robustness. Additionally, we integrate
a Shapelet Extractor that strategically selects the most discriminative parts
of the high-dimensional queried time series to enhance the sparsity of
counterfactual explanation and efficiency of the training process.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06529v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06529v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FractalBench: Diagnosing Visual-Mathematical Reasoning Through Recursive Program Synthesis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mathematical reasoning requires abstracting symbolic rules from visual
patterns -- inferring the infinite from the finite. We investigate whether
multimodal AI systems possess this capability through FractalBench, a benchmark
evaluating fractal program synthesis from images. Fractals provide ideal test
cases: Iterated Function Systems with only a few contraction maps generate
complex self-similar patterns through simple recursive rules, requiring models
to bridge visual perception with mathematical abstraction. We evaluate four
leading MLLMs -- GPT-4o, Claude 3.7 Sonnet, Gemini 2.5 Flash, and Qwen 2.5-VL
-- on 12 canonical fractals. Models must generate executable Python code
reproducing the fractal, enabling objective evaluation. Results reveal a
striking disconnect: 76% generate syntactically valid code but only 4% capture
mathematical structure. Success varies systematically -- models handle
geometric transformations (Koch curves: 17-21%) but fail at branching recursion
(trees: <2%), revealing fundamental gaps in mathematical abstraction.
FractalBench provides a contamination-resistant diagnostic for
visual-mathematical reasoning and is available at
https://github.com/NaiveNeuron/FractalBench
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06522v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06522v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On the Analogy between Human Brain and LLMs: Spotting Key Neurons in Grammar Perception
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Artificial Neural Networks, the building blocks of AI, were inspired by the
human brain's network of neurons. Over the years, these networks have evolved
to replicate the complex capabilities of the brain, allowing them to handle
tasks such as image and language processing. In the realm of Large Language
Models, there has been a keen interest in making the language learning process
more akin to that of humans. While neuroscientific research has shown that
different grammatical categories are processed by different neurons in the
brain, we show that LLMs operate in a similar way. Utilizing Llama 3, we
identify the most important neurons associated with the prediction of words
belonging to different part-of-speech tags. Using the achieved knowledge, we
train a classifier on a dataset, which shows that the activation patterns of
these key neurons can reliably predict part-of-speech tags on fresh data. The
results suggest the presence of a subspace in LLMs focused on capturing
part-of-speech tag concepts, resembling patterns observed in lesion studies of
the brain in neuroscience.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06519v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06519v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rethinking what Matters: Effective and Robust Multilingual Realignment for Low-Resource Languages
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Realignment is a promising strategy to improve cross-lingual transfer in
multilingual language models. However, empirical results are mixed and often
unreliable, particularly for typologically distant or low-resource languages
(LRLs) compared to English. Moreover, word realignment tools often rely on
high-quality parallel data, which can be scarce or noisy for many LRLs. In this
work, we conduct an extensive empirical study to investigate whether
realignment truly benefits from using all available languages, or if
strategically selected subsets can offer comparable or even improved
cross-lingual transfer, and study the impact on LRLs. Our controlled
experiments show that realignment can be particularly effective for LRLs and
that using carefully selected, linguistically diverse subsets can match full
multilingual alignment, and even outperform it for unseen LRLs. This indicates
that effective realignment does not require exhaustive language coverage and
can reduce data collection overhead, while remaining both efficient and robust
when guided by informed language selection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06497v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06497v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Low-Rank Method for Vision Language Model Hallucination Mitigation in Autonomous Driving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Vision Language Models (VLMs) are increasingly used in autonomous driving to
help understand traffic scenes, but they sometimes produce hallucinations,
which are false details not grounded in the visual input. Detecting and
mitigating hallucinations is challenging when ground-truth references are
unavailable and model internals are inaccessible. This paper proposes a novel
self-contained low-rank approach to automatically rank multiple candidate
captions generated by multiple VLMs based on their hallucination levels, using
only the captions themselves without requiring external references or model
access. By constructing a sentence-embedding matrix and decomposing it into a
low-rank consensus component and a sparse residual, we use the residual
magnitude to rank captions: selecting the one with the smallest residual as the
most hallucination-free. Experiments on the NuScenes dataset demonstrate that
our approach achieves 87% selection accuracy in identifying hallucination-free
captions, representing a 19% improvement over the unfiltered baseline and a
6-10% improvement over multi-agent debate method. The sorting produced by
sparse error magnitudes shows strong correlation with human judgments of
hallucinations, validating our scoring mechanism. Additionally, our method,
which can be easily parallelized, reduces inference time by 51-67% compared to
debate approaches, making it practical for real-time autonomous driving
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06496v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06496v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Route Experts by Sequence, not by Token
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mixture-of-Experts (MoE) architectures scale large language models (LLMs) by
activating only a subset of experts per token, but the standard TopK routing
assigns the same fixed number of experts to all tokens, ignoring their varying
complexity. Prior adaptive routing methods introduce additional modules and
hyperparameters, often requiring costly retraining from scratch. We propose
Sequence-level TopK (SeqTopK), a minimal modification that shifts the expert
budget from the token level to the sequence level. By selecting the top $T
\cdot K$ experts across all $T$ tokens, SeqTopK enables end-to-end learned
dynamic allocation -- assigning more experts to difficult tokens and fewer to
easy ones -- while preserving the same overall budget. SeqTopK requires only a
few lines of code, adds less than 1% overhead, and remains fully compatible
with pretrained MoE models. Experiments across math, coding, law, and writing
show consistent improvements over TopK and prior parameter-free adaptive
methods, with gains that become substantially larger under higher sparsity (up
to 16.9%). These results highlight SeqTopK as a simple, efficient, and scalable
routing strategy, particularly well-suited for the extreme sparsity regimes of
next-generation LLMs. Code is available at
https://github.com/Y-Research-SBU/SeqTopK.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06494v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06494v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Explainable AI For Early Detection Of Sepsis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sepsis is a life-threatening condition that requires rapid detection and
treatment to prevent progression to severe sepsis, septic shock, or multi-organ
failure. Despite advances in medical technology, it remains a major challenge
for clinicians. While recent machine learning models have shown promise in
predicting sepsis onset, their black-box nature limits interpretability and
clinical trust. In this study, we present an interpretable AI approach for
sepsis analysis that integrates machine learning with clinical knowledge. Our
method not only delivers accurate predictions of sepsis onset but also enables
clinicians to understand, validate, and align model outputs with established
medical expertise.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06492v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06492v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06490v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06490v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GHOST: Solving the Traveling Salesman Problem on Graphs of Convex Sets
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study GCS-TSP, a new variant of the Traveling Salesman Problem (TSP)
defined over a Graph of Convex Sets (GCS) -- a powerful representation for
trajectory planning that decomposes the configuration space into convex regions
connected by a sparse graph. In this setting, edge costs are not fixed but
depend on the specific trajectory selected through each convex region, making
classical TSP methods inapplicable. We introduce GHOST, a hierarchical
framework that optimally solves the GCS-TSP by combining combinatorial tour
search with convex trajectory optimization. GHOST systematically explores tours
on a complete graph induced by the GCS, using a novel abstract-path-unfolding
algorithm to compute admissible lower bounds that guide best-first search at
both the high level (over tours) and the low level (over feasible GCS paths
realizing the tour). These bounds provide strong pruning power, enabling
efficient search while avoiding unnecessary convex optimization calls. We prove
that GHOST guarantees optimality and present a bounded-suboptimal variant for
time-critical scenarios. Experiments show that GHOST is orders-of-magnitude
faster than unified mixed-integer convex programming baselines for simple cases
and uniquely handles complex trajectory planning problems involving high-order
continuity constraints and an incomplete GCS.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06471v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06471v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Brain-Inspired Planning for Better Generalization in Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Existing Reinforcement Learning (RL) systems encounter significant challenges
when applied to real-world scenarios, primarily due to poor generalization
across environments that differ from their training conditions. This thesis
explores the direction of enhancing agents' zero-shot systematic generalization
abilities by granting RL agents reasoning behaviors that are found to help
systematic generalization in the human brain. Inspired by human conscious
planning behaviors, we first introduced a top-down attention mechanism, which
allows a decision-time planning agent to dynamically focus its reasoning on the
most relevant aspects of the environmental state given its instantaneous
intentions, a process we call "spatial abstraction". This approach
significantly improves systematic generalization outside the training tasks.
Subsequently, building on spatial abstraction, we developed the Skipper
framework to automatically decompose complex tasks into simpler, more
manageable sub-tasks. Skipper provides robustness against distributional shifts
and efficacy in long-term, compositional planning by focusing on pertinent
spatial and temporal elements of the environment. Finally, we identified a
common failure mode and safety risk in planning agents that rely on generative
models to generate state targets during planning. It is revealed that most
agents blindly trust the targets they hallucinate, resulting in delusional
planning behaviors. Inspired by how the human brain rejects delusional
intentions, we propose learning a feasibility evaluator to enable rejecting
hallucinated infeasible targets, which led to significant performance
improvements in various kinds of planning agents. Finally, we suggest
directions for future research, aimed at achieving general task abstraction and
fully enabling abstract planning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06470v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06470v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EchoMark: Perceptual Acoustic Environment Transfer with Watermark-Embedded Room Impulse Response
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Acoustic Environment Matching (AEM) is the task of transferring clean audio
into a target acoustic environment, enabling engaging applications such as
audio dubbing and auditory immersive virtual reality (VR). Recovering similar
room impulse response (RIR) directly from reverberant speech offers more
accessible and flexible AEM solution. However, this capability also introduces
vulnerabilities of arbitrary ``relocation" if misused by malicious user, such
as facilitating advanced voice spoofing attacks or undermining the authenticity
of recorded evidence. To address this issue, we propose EchoMark, the first
deep learning-based AEM framework that generates perceptually similar RIRs with
embedded watermark. Our design tackle the challenges posed by variable RIR
characteristics, such as different durations and energy decays, by operating in
the latent domain. By jointly optimizing the model with a perceptual loss for
RIR reconstruction and a loss for watermark detection, EchoMark achieves both
high-quality environment transfer and reliable watermark recovery. Experiments
on diverse datasets validate that EchoMark achieves room acoustic parameter
matching performance comparable to FiNS, the state-of-the-art RIR estimator.
Furthermore, a high Mean Opinion Score (MOS) of 4.22 out of 5, watermark
detection accuracy exceeding 99\%, and bit error rates (BER) below 0.3\%
collectively demonstrate the effectiveness of EchoMark in preserving perceptual
quality while ensuring reliable watermark embedding.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06458v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06458v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Multi-Agent System for Semantic Mapping of Relational Data to Knowledge Graphs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Enterprises often maintain multiple databases for storing critical business
data in siloed systems, resulting in inefficiencies and challenges with data
interoperability. A key to overcoming these challenges lies in integrating
disparate data sources, enabling businesses to unlock the full potential of
their data. Our work presents a novel approach for integrating multiple
databases using knowledge graphs, focusing on the application of large language
models as semantic agents for mapping and connecting structured data across
systems by leveraging existing vocabularies. The proposed methodology
introduces a semantic layer above tables in relational databases, utilizing a
system comprising multiple LLM agents that map tables and columns to Schema.org
terms. Our approach achieves a mapping accuracy of over 90% in multiple
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06455v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06455v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FLEX: Continuous Agent Evolution via Forward Learning from Experience
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Autonomous agents driven by Large Language Models (LLMs) have revolutionized
reasoning and problem-solving but remain static after training, unable to grow
with experience as intelligent beings do during deployment. We introduce
Forward Learning with EXperience (FLEX), a gradient-free learning paradigm that
enables LLM agents to continuously evolve through accumulated experience.
Specifically, FLEX cultivates scalable and inheritable evolution by
constructing a structured experience library through continual reflection on
successes and failures during interaction with the environment. FLEX delivers
substantial improvements on mathematical reasoning, chemical retrosynthesis,
and protein fitness prediction (up to 23% on AIME25, 10% on USPTO50k, and 14%
on ProteinGym). We further identify a clear scaling law of experiential growth
and the phenomenon of experience inheritance across agents, marking a step
toward scalable and inheritable continuous agent evolution. Project Page:
https://flex-gensi-thuair.github.io.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06449v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06449v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When AI Agents Collude Online: Financial Fraud Risks by Collaborative LLM Agents on Social Platforms
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this work, we study the risks of collective financial fraud in large-scale
multi-agent systems powered by large language model (LLM) agents. We
investigate whether agents can collaborate in fraudulent behaviors, how such
collaboration amplifies risks, and what factors influence fraud success. To
support this research, we present MultiAgentFraudBench, a large-scale benchmark
for simulating financial fraud scenarios based on realistic online
interactions. The benchmark covers 28 typical online fraud scenarios, spanning
the full fraud lifecycle across both public and private domains. We further
analyze key factors affecting fraud success, including interaction depth,
activity level, and fine-grained collaboration failure modes. Finally, we
propose a series of mitigation strategies, including adding content-level
warnings to fraudulent posts and dialogues, using LLMs as monitors to block
potentially malicious agents, and fostering group resilience through
information sharing at the societal level. Notably, we observe that malicious
agents can adapt to environmental interventions. Our findings highlight the
real-world risks of multi-agent financial fraud and suggest practical measures
for mitigating them. Code is available at
https://github.com/zheng977/MutiAgent4Fraud.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06448v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06448v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Conversational search interfaces, like ChatGPT, offer an interactive,
personalized, and engaging user experience compared to traditional search. On
the downside, they are prone to cause overtrust issues where users rely on
their responses even when they are incorrect. What aspects of the
conversational interaction paradigm drive people to adopt it, and how it
creates personalized experiences that lead to overtrust, is not clear. To
understand the factors influencing the adoption of conversational interfaces,
we conducted a survey with 173 participants. We examined user perceptions
regarding trust, human-likeness (anthropomorphism), and design preferences
between ChatGPT and Google. To better understand the overtrust phenomenon, we
asked users about their willingness to trade off factuality for constructs like
ease of use or human-likeness. Our analysis identified two distinct user
groups: those who use both ChatGPT and Google daily (DUB), and those who
primarily rely on Google (DUG). The DUB group exhibited higher trust in
ChatGPT, perceiving it as more human-like, and expressed greater willingness to
trade factual accuracy for enhanced personalization and conversational flow.
Conversely, the DUG group showed lower trust toward ChatGPT but still
appreciated aspects like ad-free experiences and responsive interactions.
Demographic analysis further revealed nuanced patterns, with middle-aged adults
using ChatGPT less frequently yet trusting it more, suggesting potential
vulnerability to misinformation. Our findings contribute to understanding user
segmentation, emphasizing the critical roles of personalization and
human-likeness in conversational IR systems, and reveal important implications
regarding users' willingness to compromise factual accuracy for more engaging
interactions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06447v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06447v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SR-KI: Scalable and Real-Time Knowledge Integration into LLMs via Supervised Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper proposes SR-KI, a novel approach for integrating real-time and
large-scale structured knowledge bases (KBs) into large language models (LLMs).
SR-KI begins by encoding KBs into key-value pairs using a pretrained encoder,
and injects them into LLMs' KV cache. Building on this representation, we
employ a two-stage training paradigm: first locating a dedicated retrieval
layer within the LLM, and then applying an attention-based loss at this layer
to explicitly supervise attention toward relevant KB entries. Unlike
traditional retrieval-augmented generation methods that rely heavily on the
performance of external retrievers and multi-stage pipelines, SR-KI supports
end-to-end inference by performing retrieval entirely within the models latent
space. This design enables efficient compression of injected knowledge and
facilitates dynamic knowledge updates. Comprehensive experiments demonstrate
that SR-KI enables the integration of up to 40K KBs into a 7B LLM on a single
A100 40GB GPU, and achieves strong retrieval performance, maintaining over 98%
Recall@10 on the best-performing task and exceeding 88% on average across all
tasks. Task performance on question answering and KB ID generation also
demonstrates that SR-KI maintains strong performance while achieving up to
99.75% compression of the injected KBs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06446v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06446v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Optimizing Chain-of-Thought Confidence via Topological and Dirichlet Risk Analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chain-of-thought (CoT) prompting enables Large Language Models to solve
complex problems, but deploying these models safely requires reliable
confidence estimates, a capability where existing methods suffer from poor
calibration and severe overconfidence on incorrect predictions. We propose
Enhanced Dirichlet and Topology Risk (EDTR), a novel decoding strategy that
combines topological analysis with Dirichlet-based uncertainty quantification
to measure LLM confidence across multiple reasoning paths. EDTR treats each CoT
as a vector in high-dimensional space and extracts eight topological risk
features capturing the geometric structure of reasoning distributions: tighter,
more coherent clusters indicate higher confidence while dispersed, inconsistent
paths signal uncertainty. We evaluate EDTR against three state-of-the-art
calibration methods across four diverse reasoning benchmarks spanning
olympiad-level mathematics (AIME), grade school math (GSM8K), commonsense
reasoning, and stock price prediction \cite{zhang2025aime, cobbe2021training,
talmor-etal-2019-commonsenseqa, yahoo_finance}. EDTR achieves 41\% better
calibration than competing methods with an average ECE of 0.287 and the best
overall composite score of 0.672, while notably achieving perfect accuracy on
AIME and exceptional calibration on GSM8K with an ECE of 0.107, domains where
baselines exhibit severe overconfidence. Our work provides a geometric
framework for understanding and quantifying uncertainty in multi-step LLM
reasoning, enabling more reliable deployment where calibrated confidence
estimates are essential.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06437v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06437v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Walking the Tightrope of LLMs for Software Development: A Practitioners' Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Background: Large Language Models emerged with the potential of provoking a
revolution in software development (e.g., automating processes, workforce
transformation). Although studies have started to investigate the perceived
impact of LLMs for software development, there is a need for empirical studies
to comprehend how to balance forward and backward effects of using LLMs.
Objective: We investigated how LLMs impact software development and how to
manage the impact from a software developer's perspective. Method: We conducted
22 interviews with software practitioners across 3 rounds of data collection
and analysis, between October (2024) and September (2025). We employed
socio-technical grounded theory (STGT) for data analysis to rigorously analyse
interview participants' responses. Results: We identified the benefits (e.g.,
maintain software development flow, improve developers' mental model, and
foster entrepreneurship) and disadvantages (e.g., negative impact on
developers' personality and damage to developers' reputation) of using LLMs at
individual, team, organisation, and society levels; as well as best practices
on how to adopt LLMs. Conclusion: Critically, we present the trade-offs that
software practitioners, teams, and organisations face in working with LLMs. Our
findings are particularly useful for software team leaders and IT managers to
assess the viability of LLMs within their specific context.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06428v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06428v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While zero-shot diffusion-based compression methods have seen significant
progress in recent years, they remain notoriously slow and computationally
demanding. This paper presents an efficient zero-shot diffusion-based
compression method that runs substantially faster than existing methods, while
maintaining performance that is on par with the state-of-the-art techniques.
Our method builds upon the recently proposed Denoising Diffusion Codebook
Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by
sequentially choosing the diffusion noise vectors from reproducible random
codebooks, guiding the denoiser's output to reconstruct the target image. We
modify this framework with Turbo-DDCM, which efficiently combines a large
number of noise vectors at each denoising step, thereby significantly reducing
the number of required denoising operations. This modification is also coupled
with an improved encoding protocol. Furthermore, we introduce two flexible
variants of Turbo-DDCM, a priority-aware variant that prioritizes
user-specified regions and a distortion-controlled variant that compresses an
image based on a target PSNR rather than a target BPP. Comprehensive
experiments position Turbo-DDCM as a compelling, practical, and flexible image
compression scheme.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06424v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06424v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MONICA: Real-Time Monitoring and Calibration of Chain-of-Thought Sycophancy in Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Reasoning Models (LRMs) suffer from sycophantic behavior, where models
tend to agree with users' incorrect beliefs and follow misinformation rather
than maintain independent reasoning. This behavior undermines model reliability
and poses societal risks. Mitigating LRM sycophancy requires monitoring how
this sycophancy emerges during the reasoning trajectory; however, current
methods mainly focus on judging based on final answers and correcting them,
without understanding how sycophancy develops during reasoning processes. To
address this limitation, we propose MONICA, a novel Monitor-guided Calibration
framework that monitors and mitigates sycophancy during model inference at the
level of reasoning steps, without requiring the model to finish generating its
complete answer. MONICA integrates a sycophantic monitor that provides
real-time monitoring of sycophantic drift scores during response generation
with a calibrator that dynamically suppresses sycophantic behavior when scores
exceed predefined thresholds. Extensive experiments across 12 datasets and 3
LRMs demonstrate that our method effectively reduces sycophantic behavior in
both intermediate reasoning steps and final answers, yielding robust
performance improvements.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06419v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06419v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AUTO-Explorer: Automated Data Collection for GUI Agent
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in GUI agents have significantly expanded their ability
to interpret natural language commands to manage software interfaces. However,
acquiring GUI data remains a significant challenge. Existing methods often
involve designing automated agents that browse URLs from the Common Crawl,
using webpage HTML to collect screenshots and corresponding annotations,
including the names and bounding boxes of UI elements. However, this method is
difficult to apply to desktop software or some newly launched websites not
included in the Common Crawl. While we expect the model to possess strong
generalization capabilities to handle this, it is still crucial for
personalized scenarios that require rapid and perfect adaptation to new
software or websites. To address this, we propose an automated data collection
method with minimal annotation costs, named Auto-Explorer. It incorporates a
simple yet effective exploration mechanism that autonomously parses and
explores GUI environments, gathering data efficiently. Additionally, to assess
the quality of exploration, we have developed the UIXplore benchmark. This
benchmark creates environments for explorer agents to discover and save
software states. Using the data gathered, we fine-tune a multimodal large
language model (MLLM) and establish a GUI element grounding testing set to
evaluate the effectiveness of the exploration strategies. Our experiments
demonstrate the superior performance of Auto-Explorer, showing that our method
can quickly enhance the capabilities of an MLLM in explored software.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06417v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06417v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SofT-GRPO: Surpassing Discrete-Token LLM Reinforcement Learning via Gumbel-Reparameterized Soft-Thinking Policy Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The soft-thinking paradigm for Large Language Model (LLM) reasoning can
outperform the conventional discrete-token Chain-of-Thought (CoT) reasoning in
some scenarios, underscoring its research and application value. However, while
the discrete-token CoT reasoning pattern can be reinforced through policy
optimization algorithms such as group relative policy optimization (GRPO),
extending the soft-thinking pattern with Reinforcement Learning (RL) remains
challenging. This difficulty stems from the complexities of injecting
stochasticity into soft-thinking tokens and updating soft-thinking policies
accordingly. As a result, previous attempts to combine soft-thinking with GRPO
typically underperform their discrete-token GRPO counterparts. To fully unlock
the potential of soft-thinking, this paper presents a novel policy optimization
algorithm, SofT-GRPO, to reinforce LLMs under the soft-thinking reasoning
pattern. SofT-GRPO injects the Gumbel noise into logits, employs the
Gumbel-Softmax technique to avoid soft-thinking tokens outside the pre-trained
embedding space, and leverages the reparameterization trick in policy gradient.
We conduct experiments across base LLMs ranging from 1.5B to 7B parameters, and
results demonstrate that SofT-GRPO enables soft-thinking LLMs to slightly
outperform discrete-token GRPO on Pass@1 (+0.13% on average accuracy), while
exhibiting a substantial uplift on Pass@32 (+2.19% on average accuracy). Codes
and weights are available on https://github.com/zz1358m/SofT-GRPO-master
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06411v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06411v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06406v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06406v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient LLM Safety Evaluation through Multi-Agent Debate
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Safety evaluation of large language models (LLMs) increasingly relies on
LLM-as-a-Judge frameworks, but the high cost of frontier models limits
scalability. We propose a cost-efficient multi-agent judging framework that
employs Small Language Models (SLMs) through structured debates among critic,
defender, and judge agents. To rigorously assess safety judgments, we construct
HAJailBench, a large-scale human-annotated jailbreak benchmark comprising
12,000 adversarial interactions across diverse attack methods and target
models. The dataset provides fine-grained, expert-labeled ground truth for
evaluating both safety robustness and judge reliability. Our SLM-based
framework achieves agreement comparable to GPT-4o judges on HAJailBench while
substantially reducing inference cost. Ablation results show that three rounds
of debate yield the optimal balance between accuracy and efficiency. These
findings demonstrate that structured, value-aligned debate enables SLMs to
capture semantic nuances of jailbreak attacks and that HAJailBench offers a
reliable foundation for scalable LLM safety evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06396v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06396v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        HatePrototypes: Interpretable and Transferable Representations for Implicit and Explicit Hate Speech Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Optimization of offensive content moderation models for different types of
hateful messages is typically achieved through continued pre-training or
fine-tuning on new hate speech benchmarks. However, existing benchmarks mainly
address explicit hate toward protected groups and often overlook implicit or
indirect hate, such as demeaning comparisons, calls for exclusion or violence,
and subtle discriminatory language that still causes harm. While explicit hate
can often be captured through surface features, implicit hate requires deeper,
full-model semantic processing. In this work, we question the need for repeated
fine-tuning and analyze the role of HatePrototypes, class-level vector
representations derived from language models optimized for hate speech
detection and safety moderation. We find that these prototypes, built from as
few as 50 examples per class, enable cross-task transfer between explicit and
implicit hate, with interchangeable prototypes across benchmarks. Moreover, we
show that parameter-free early exiting with prototypes is effective for both
hate types. We release the code, prototype resources, and evaluation scripts to
support future research on efficient and transferable hate speech detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06391v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06391v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ghost in the Transformer: Tracing LLM Lineage with SVD-Fingerprint
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have rapidly advanced and are widely adopted
across diverse fields. Due to the substantial computational cost and data
requirements of training from scratch, many developers choose to fine-tune or
modify existing open-source models. While most adhere to open-source licenses,
some falsely claim original training despite clear derivation from public
models. This raises pressing concerns about intellectual property protection
and highlights the need for reliable methods to verify model provenance. In
this paper, we propose GhostSpec, a lightweight yet effective method for
verifying LLM lineage without access to training data or modification of model
behavior. Our approach constructs compact and robust fingerprints by applying
singular value decomposition (SVD) to invariant products of internal attention
weight matrices, effectively capturing the structural identity of a model.
Unlike watermarking or output-based methods, GhostSpec is fully data-free,
non-invasive, and computationally efficient. It demonstrates strong robustness
to sequential fine-tuning, pruning, block expansion, and even adversarial
transformations. Extensive experiments show that GhostSpec can reliably trace
the lineage of transformed models with minimal overhead. By offering a
practical solution for model verification and reuse tracking, our method
contributes to the protection of intellectual property and fosters a
transparent, trustworthy ecosystem for large-scale language models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06390v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06390v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        HyMoERec: Hybrid Mixture-of-Experts for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose HyMoERec, a novel sequential recommendation framework that
addresses the limitations of uniform Position-wise Feed-Forward Networks in
existing models. Current approaches treat all user interactions and items
equally, overlooking the heterogeneity in user behavior patterns and diversity
in item complexity. HyMoERec initially introduces a hybrid mixture-of-experts
architecture that combines shared and specialized expert branches with an
adaptive expert fusion mechanism for the sequential recommendation task. This
design captures diverse reasoning for varied users and items while ensuring
stable training. Experiments on MovieLens-1M and Beauty datasets demonstrate
that HyMoERec consistently outperforms state-of-the-art baselines.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06388v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06388v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        What Makes Reasoning Invalid: Echo Reflection Mitigation for Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have demonstrated remarkable performance across
a wide range of reasoning tasks. Recent methods have further improved LLM
performance in complex mathematical reasoning. However, when extending these
methods beyond the domain of mathematical reasoning to tasks involving complex
domain-specific knowledge, we observe a consistent failure of LLMs to generate
novel insights during the reflection stage. Instead of conducting genuine
cognitive refinement, the model tends to mechanically reiterate earlier
reasoning steps without introducing new information or perspectives, a
phenomenon referred to as "Echo Reflection". We attribute this behavior to two
key defects: (1) Uncontrollable information flow during response generation,
which allows premature intermediate thoughts to propagate unchecked and distort
final decisions; (2) Insufficient exploration of internal knowledge during
reflection, leading to repeating earlier findings rather than generating new
cognitive insights. Building on these findings, we proposed a novel
reinforcement learning method termed Adaptive Entropy Policy Optimization
(AEPO). Specifically, the AEPO framework consists of two major components: (1)
Reflection-aware Information Filtration, which quantifies the cognitive
information flow and prevents the final answer from being affected by earlier
bad cognitive information; (2) Adaptive-Entropy Optimization, which dynamically
balances exploration and exploitation across different reasoning stages,
promoting both reflective diversity and answer correctness. Extensive
experiments demonstrate that AEPO consistently achieves state-of-the-art
performance over mainstream reinforcement learning baselines across diverse
benchmarks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06380v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06380v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Privacy-Preserving Federated Learning for Fair and Efficient Urban Traffic Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The optimization of urban traffic is threatened by the complexity of
achieving a balance between transport efficiency and the maintenance of
privacy, as well as the equitable distribution of traffic based on
socioeconomically diverse neighborhoods. Current centralized traffic management
schemes invade user location privacy and further entrench traffic disparity by
offering disadvantaged route suggestions, whereas current federated learning
frameworks do not consider fairness constraints in multi-objective traffic
settings. This study presents a privacy-preserving federated learning
framework, termed FedFair-Traffic, that jointly and simultaneously optimizes
travel efficiency, traffic fairness, and differential privacy protection. This
is the first attempt to integrate three conflicting objectives to improve urban
transportation systems. The proposed methodology enables collaborative learning
between related vehicles with data locality by integrating Graph Neural
Networks with differential privacy mechanisms ($\epsilon$-privacy guarantees)
and Gini coefficient-based fair constraints using multi-objective optimization.
The framework uses federated aggregation methods of gradient clipping and noise
injection to provide differential privacy and optimize Pareto-efficient
solutions for the efficiency-fairness tradeoff. Real-world comprehensive
experiments on the METR-LA traffic dataset showed that FedFair-Traffic can
reduce the average travel time by 7\% (14.2 minutes) compared with their
centralized baselines, promote traffic fairness by 73\% (Gini coefficient,
0.78), and offer high privacy protection (privacy score, 0.8) with an 89\%
reduction in communication overhead. These outcomes demonstrate that
FedFair-Traffic is a scalable privacy-aware smart city infrastructure with
possible use-cases in metropolitan traffic flow control and federated
transportation networks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06363v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06363v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Understanding Student Interaction with AI-Powered Next-Step Hints: Strategies and Challenges
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Automated feedback generation plays a crucial role in enhancing personalized
learning experiences in computer science education. Among different types of
feedback, next-step hint feedback is particularly important, as it provides
students with actionable steps to progress towards solving programming tasks.
This study investigates how students interact with an AI-driven next-step hint
system in an in-IDE learning environment. We gathered and analyzed a dataset
from 34 students solving Kotlin tasks, containing detailed hint interaction
logs. We applied process mining techniques and identified 16 common interaction
scenarios. Semi-structured interviews with 6 students revealed strategies for
managing unhelpful hints, such as adapting partial hints or modifying code to
generate variations of the same hint. These findings, combined with our
publicly available dataset, offer valuable opportunities for future research
and provide key insights into student behavior, helping improve hint design for
enhanced learning support.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06362v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06362v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Graph-Theoretical Perspective on Law Design for Multiagent Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A law in a multiagent system is a set of constraints imposed on agents'
behaviours to avoid undesirable outcomes. The paper considers two types of
laws: useful laws that, if followed, completely eliminate the undesirable
outcomes and gap-free laws that guarantee that at least one agent can be held
responsible each time an undesirable outcome occurs. In both cases, we study
the problem of finding a law that achieves the desired result by imposing the
minimum restrictions.
  We prove that, for both types of laws, the minimisation problem is NP-hard
even in the simple case of one-shot concurrent interactions. We also show that
the approximation algorithm for the vertex cover problem in hypergraphs could
be used to efficiently approximate the minimum laws in both cases.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06361v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06361v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reaction Prediction via Interaction Modeling of Symmetric Difference Shingle Sets
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chemical reaction prediction remains a fundamental challenge in organic
chemistry, where existing machine learning models face two critical
limitations: sensitivity to input permutations (molecule/atom orderings) and
inadequate modeling of substructural interactions governing reactivity. These
shortcomings lead to inconsistent predictions and poor generalization to
real-world scenarios. To address these challenges, we propose ReaDISH, a novel
reaction prediction model that learns permutation-invariant representations
while incorporating interaction-aware features. It introduces two innovations:
(1) symmetric difference shingle encoding, which computes molecular shingle
differences to capture reaction-specific structural changes while eliminating
order sensitivity; and (2) geometry-structure interaction attention, a
mechanism that models intra- and inter-molecular interactions at the shingle
level. Extensive experiments demonstrate that ReaDISH improves reaction
prediction performance across diverse benchmarks. It shows enhanced robustness
with an average improvement of 8.76% on R$^2$ under permutation perturbations.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06356v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06356v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06348v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06348v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LPFQA: A Long-Tail Professional Forum-based Benchmark for LLM Evaluation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) have made rapid progress in reasoning, question
answering, and professional applications; however, their true capabilities
remain difficult to evaluate using existing benchmarks. Current datasets often
focus on simplified tasks or artificial scenarios, overlooking long-tail
knowledge and the complexities of real-world applications. To bridge this gap,
we propose LPFQA, a long-tail knowledge-based benchmark derived from authentic
professional forums across 20 academic and industrial fields, covering 502
tasks grounded in practical expertise. LPFQA introduces four key innovations:
fine-grained evaluation dimensions that target knowledge depth, reasoning,
terminology comprehension, and contextual analysis; a hierarchical difficulty
structure that ensures semantic clarity and unique answers; authentic
professional scenario modeling with realistic user personas; and
interdisciplinary knowledge integration across diverse domains. We evaluated 12
mainstream LLMs on LPFQA and observed significant performance disparities,
especially in specialized reasoning tasks. LPFQA provides a robust, authentic,
and discriminative benchmark for advancing LLM evaluation and guiding future
model development.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06346v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06346v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PRAGMA: A Profiling-Reasoned Multi-Agent Framework for Automatic Kernel Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Designing high-performance kernels requires expert-level tuning and a deep
understanding of hardware characteristics. Recent advances in large language
models (LLMs) have enabled automated kernel generation, yet most existing
systems rely solely on correctness or execution time feedback, lacking the
ability to reason about low-level performance bottlenecks. In this paper, we
introduce PRAGMA, a profile-guided AI kernel generation framework that
integrates execution feedback and fine-grained hardware profiling into the
reasoning loop. PRAGMA enables LLMs to identify performance bottlenecks,
preserve historical best versions, and iteratively refine code quality. We
evaluate PRAGMA on KernelBench, covering GPU and CPU backends. Results show
that PRAGMA consistently outperforms baseline AIKG without profiling enabled
and achieves 2.81$\times$ and 2.30$\times$ averaged speedups against Torch on
CPU and GPU platforms, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06345v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06345v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TimeSense:Making Large Language Models Proficient in Time-Series Analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In the time-series domain, an increasing number of works combine text with
temporal data to leverage the reasoning capabilities of large language models
(LLMs) for various downstream time-series understanding tasks. This enables a
single model to flexibly perform tasks that previously required specialized
models for each domain. However, these methods typically rely on text labels
for supervision during training, biasing the model toward textual cues while
potentially neglecting the full temporal features. Such a bias can lead to
outputs that contradict the underlying time-series context. To address this
issue, we construct the EvalTS benchmark, comprising 10 tasks across three
difficulty levels, from fundamental temporal pattern recognition to complex
real-world reasoning, to evaluate models under more challenging and realistic
scenarios. We also propose TimeSense, a multimodal framework that makes LLMs
proficient in time-series analysis by balancing textual reasoning with a
preserved temporal sense. TimeSense incorporates a Temporal Sense module that
reconstructs the input time-series within the model's context, ensuring that
textual reasoning is grounded in the time-series dynamics. Moreover, to enhance
spatial understanding of time-series data, we explicitly incorporate
coordinate-based positional embeddings, which provide each time point with
spatial context and enable the model to capture structural dependencies more
effectively. Experimental results demonstrate that TimeSense achieves
state-of-the-art performance across multiple tasks, and it particularly
outperforms existing methods on complex multi-dimensional time-series reasoning
tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06344v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06344v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06325v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06325v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ALIGN: A Vision-Language Framework for High-Accuracy Accident Location Inference through Geo-Spatial Neural Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliable geospatial information on road accidents is vital for safety
analysis and infrastructure planning, yet most low- and middle-income countries
continue to face a critical shortage of accurate, location-specific crash data.
Existing text-based geocoding tools perform poorly in multilingual and
unstructured news environments, where incomplete place descriptions and mixed
Bangla-English scripts obscure spatial context. To address these limitations,
this study introduces ALIGN (Accident Location Inference through Geo-Spatial
Neural Reasoning)- a vision-language framework that emulates human spatial
reasoning to infer accident coordinates directly from textual and map-based
cues. ALIGN integrates large language and vision-language models within a
multi-stage pipeline that performs optical character recognition, linguistic
reasoning, and map-level verification through grid-based spatial scanning. The
framework systematically evaluates each predicted location against contextual
and visual evidence, ensuring interpretable, fine-grained geolocation outcomes
without requiring model retraining. Applied to Bangla-language news data, ALIGN
demonstrates consistent improvements over traditional geoparsing methods,
accurately identifying district and sub-district-level crash sites. Beyond its
technical contribution, the framework establishes a high accuracy foundation
for automated crash mapping in data-scarce regions, supporting evidence-driven
road-safety policymaking and the broader integration of multimodal artificial
intelligence in transportation analytics. The code for this paper is
open-source and available at: https://github.com/Thamed-Chowdhury/ALIGN
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06316v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06316v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Precision-Scalable Microscaling Datapaths with Optimized Reduction Tree for Efficient NPU Integration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Emerging continual learning applications necessitate next-generation neural
processing unit (NPU) platforms to support both training and inference
operations. The promising Microscaling (MX) standard enables narrow bit-widths
for inference and large dynamic ranges for training. However, existing MX
multiply-accumulate (MAC) designs face a critical trade-off: integer
accumulation requires expensive conversions from narrow floating-point
products, while FP32 accumulation suffers from quantization losses and costly
normalization. To address these limitations, we propose a hybrid
precision-scalable reduction tree for MX MACs that combines the benefits of
both approaches, enabling efficient mixed-precision accumulation with
controlled accuracy relaxation. Moreover, we integrate an 8x8 array of these
MACs into the state-of-the-art (SotA) NPU integration platform, SNAX, to
provide efficient control and data transfer to our optimized precision-scalable
MX datapath. We evaluate our design both on MAC and system level and compare it
to the SotA. Our integrated system achieves an energy efficiency of 657,
1438-1675, and 4065 GOPS/W, respectively, for MXINT8, MXFP8/6, and MXFP4, with
a throughput of 64, 256, and 512 GOPS.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06313v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06313v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Station: An Open-World Environment for AI-Driven Discovery
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce the STATION, an open-world multi-agent environment that models a
miniature scientific ecosystem. Leveraging their extended context windows,
agents in the Station can engage in long scientific journeys that include
reading papers from peers, formulating hypotheses, submitting code, performing
analyses, and publishing results. Importantly, there is no centralized system
coordinating their activities - agents are free to choose their own actions and
develop their own narratives within the Station. Experiments demonstrate that
AI agents in the Station achieve new state-of-the-art performance on a wide
range of benchmarks, spanning from mathematics to computational biology to
machine learning, notably surpassing AlphaEvolve in circle packing. A rich
tapestry of narratives emerges as agents pursue independent research, interact
with peers, and build upon a cumulative history. From these emergent
narratives, novel methods arise organically, such as a new density-adaptive
algorithm for scRNA-seq batch integration. The Station marks a first step
towards autonomous scientific discovery driven by emergent behavior in an
open-world environment, representing a new paradigm that moves beyond rigid
optimization.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06309v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06309v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Kaggle Chronicles: 15 Years of Competitions, Community and Data Science Innovation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Since 2010, Kaggle has been a platform where data scientists from around the
world come together to compete, collaborate, and push the boundaries of Data
Science. Over these 15 years, it has grown from a purely competition-focused
site into a broader ecosystem with forums, notebooks, models, datasets, and
more. With the release of the Kaggle Meta Code and Kaggle Meta Datasets, we now
have a unique opportunity to explore these competitions, technologies, and
real-world applications of Machine Learning and AI. And so in this study, we
take a closer look at 15 years of data science on Kaggle - through metadata,
shared code, community discussions, and the competitions themselves. We explore
Kaggle's growth, its impact on the data science community, uncover hidden
technological trends, analyze competition winners, how Kagglers approach
problems in general, and more. We do this by analyzing millions of kernels and
discussion threads to perform both longitudinal trend analysis and standard
exploratory data analysis. Our findings show that Kaggle is a steadily growing
platform with increasingly diverse use cases, and that Kagglers are quick to
adapt to new trends and apply them to real-world challenges, while producing -
on average - models with solid generalization capabilities. We also offer a
snapshot of the platform as a whole, highlighting its history and technological
evolution. Finally, this study is accompanied by a video
(https://www.youtube.com/watch?v=YVOV9bIUNrM) and a Kaggle write-up
(https://kaggle.com/competitions/meta-kaggle-hackathon/writeups/kaggle-chronicles-15-years-of-competitions-communi)
for your convenience.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06304v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06304v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Secu-Table: a Comprehensive security table dataset for evaluating semantic table interpretation systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Evaluating semantic tables interpretation (STI) systems, (particularly, those
based on Large Language Models- LLMs) especially in domain-specific contexts
such as the security domain, depends heavily on the dataset. However, in the
security domain, tabular datasets for state-of-the-art are not publicly
available. In this paper, we introduce Secu-Table dataset, composed of more
than 1500 tables with more than 15k entities constructed using security data
extracted from Common Vulnerabilities and Exposures (CVE) and Common Weakness
Enumeration (CWE) data sources and annotated using Wikidata and the SEmantic
Processing of Security Event Streams CyberSecurity Knowledge Graph (SEPSES
CSKG). Along with the dataset, all the code is publicly released. This dataset
is made available to the research community in the context of the SemTab
challenge on Tabular to Knowledge Graph Matching. This challenge aims to
evaluate the performance of several STI based on open source LLMs. Preliminary
evaluation, serving as baseline, was conducted using Falcon3-7b-instruct and
Mistral-7B-Instruct, two open source LLMs and GPT-4o mini one closed source
LLM.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06301v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06301v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Decomate: Leveraging Generative Models for Co-Creative SVG Animation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Designers often encounter friction when animating static SVG graphics,
especially when the visual structure does not match the desired level of motion
detail. Existing tools typically depend on predefined groupings or require
technical expertise, which limits designers' ability to experiment and iterate
independently. We present Decomate, a system that enables intuitive SVG
animation through natural language. Decomate leverages a multimodal large
language model to restructure raw SVGs into semantically meaningful,
animation-ready components. Designers can then specify motions for each
component via text prompts, after which the system generates corresponding
HTML/CSS/JS animations. By supporting iterative refinement through natural
language interaction, Decomate integrates generative AI into creative
workflows, allowing animation outcomes to be directly shaped by user intent.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06297v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06297v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Transolver is a Linear Transformer: Revisiting Physics-Attention through the Lens of Linear Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in Transformer-based Neural Operators have enabled
significant progress in data-driven solvers for Partial Differential Equations
(PDEs). Most current research has focused on reducing the quadratic complexity
of attention to address the resulting low training and inference efficiency.
Among these works, Transolver stands out as a representative method that
introduces Physics-Attention to reduce computational costs. Physics-Attention
projects grid points into slices for slice attention, then maps them back
through deslicing. However, we observe that Physics-Attention can be
reformulated as a special case of linear attention, and that the slice
attention may even hurt the model performance. Based on these observations, we
argue that its effectiveness primarily arises from the slice and deslice
operations rather than interactions between slices. Building on this insight,
we propose a two-step transformation to redesign Physics-Attention into a
canonical linear attention, which we call Linear Attention Neural Operator
(LinearNO). Our method achieves state-of-the-art performance on six standard
PDE benchmarks, while reducing the number of parameters by an average of 40.0%
and computational cost by 36.2%. Additionally, it delivers superior performance
on two challenging, industrial-level datasets: AirfRANS and Shape-Net Car.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06294v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06294v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Synthetic Data-Driven Prompt Tuning for Financial QA over Tables and Documents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Financial documents like earning reports or balance sheets often involve long
tables and multi-page reports. Large language models have become a new tool to
help numerical reasoning and understanding these documents. However, prompt
quality can have a major effect on how well LLMs perform these financial
reasoning tasks. Most current methods tune prompts on fixed datasets of
financial text or tabular data, which limits their ability to adapt to new
question types or document structures, or they involve costly and manually
labeled/curated dataset to help build the prompts. We introduce a
self-improving prompt framework driven by data-augmented optimization. In this
closed-loop process, we generate synthetic financial tables and document
excerpts, verify their correctness and robustness, and then update the prompt
based on the results. Specifically, our framework combines a synthetic data
generator with verifiers and a prompt optimizer, where the generator produces
new examples that exposes weaknesses in the current prompt, the verifiers check
the validity and robustness of the produced examples, and the optimizer
incrementally refines the prompt in response. By iterating these steps in a
feedback cycle, our method steadily improves prompt accuracy on financial
reasoning tasks without needing external labels. Evaluation on DocMath-Eval
benchmark demonstrates that our system achieves higher performance in both
accuracy and robustness than standard prompt methods, underscoring the value of
incorporating synthetic data generation into prompt learning for financial
applications.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06292v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06292v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Exploiting Inter-Session Information with Frequency-enhanced Dual-Path Networks for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sequential recommendation (SR) aims to predict a user's next item preference
by modeling historical interaction sequences. Recent advances often integrate
frequency-domain modules to compensate for self-attention's low-pass nature by
restoring the high-frequency signals critical for personalized recommendations.
Nevertheless, existing frequency-aware solutions process each session in
isolation and optimize exclusively with time-domain objectives. Consequently,
they overlook cross-session spectral dependencies and fail to enforce alignment
between predicted and actual spectral signatures, leaving valuable frequency
information under-exploited. To this end, we propose FreqRec, a
Frequency-Enhanced Dual-Path Network for sequential Recommendation that jointly
captures inter-session and intra-session behaviors via a learnable
Frequency-domain Multi-layer Perceptrons. Moreover, FreqRec is optimized under
a composite objective that combines cross entropy with a frequency-domain
consistency loss, explicitly aligning predicted and true spectral signatures.
Extensive experiments on three benchmarks show that FreqRec surpasses strong
baselines and remains robust under data sparsity and noisy-log conditions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06285v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06285v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        COTN: A Chaotic Oscillatory Transformer Network for Complex Volatile Systems under Extreme Conditions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate prediction of financial and electricity markets, especially under
extreme conditions, remains a significant challenge due to their intrinsic
nonlinearity, rapid fluctuations, and chaotic patterns. To address these
limitations, we propose the Chaotic Oscillatory Transformer Network (COTN).
COTN innovatively combines a Transformer architecture with a novel Lee
Oscillator activation function, processed through Max-over-Time pooling and a
lambda-gating mechanism. This design is specifically tailored to effectively
capture chaotic dynamics and improve responsiveness during periods of
heightened volatility, where conventional activation functions (e.g., ReLU,
GELU) tend to saturate. Furthermore, COTN incorporates an Autoencoder
Self-Regressive (ASR) module to detect and isolate abnormal market patterns,
such as sudden price spikes or crashes, thereby preventing corruption of the
core prediction process and enhancing robustness. Extensive experiments across
electricity spot markets and financial markets demonstrate the practical
applicability and resilience of COTN. Our approach outperforms state-of-the-art
deep learning models like Informer by up to 17% and traditional statistical
methods like GARCH by as much as 40%. These results underscore COTN's
effectiveness in navigating real-world market uncertainty and complexity,
offering a powerful tool for forecasting highly volatile systems under duress.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06273v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06273v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06272v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06272v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GAIA: A General Agency Interaction Architecture for LLM-Human B2B Negotiation & Screening
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Organizations are increasingly exploring delegation of screening and
negotiation tasks to AI systems, yet deployment in high-stakes B2B settings is
constrained by governance: preventing unauthorized commitments, ensuring
sufficient information before bargaining, and maintaining effective human
oversight and auditability. Prior work on large language model negotiation
largely emphasizes autonomous bargaining between agents and omits practical
needs such as staged information gathering, explicit authorization boundaries,
and systematic feedback integration. We propose GAIA, a governance-first
framework for LLM-human agency in B2B negotiation and screening. GAIA defines
three essential roles - Principal (human), Delegate (LLM agent), and
Counterparty - with an optional Critic to enhance performance, and organizes
interactions through three mechanisms: information-gated progression that
separates screening from negotiation; dual feedback integration that combines
AI critique with lightweight human corrections; and authorization boundaries
with explicit escalation paths. Our contributions are fourfold: (1) a formal
governance framework with three coordinated mechanisms and four safety
invariants for delegation with bounded authorization; (2) information-gated
progression via task-completeness tracking (TCI) and explicit state transitions
that separate screening from commitment; (3) dual feedback integration that
blends Critic suggestions with human oversight through parallel learning
channels; and (4) a hybrid validation blueprint that combines automated
protocol metrics with human judgment of outcomes and safety. By bridging theory
and practice, GAIA offers a reproducible specification for safe, efficient, and
accountable AI delegation that can be instantiated across procurement, real
estate, and staffing workflows.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06262v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06262v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM-Guided Reinforcement Learning with Representative Agents for Traffic Modeling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) are increasingly used as behavioral proxies for
self-interested travelers in agent-based traffic models. Although more flexible
and generalizable than conventional models, the practical use of these
approaches remains limited by scalability due to the cost of calling one LLM
for every traveler. Moreover, it has been found that LLM agents often make
opaque choices and produce unstable day-to-day dynamics. To address these
challenges, we propose to model each homogeneous traveler group facing the same
decision context with a single representative LLM agent who behaves like the
population's average, maintaining and updating a mixed strategy over routes
that coincides with the group's aggregate flow proportions. Each day, the LLM
reviews the travel experience and flags routes with positive reinforcement that
they hope to use more often, and an interpretable update rule then converts
this judgment into strategy adjustments using a tunable (progressively
decaying) step size. The representative-agent design improves scalability,
while the separation of reasoning from updating clarifies the decision logic
while stabilizing learning. In classic traffic assignment settings, we find
that the proposed approach converges rapidly to the user equilibrium. In richer
settings with income heterogeneity, multi-criteria costs, and multi-modal
choices, the generated dynamics remain stable and interpretable, reproducing
plausible behavioral patterns well-documented in psychology and economics, for
example, the decoy effect in toll versus non-toll road selection, and higher
willingness-to-pay for convenience among higher-income travelers when choosing
between driving, transit, and park-and-ride options.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06260v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06260v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Breaking the Modality Barrier: Generative Modeling for Accurate Molecule Retrieval from Mass Spectra
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieving molecular structures from tandem mass spectra is a crucial step in
rapid compound identification. Existing retrieval methods, such as traditional
mass spectral library matching, suffer from limited spectral library coverage,
while recent cross-modal representation learning frameworks often encounter
modality misalignment, resulting in suboptimal retrieval accuracy and
generalization. To address these limitations, we propose GLMR, a Generative
Language Model-based Retrieval framework that mitigates the cross-modal
misalignment through a two-stage process. In the pre-retrieval stage, a
contrastive learning-based model identifies top candidate molecules as
contextual priors for the input mass spectrum. In the generative retrieval
stage, these candidate molecules are integrated with the input mass spectrum to
guide a generative model in producing refined molecular structures, which are
then used to re-rank the candidates based on molecular similarity. Experiments
on both MassSpecGym and the proposed MassRET-20k dataset demonstrate that GLMR
significantly outperforms existing methods, achieving over 40% improvement in
top-1 accuracy and exhibiting strong generalizability.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06259v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06259v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MrCoM: A Meta-Regularized World-Model Generalizing Across Multi-Scenarios
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Model-based reinforcement learning (MBRL) is a crucial approach to enhance
the generalization capabilities and improve the sample efficiency of RL
algorithms. However, current MBRL methods focus primarily on building world
models for single tasks and rarely address generalization across different
scenarios. Building on the insight that dynamics within the same simulation
engine share inherent properties, we attempt to construct a unified world model
capable of generalizing across different scenarios, named Meta-Regularized
Contextual World-Model (MrCoM). This method first decomposes the latent state
space into various components based on the dynamic characteristics, thereby
enhancing the accuracy of world-model prediction. Further, MrCoM adopts
meta-state regularization to extract unified representation of
scenario-relevant information, and meta-value regularization to align
world-model optimization with policy learning across diverse scenario
objectives. We theoretically analyze the generalization error upper bound of
MrCoM in multi-scenario settings. We systematically evaluate our algorithm's
generalization ability across diverse scenarios, demonstrating significantly
better performance than previous state-of-the-art methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06252v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06252v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WebVIA: A Web-based Vision-Language Agentic Framework for Interactive and Verifiable UI-to-Code Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    User interface (UI) development requires translating design mockups into
functional code, a process that remains repetitive and labor-intensive. While
recent Vision-Language Models (VLMs) automate UI-to-Code generation, they
generate only static HTML/CSS/JavaScript layouts lacking interactivity. To
address this, we propose WebVIA, the first agentic framework for interactive
UI-to-Code generation and validation. The framework comprises three components:
1) an exploration agent to capture multi-state UI screenshots; 2) a UI2Code
model that generates executable interactive code; 3) a validation module that
verifies the interactivity. Experiments demonstrate that WebVIA-Agent achieves
more stable and accurate UI exploration than general-purpose agents (e.g.,
Gemini-2.5-Pro). In addition, our fine-tuned WebVIA-UI2Code models exhibit
substantial improvements in generating executable and interactive
HTML/CSS/JavaScript code, outperforming their base counterparts across both
interactive and static UI2Code benchmarks. Our code and models are available at
\href{https://zheny2751-dotcom.github.io/webvia.github.io/}{\texttt{https://webvia.github.io}}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06251v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06251v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Constraint-Informed Active Learning for End-to-End ACOPF Optimization Proxies
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper studies optimization proxies, machine learning (ML) models trained
to efficiently predict optimal solutions for AC Optimal Power Flow (ACOPF)
problems. While promising, optimization proxy performance heavily depends on
training data quality. To address this limitation, this paper introduces a
novel active sampling framework for ACOPF optimization proxies designed to
generate realistic and diverse training data. The framework actively explores
varied, flexible problem specifications reflecting plausible operational
realities. More importantly, the approach uses optimization-specific quantities
(active constraint sets) that better capture the salient features of an ACOPF
that lead to the optimal solution. Numerical results show superior
generalization over existing sampling methods with an equivalent training
budget, significantly advancing the state-of-practice for trustworthy ACOPF
optimization proxies.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06248v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06248v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Affordance-Guided Coarse-to-Fine Exploration for Base Placement in Open-Vocabulary Mobile Manipulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In open-vocabulary mobile manipulation (OVMM), task success often hinges on
the selection of an appropriate base placement for the robot. Existing
approaches typically navigate to proximity-based regions without considering
affordances, resulting in frequent manipulation failures. We propose
Affordance-Guided Coarse-to-Fine Exploration, a zero-shot framework for base
placement that integrates semantic understanding from vision-language models
(VLMs) with geometric feasibility through an iterative optimization process.
Our method constructs cross-modal representations, namely Affordance RGB and
Obstacle Map+, to align semantics with spatial context. This enables reasoning
that extends beyond the egocentric limitations of RGB perception. To ensure
interaction is guided by task-relevant affordances, we leverage coarse semantic
priors from VLMs to guide the search toward task-relevant regions and refine
placements with geometric constraints, thereby reducing the risk of convergence
to local optima. Evaluated on five diverse open-vocabulary mobile manipulation
tasks, our system achieves an 85% success rate, significantly outperforming
classical geometric planners and VLM-based methods. This demonstrates the
promise of affordance-aware and multimodal reasoning for generalizable,
instruction-conditioned planning in OVMM.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06240v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06240v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Mixtures of SubExperts for Large Language Continual Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Adapting Large Language Models (LLMs) to a continuous stream of tasks is a
critical yet challenging endeavor. While Parameter-Efficient Fine-Tuning (PEFT)
methods have become a standard for this, they face a fundamental dilemma in
continual learning. Reusing a single set of PEFT parameters for new tasks often
leads to catastrophic forgetting of prior knowledge. Conversely, allocating
distinct parameters for each task prevents forgetting but results in a linear
growth of the model's size and fails to facilitate knowledge transfer between
related tasks. To overcome these limitations, we propose a novel adaptive PEFT
method referred to as \textit{Mixtures of SubExperts (MoSEs)}, a novel
continual learning framework designed for minimal forgetting and efficient
scalability. MoSEs integrate a sparse Mixture of SubExperts into the
transformer layers, governed by a task-specific routing mechanism. This
architecture allows the model to isolate and protect knowledge within dedicated
SubExperts, thereby minimizing parameter interference and catastrophic
forgetting. Crucially, the router can adaptively select and combine previously
learned sparse parameters for new tasks, enabling effective knowledge transfer
while ensuring that the model's capacity grows sublinearly. We evaluate MoSEs
on the comprehensive TRACE benchmark datasets. Our experiments demonstrate that
MoSEs significantly outperform conventional continual learning approaches in
both knowledge retention and scalability to new tasks, achieving
state-of-the-art performance with substantial memory and computational savings.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06237v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06237v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Analyzing and Mitigating Negation Artifacts using Data Augmentation for Improving ELECTRA-Small Model Accuracy
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Pre-trained models for natural language inference (NLI) often achieve high
performance on benchmark datasets by using spurious correlations, or dataset
artifacts, rather than understanding language touches such as negation. In this
project, we investigate the performance of an ELECTRA-small model fine-tuned on
the Stanford Natural Language Inference (SNLI) dataset, focusing on its
handling of negation. Through analysis, we identify that the model struggles
with correctly classifying examples containing negation. To address this, we
augment the training data with contrast sets and adversarial examples
emphasizing negation. Our results demonstrate that this targeted data
augmentation improves the model's accuracy on negation-containing examples
without adversely affecting overall performance, therefore mitigating the
identified dataset artifact.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06234v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06234v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scaling Laws and In-Context Learning: A Unified Theoretical Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In-context learning (ICL) enables large language models to adapt to new tasks
from demonstrations without parameter updates. Despite extensive empirical
studies, a principled understanding of ICL emergence at scale remains more
elusive. We present a unified theoretical framework connecting scaling laws to
ICL emergence in transformers. Our analysis establishes that ICL performance
follows power-law relationships with model depth $L$, width $d$, context length
$k$, and training data $D$, with exponents determined by task structure. We
show that under specific conditions, transformers implement gradient-based
metalearning in their forward pass, with an effective learning rate
$\eta_{\text{eff}} = \Theta(1/\sqrt{Ld})$. We demonstrate sharp phase
transitions at critical scales and derive optimal depth-width allocations
favoring $L^* \propto N^{2/3}$, $d^* \propto N^{1/3}$ for the fixed parameter
budget $N = Ld$. Systematic experiments on synthetic tasks validate our
predictions, with measured scaling exponents closely matching theory. This work
provides both necessary and sufficient conditions for the emergence of ICLs and
establishes fundamental computational limits on what transformers can learn
in-context.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06232v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06232v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Overview of CHIP 2025 Shared Task 2: Discharge Medication Recommendation for Metabolic Diseases Based on Chinese Electronic Health Records
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Discharge medication recommendation plays a critical role in ensuring
treatment continuity, preventing readmission, and improving long-term
management for patients with chronic metabolic diseases. This paper present an
overview of the CHIP 2025 Shared Task 2 competition, which aimed to develop
state-of-the-art approaches for automatically recommending appro-priate
discharge medications using real-world Chinese EHR data. For this task, we
constructed CDrugRed, a high-quality dataset consisting of 5,894 de-identified
hospitalization records from 3,190 patients in China. This task is challenging
due to multi-label nature of medication recommendation, het-erogeneous clinical
text, and patient-specific variability in treatment plans. A total of 526 teams
registered, with 167 and 95 teams submitting valid results to the Phase A and
Phase B leaderboards, respectively. The top-performing team achieved the
highest overall performance on the final test set, with a Jaccard score of
0.5102, F1 score of 0.6267, demonstrating the potential of advanced large
language model (LLM)-based ensemble systems. These re-sults highlight both the
promise and remaining challenges of applying LLMs to medication recommendation
in Chinese EHRs. The post-evaluation phase remains open at
https://tianchi.aliyun.com/competition/entrance/532411/.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06230v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06230v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Assertion-Aware Test Code Summarization with Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unit tests often lack concise summaries that convey test intent, especially
in auto-generated or poorly documented codebases. Large Language Models (LLMs)
offer a promising solution, but their effectiveness depends heavily on how they
are prompted. Unlike generic code summarization, test-code summarization poses
distinct challenges because test methods validate expected behavior through
assertions rather than im- plementing functionality. This paper presents a new
benchmark of 91 real-world Java test cases paired with developer-written
summaries and conducts a controlled ablation study to investigate how test
code-related components-such as the method under test (MUT), assertion
messages, and assertion semantics-affect the performance of LLM-generated test
summaries. We evaluate four code LLMs (Codex, Codestral, DeepSeek, and
Qwen-Coder) across seven prompt configurations using n-gram metrics (BLEU,
ROUGE-L, METEOR), semantic similarity (BERTScore), and LLM-based evaluation.
Results show that prompting with as- sertion semantics improves summary quality
by an average of 0.10 points (2.3%) over full MUT context (4.45 vs. 4.35) while
requiring fewer input tokens. Codex and Qwen-Coder achieve the highest
alignment with human-written summaries, while DeepSeek underperforms despite
high lexical overlap. The replication package is publicly available at
https://doi.org/10. 5281/zenodo.17067550
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06227v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06227v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ROAR: Robust Accident Recognition and Anticipation for Autonomous Driving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate accident anticipation is essential for enhancing the safety of
autonomous vehicles (AVs). However, existing methods often assume ideal
conditions, overlooking challenges such as sensor failures, environmental
disturbances, and data imperfections, which can significantly degrade
prediction accuracy. Additionally, previous models have not adequately
addressed the considerable variability in driver behavior and accident rates
across different vehicle types. To overcome these limitations, this study
introduces ROAR, a novel approach for accident detection and prediction. ROAR
combines Discrete Wavelet Transform (DWT), a self adaptive object aware module,
and dynamic focal loss to tackle these challenges. The DWT effectively extracts
features from noisy and incomplete data, while the object aware module improves
accident prediction by focusing on high-risk vehicles and modeling the spatial
temporal relationships among traffic agents. Moreover, dynamic focal loss
mitigates the impact of class imbalance between positive and negative samples.
Evaluated on three widely used datasets, Dashcam Accident Dataset (DAD), Car
Crash Dataset (CCD), and AnAn Accident Detection (A3D), our model consistently
outperforms existing baselines in key metrics such as Average Precision (AP)
and mean Time to Accident (mTTA). These results demonstrate the model's
robustness in real-world conditions, particularly in handling sensor
degradation, environmental noise, and imbalanced data distributions. This work
offers a promising solution for reliable and accurate accident anticipation in
complex traffic environments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06226v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06226v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Tiny Model, Big Logic: Diversity-Driven Optimization Elicits Large-Model Reasoning Ability in VibeThinker-1.5B
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Challenging the prevailing consensus that small models inherently lack robust
reasoning, this report introduces VibeThinker-1.5B, a 1.5B-parameter dense
model developed via our Spectrum-to-Signal Principle (SSP). This challenges the
prevailing approach of scaling model parameters to enhance capabilities, as
seen in models like DeepSeek R1 (671B) and Kimi k2 (>1T). The SSP framework
first employs a Two-Stage Diversity-Exploring Distillation (SFT) to generate a
broad spectrum of solutions, followed by MaxEnt-Guided Policy Optimization (RL)
to amplify the correct signal. With a total training cost of only $7,800,
VibeThinker-1.5B demonstrates superior reasoning capabilities compared to
closed-source models like Magistral Medium and Claude Opus 4, and performs on
par with open-source models like GPT OSS-20B Medium. Remarkably, it surpasses
the 400x larger DeepSeek R1 on three math benchmarks: AIME24 (80.3 vs. 79.8),
AIME25 (74.4 vs. 70.0), and HMMT25 (50.4 vs. 41.7). This is a substantial
improvement over its base model (6.7, 4.3, and 0.6, respectively). On
LiveCodeBench V6, it scores 51.1, outperforming Magistral Medium's 50.3 and its
base model's 0.0. These findings demonstrate that small models can achieve
reasoning capabilities comparable to large models, drastically reducing
training and inference costs and thereby democratizing advanced AI research.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06221v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06221v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Explicit Knowledge-Guided In-Context Learning for Early Detection of Alzheimer's Disease
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Detecting Alzheimer's Disease (AD) from narrative transcripts remains a
challenging task for large language models (LLMs), particularly under
out-of-distribution (OOD) and data-scarce conditions. While in-context learning
(ICL) provides a parameter-efficient alternative to fine-tuning, existing ICL
approaches often suffer from task recognition failure, suboptimal demonstration
selection, and misalignment between label words and task objectives, issues
that are amplified in clinical domains like AD detection. We propose Explicit
Knowledge In-Context Learners (EK-ICL), a novel framework that integrates
structured explicit knowledge to enhance reasoning stability and task alignment
in ICL. EK-ICL incorporates three knowledge components: confidence scores
derived from small language models (SLMs) to ground predictions in
task-relevant patterns, parsing feature scores to capture structural
differences and improve demo selection, and label word replacement to resolve
semantic misalignment with LLM priors. In addition, EK-ICL employs a
parsing-based retrieval strategy and ensemble prediction to mitigate the
effects of semantic homogeneity in AD transcripts. Extensive experiments across
three AD datasets demonstrate that EK-ICL significantly outperforms
state-of-the-art fine-tuning and ICL baselines. Further analysis reveals that
ICL performance in AD detection is highly sensitive to the alignment of label
semantics and task-specific context, underscoring the importance of explicit
knowledge in clinical reasoning under low-resource conditions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06215v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06215v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RAG-targeted Adversarial Attack on LLM-based Threat Detection and Mitigation Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid expansion of the Internet of Things (IoT) is reshaping
communication and operational practices across industries, but it also broadens
the attack surface and increases susceptibility to security breaches.
Artificial Intelligence has become a valuable solution in securing IoT
networks, with Large Language Models (LLMs) enabling automated attack behavior
analysis and mitigation suggestion in Network Intrusion Detection Systems
(NIDS). Despite advancements, the use of LLMs in such systems further expands
the attack surface, putting entire networks at risk by introducing
vulnerabilities such as prompt injection and data poisoning. In this work, we
attack an LLM-based IoT attack analysis and mitigation framework to test its
adversarial robustness. We construct an attack description dataset and use it
in a targeted data poisoning attack that applies word-level, meaning-preserving
perturbations to corrupt the Retrieval-Augmented Generation (RAG) knowledge
base of the framework. We then compare pre-attack and post-attack mitigation
responses from the target model, ChatGPT-5 Thinking, to measure the impact of
the attack on model performance, using an established evaluation rubric
designed for human experts and judge LLMs. Our results show that small
perturbations degrade LLM performance by weakening the linkage between observed
network traffic features and attack behavior, and by reducing the specificity
and practicality of recommended mitigations for resource-constrained devices.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06212v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06212v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reasoning with Confidence: Efficient Verification of LLM Reasoning Steps via Uncertainty Heads
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Solving complex tasks usually requires LLMs to generate long multi-step
reasoning chains. Previous work has shown that verifying the correctness of
individual reasoning steps can further improve the performance and efficiency
of LLMs on such tasks and enhance solution interpretability. However, existing
verification approaches, such as Process Reward Models (PRMs), are either
computationally expensive, limited to specific domains, or require large-scale
human or model-generated annotations. Thus, we propose a lightweight
alternative for step-level reasoning verification based on data-driven
uncertainty scores. We train transformer-based uncertainty quantification heads
(UHeads) that use the internal states of a frozen LLM to estimate the
uncertainty of its reasoning steps during generation. The approach is fully
automatic: target labels are generated either by another larger LLM (e.g.,
DeepSeek R1) or in a self-supervised manner by the original model itself.
UHeads are both effective and lightweight, containing less than 10M parameters.
Across multiple domains, including mathematics, planning, and general knowledge
question answering, they match or even surpass the performance of PRMs that are
up to 810x larger. Our findings suggest that the internal states of LLMs encode
their uncertainty and can serve as reliable signals for reasoning verification,
offering a promising direction toward scalable and generalizable introspective
LLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06209v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06209v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Resilience Inference for Supply Chains with Hypergraph Neural Network
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Supply chains are integral to global economic stability, yet disruptions can
swiftly propagate through interconnected networks, resulting in substantial
economic impacts. Accurate and timely inference of supply chain resilience the
capability to maintain core functions during disruptions is crucial for
proactive risk mitigation and robust network design. However, existing
approaches lack effective mechanisms to infer supply chain resilience without
explicit system dynamics and struggle to represent the higher-order,
multi-entity dependencies inherent in supply chain networks. These limitations
motivate the definition of a novel problem and the development of targeted
modeling solutions. To address these challenges, we formalize a novel problem:
Supply Chain Resilience Inference (SCRI), defined as predicting supply chain
resilience using hypergraph topology and observed inventory trajectories
without explicit dynamic equations. To solve this problem, we propose the
Supply Chain Resilience Inference Hypergraph Network (SC-RIHN), a novel
hypergraph-based model leveraging set-based encoding and hypergraph message
passing to capture multi-party firm-product interactions. Comprehensive
experiments demonstrate that SC-RIHN significantly outperforms traditional MLP,
representative graph neural network variants, and ResInf baselines across
synthetic benchmarks, underscoring its potential for practical, early-warning
risk assessment in complex supply chain systems.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06208v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06208v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Adversarial Robustness of IoT Intrusion Detection via SHAP-Based Attribution Fingerprinting
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid proliferation of Internet of Things (IoT) devices has transformed
numerous industries by enabling seamless connectivity and data-driven
automation. However, this expansion has also exposed IoT networks to
increasingly sophisticated security threats, including adversarial attacks
targeting artificial intelligence (AI) and machine learning (ML)-based
intrusion detection systems (IDS) to deliberately evade detection, induce
misclassification, and systematically undermine the reliability and integrity
of security defenses. To address these challenges, we propose a novel
adversarial detection model that enhances the robustness of IoT IDS against
adversarial attacks through SHapley Additive exPlanations (SHAP)-based
fingerprinting. Using SHAP's DeepExplainer, we extract attribution fingerprints
from network traffic features, enabling the IDS to reliably distinguish between
clean and adversarially perturbed inputs. By capturing subtle attribution
patterns, the model becomes more resilient to evasion attempts and adversarial
manipulations. We evaluated the model on a standard IoT benchmark dataset,
where it significantly outperformed a state-of-the-art method in detecting
adversarial attacks. In addition to enhanced robustness, this approach improves
model transparency and interpretability, thereby increasing trust in the IDS
through explainable AI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06197v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06197v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AI as intermediary in modern-day ritual: An immersive, interactive production of the roller disco musical Xanadu at UCLA
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Interfaces for contemporary large language, generative media, and perception
AI models are often engineered for single user interaction. We investigate
ritual as a design scaffold for developing collaborative, multi-user human-AI
engagement. We consider the specific case of an immersive staging of the
musical Xanadu performed at UCLA in Spring 2025. During a two-week run, over
five hundred audience members contributed sketches and jazzercise moves that
vision language models translated to virtual scenery elements and from
choreographic prompts. This paper discusses four facets of
interaction-as-ritual within the show: audience input as offerings that AI
transforms into components of the ritual; performers as ritual guides,
demonstrating how to interact with technology and sorting audience members into
cohorts; AI systems as instruments "played" by the humans, in which sensing,
generative components, and stagecraft create systems that can be mastered over
time; and reciprocity of interaction, in which the show's AI machinery guides
human behavior as well as being guided by humans, completing a human-AI
feedback loop that visibly reshapes the virtual world. Ritual served as a frame
for integrating linear narrative, character identity, music and interaction.
The production explored how AI systems can support group creativity and play,
addressing a critical gap in prevailing single user AI design paradigms.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06195v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06195v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Dataforge: A Data Agent Platform for Autonomous Data Engineering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The growing demand for AI applications in fields such as materials discovery,
molecular modeling, and climate science has made data preparation an important
but labor-intensive step. Raw data from diverse sources must be cleaned,
normalized, and transformed to become AI-ready, while effective feature
transformation and selection are essential for efficient training and
inference. To address the challenges of scalability and expertise dependence,
we present Data Agent, a fully autonomous system specialized for tabular data.
Leveraging large language model (LLM) reasoning and grounded validation, Data
Agent automatically performs data cleaning, hierarchical routing, and
feature-level optimization through dual feedback loops. It embodies three core
principles: automatic, safe, and non-expert friendly, which ensure end-to-end
reliability without human supervision. This demo showcases the first practical
realization of an autonomous Data Agent, illustrating how raw data can be
transformed "From Data to Better Data."
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06185v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06185v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MemoriesDB: A Temporal-Semantic-Relational Database for Long-Term Agent Memory / Modeling Experience as a Graph of Temporal-Semantic Surfaces
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce MemoriesDB, a unified data architecture designed to avoid
decoherence across time, meaning, and relation in long-term computational
memory. Each memory is a time-semantic-relational entity-a structure that
simultaneously encodes when an event occurred, what it means, and how it
connects to other events. Built initially atop PostgreSQL with pgvector
extensions, MemoriesDB combines the properties of a time-series datastore, a
vector database, and a graph system within a single append-only schema. Each
memory is represented as a vertex uniquely labeled by its microsecond timestamp
and accompanied by low- and high-dimensional normalized embeddings that capture
semantic context. Directed edges between memories form labeled relations with
per-edge metadata, enabling multiple contextual links between the same
vertices. Together these constructs form a time-indexed stack of
temporal-semantic surfaces, where edges project as directional arrows in a
1+1-dimensional similarity field, tracing the evolution of meaning through time
while maintaining cross-temporal coherence. This formulation supports efficient
time-bounded retrieval, hybrid semantic search, and lightweight structural
reasoning in a single query path. A working prototype demonstrates scalable
recall and contextual reinforcement using standard relational infrastructure,
and we discuss extensions toward a columnar backend, distributed clustering,
and emergent topic modeling.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06179v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06179v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CSP4SDG: Constraint and Information-Theory Based Role Identification in Social Deduction Games with LLM-Enhanced Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In Social Deduction Games (SDGs) such as Avalon, Mafia, and Werewolf, players
conceal their identities and deliberately mislead others, making hidden-role
inference a central and demanding task. Accurate role identification, which
forms the basis of an agent's belief state, is therefore the keystone for both
human and AI performance. We introduce CSP4SDG, a probabilistic,
constraint-satisfaction framework that analyses gameplay objectively. Game
events and dialogue are mapped to four linguistically-agnostic constraint
classes-evidence, phenomena, assertions, and hypotheses. Hard constraints prune
impossible role assignments, while weighted soft constraints score the
remainder; information-gain weighting links each hypothesis to its expected
value under entropy reduction, and a simple closed-form scoring rule guarantees
that truthful assertions converge to classical hard logic with minimum error.
The resulting posterior over roles is fully interpretable and updates in real
time. Experiments on three public datasets show that CSP4SDG (i) outperforms
LLM-based baselines in every inference scenario, and (ii) boosts LLMs when
supplied as an auxiliary "reasoning tool." Our study validates that principled
probabilistic reasoning with information theory is a scalable alternative-or
complement-to heavy-weight neural models for SDGs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06175v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06175v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LUT-LLM: Efficient Large Language Model Inference with Memory-based Computations on FPGAs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid progress of large language models (LLMs) has advanced numerous
applications, yet efficient single-batch inference remains vital for on-device
intelligence. While FPGAs offer fine-grained data control and high energy
efficiency, recent GPU optimizations have narrowed their advantage, especially
under arithmetic-based computation. To overcome this, we leverage FPGAs'
abundant on-chip memory to shift LLM inference from arithmetic- to memory-based
computation through table lookups. We present LUT-LLM, the first FPGA
accelerator enabling 1B+ LLM inference via vector-quantized memory operations.
Our analysis identifies activation-weight co-quantization as the most effective
scheme, supported by (1) bandwidth-aware parallel centroid search, (2)
efficient 2D table lookups, and (3) a spatial-temporal hybrid design minimizing
data caching. Implemented on an AMD V80 FPGA for a customized Qwen 3 1.7B
model, LUT-LLM achieves 1.66x lower latency than AMD MI210 and 1.72x higher
energy efficiency than NVIDIA A100, scaling to 32B models with 2.16x efficiency
gain over A100.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06174v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06174v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chinese opera is celebrated for preserving classical art. However, early
filming equipment limitations have degraded videos of last-century performances
by renowned artists (e.g., low frame rates and resolution), hindering archival
efforts. Although space-time video super-resolution (STVSR) has advanced
significantly, applying it directly to opera videos remains challenging. The
scarcity of datasets impedes the recovery of high frequency details, and
existing STVSR methods lack global modeling capabilities, compromising visual
quality when handling opera's characteristic large motions. To address these
challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset
and propose the Mamba-based multiscale fusion network for space-time Opera
Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three
novel components: the Global Fusion Module (GFM) for motion modeling through a
multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba
Module (MSMM) for alignment across different sequence lengths. Additionally,
our MambaVR block resolves feature artifacts and positional information loss
during alignment. Experimental results on the COVC dataset show that MambaOVSR
significantly outperforms the SOTA STVSR method by an average of 1.86 dB in
terms of PSNR. Dataset and Code will be publicly released.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06172v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06172v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Chasing Consistency: Quantifying and Optimizing Human-Model Alignment in Chain-of-Thought Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper presents a framework for evaluating and optimizing reasoning
consistency in Large Language Models (LLMs) via a new metric, the Alignment
Score, which quantifies the semantic alignment between model-generated
reasoning chains and human-written reference chains in Chain-of-Thought (CoT)
reasoning. Empirically, we find that 2-hop reasoning chains achieve the highest
Alignment Score. To explain this phenomenon, we define four key error types:
logical disconnection, thematic shift, redundant reasoning, and causal
reversal, and show how each contributes to the degradation of the Alignment
Score. Building on this analysis, we further propose Semantic Consistency
Optimization Sampling (SCOS), a method that samples and favors chains with
minimal alignment errors, significantly improving Alignment Scores by an
average of 29.84% with longer reasoning chains, such as in 3-hop tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06168v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06168v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Practical Policy Distillation for Reinforcement Learning in Radio Access Networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Adopting artificial intelligence (AI) in radio access networks (RANs)
presents several challenges, including limited availability of link-level
measurements (e.g., CQI reports), stringent real-time processing constraints
(e.g., sub-1 ms per TTI), and network heterogeneity (different spectrum bands,
cell types, and vendor equipment). A critical yet often overlooked barrier lies
in the computational and memory limitations of RAN baseband hardware,
particularly in legacy 4th Generation (4G) systems, which typically lack
on-chip neural accelerators. As a result, only lightweight AI models (under 1
Mb and sub-100~\mu s inference time) can be effectively deployed, limiting both
their performance and applicability. However, achieving strong generalization
across diverse network conditions often requires large-scale models with
substantial resource demands. To address this trade-off, this paper
investigates policy distillation in the context of a reinforcement
learning-based link adaptation task. We explore two strategies: single-policy
distillation, where a scenario-agnostic teacher model is compressed into one
generalized student model; and multi-policy distillation, where multiple
scenario-specific teachers are consolidated into a single generalist student.
Experimental evaluations in a high-fidelity, 5th Generation (5G)-compliant
simulator demonstrate that both strategies produce compact student models that
preserve the teachers' generalization capabilities while complying with the
computational and memory limitations of existing RAN hardware.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06563v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06563v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Bayesian Uncertainty Quantification with Anchored Ensembles for Robust EV Power Consumption Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate EV power estimation underpins range prediction and energy
management, yet practitioners need both point accuracy and trustworthy
uncertainty. We propose an anchored-ensemble Long Short-Term Memory (LSTM) with
a Student-t likelihood that jointly captures epistemic (model) and aleatoric
(data) uncertainty. Anchoring imposes a Gaussian weight prior (MAP training),
yielding posterior-like diversity without test-time sampling, while the t-head
provides heavy-tailed robustness and closed-form prediction intervals. Using
vehicle-kinematic time series (e.g., speed, motor RPM), our model attains
strong accuracy: RMSE 3.36 +/- 1.10, MAE 2.21 +/- 0.89, R-squared = 0.93 +/-
0.02, explained variance 0.93 +/- 0.02, and delivers well-calibrated
uncertainty bands with near-nominal coverage. Against competitive baselines
(Student-t MC dropout; quantile regression with/without anchoring), our method
matches or improves log-scores while producing sharper intervals at the same
coverage. Crucially for real-time deployment, inference is a single
deterministic pass per ensemble member (or a weight-averaged collapse),
eliminating Monte Carlo latency. The result is a compact, theoretically
grounded estimator that couples accuracy, calibration, and systems efficiency,
enabling reliable range estimation and decision-making for production EV energy
management.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06538v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06538v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient Approximation of Volterra Series for High-Dimensional Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The identification of high-dimensional nonlinear dynamical systems via the
Volterra series has significant potential, but has been severely hindered by
the curse of dimensionality. Tensor Network (TN) methods such as the Modified
Alternating Linear Scheme (MVMALS) have been a breakthrough for the field,
offering a tractable approach by exploiting the low-rank structure in Volterra
kernels. However, these techniques still encounter prohibitive computational
and memory bottlenecks due to high-order polynomial scaling with respect to
input dimension. To overcome this barrier, we introduce the Tensor Head
Averaging (THA) algorithm, which significantly reduces complexity by
constructing an ensemble of localized MVMALS models trained on small subsets of
the input space. In this paper, we present a theoretical foundation for the THA
algorithm. We establish observable, finite-sample bounds on the error between
the THA ensemble and a full MVMALS model, and we derive an exact decomposition
of the squared error. This decomposition is used to analyze the manner in which
subset models implicitly compensate for omitted dynamics. We quantify this
effect, and prove that correlation between the included and omitted dynamics
creates an optimization incentive which drives THA's performance toward
accuracy superior to a simple truncation of a full MVMALS model. THA thus
offers a scalable and theoretically grounded approach for identifying
previously intractable high-dimensional systems.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06527v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06527v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EASE: Practical and Efficient Safety Alignment for Small Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Small language models (SLMs) are increasingly deployed on edge devices,
making their safety alignment crucial yet challenging. Current shallow
alignment methods that rely on direct refusal of malicious queries fail to
provide robust protection, particularly against adversarial jailbreaks. While
deliberative safety reasoning alignment offers deeper alignment for defending
against sophisticated attacks, effectively implanting such reasoning capability
in SLMs with limited capabilities remains an open challenge. Moreover, safety
reasoning incurs significant computational overhead as models apply reasoning
to nearly all queries, making it impractical for resource-constrained edge
deployment scenarios that demand rapid responses. We propose EASE, a novel
framework that enables practical and Efficient safety Alignment for Small
languagE models. Our approach first identifies the optimal safety reasoning
teacher that can effectively distill safety reasoning capabilities to SLMs. We
then align models to selectively activate safety reasoning for dangerous
adversarial jailbreak queries while providing direct responses to
straightforward malicious queries and general helpful tasks. This selective
mechanism enables small models to maintain robust safety guarantees against
sophisticated attacks while preserving computational efficiency for benign
interactions. Experimental results demonstrate that EASE reduces jailbreak
attack success rates by up to 17% compared to shallow alignment methods while
reducing inference overhead by up to 90% compared to deliberative safety
reasoning alignment, making it practical for SLMs real-world edge deployments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06512v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06512v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Probably Approximately Global Robustness Certification
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose and investigate probabilistic guarantees for the adversarial
robustness of classification algorithms. While traditional formal verification
approaches for robustness are intractable and sampling-based approaches do not
provide formal guarantees, our approach is able to efficiently certify a
probabilistic relaxation of robustness. The key idea is to sample an
$\epsilon$-net and invoke a local robustness oracle on the sample. Remarkably,
the size of the sample needed to achieve probably approximately global
robustness guarantees is independent of the input dimensionality, the number of
classes, and the learning algorithm itself. Our approach can, therefore, be
applied even to large neural networks that are beyond the scope of traditional
formal verification. Experiments empirically confirm that it characterizes
robustness better than state-of-the-art sampling-based approaches and scales
better than formal methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06495v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06495v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning Time-Varying Graph Signals via Koopman
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A wide variety of real-world data, such as sea measurements, e.g.,
temperatures collected by distributed sensors and multiple unmanned aerial
vehicles (UAV) trajectories, can be naturally represented as graphs, often
exhibiting non-Euclidean structures. These graph representations may evolve
over time, forming time-varying graphs. Effectively modeling and analyzing such
dynamic graph data is critical for tasks like predicting graph evolution and
reconstructing missing graph data. In this paper, we propose a framework based
on the Koopman autoencoder (KAE) to handle time-varying graph data.
Specifically, we assume the existence of a hidden non-linear dynamical system,
where the state vector corresponds to the graph embedding of the time-varying
graph signals. To capture the evolving graph structures, the graph data is
first converted into a vector time series through graph embedding, representing
the structural information in a finite-dimensional latent space. In this latent
space, the KAE is applied to learn the underlying non-linear dynamics governing
the temporal evolution of graph features, enabling both prediction and
reconstruction tasks.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06493v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06493v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Bridging Theory and Practice: A Stochastic Learning-Optimization Model for Resilient Automotive Supply Chains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Supply chain disruptions and volatile demand pose significant challenges to
the UK automotive industry, which relies heavily on Just-In-Time (JIT)
manufacturing. While qualitative studies highlight the potential of integrating
Artificial Intelligence (AI) with traditional optimization, a formal,
quantitative demonstration of this synergy is lacking. This paper introduces a
novel stochastic learning-optimization framework that integrates Bayesian
inference with inventory optimization for supply chain management (SCM). We
model a two-echelon inventory system subject to stochastic demand and supply
disruptions, comparing a traditional static optimization policy against an
adaptive policy where Bayesian learning continuously updates parameter
estimates to inform stochastic optimization. Our simulations over 365 periods
across three operational scenarios demonstrate that the integrated approach
achieves 7.4\% cost reduction in stable environments and 5.7\% improvement
during supply disruptions, while revealing important limitations during sudden
demand shocks due to the inherent conservatism of Bayesian updating. This work
provides mathematical validation for practitioner observations and establishes
a formal framework for understanding AI-driven supply chain resilience, while
identifying critical boundary conditions for successful implementation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06479v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06479v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DyKAF: Dynamical Kronecker Approximation of the Fisher Information Matrix for Gradient Preconditioning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, optimizers that explicitly treat weights as matrices, rather than
flattened vectors, have demonstrated their effectiveness. This perspective
naturally leads to structured approximations of the Fisher matrix as
preconditioners, where the matrix view induces a Kronecker-factorized form that
enables memory-efficient representation. However, constructing such
approximations both efficiently and accurately remains an open challenge, since
obtaining the optimal factorization is resource-intensive and practical methods
therefore rely on heuristic design choices. In this work, we introduce a novel
approach that leverages projector-splitting integrators to construct effective
preconditioners. Our optimizer, DyKAF (Dynamical Kronecker Approximation of the
Fisher Matrix), consistently improves the Fisher matrix approximation quality.
Experiments on large language model pre-training and fine-tuning demonstrate
that DyKAF outperforms existing optimizers across a range of evaluation
metrics.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06477v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06477v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Error Estimate and Convergence Analysis for Data Valuation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Data valuation quantifies data importance, but existing methods cannot ensure
validity in a single training process. The neural dynamic data valuation (NDDV)
method [3] addresses this limitation. Based on NDDV, we are the first to
explore error estimation and convergence analysis in data valuation. Under
Lipschitz and smoothness assumptions, we derive quadratic error bounds for loss
differences that scale inversely with time steps and quadratically with control
variations, ensuring stability. We also prove that the expected squared
gradient norm for the training loss vanishes asymptotically, and that the meta
loss converges sublinearly over iterations. In particular, NDDV achieves
sublinear convergence.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06463v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06463v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reconstruction and Secrecy under Approximate Distance Queries
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Consider the task of locating an unknown target point using approximate
distance queries: in each round, a reconstructor selects a query point and
receives a noisy version of its distance to the target. This problem arises
naturally in various contexts ranging from localization in GPS and sensor
networks to privacy-aware data access, and spans a wide variety of metric
spaces. It is relevant from the perspective of both the reconstructor (seeking
accurate recovery) and the responder (aiming to limit information disclosure,
e.g., for privacy or security reasons). We study this reconstruction game
through a learning-theoretic lens, focusing on the rate and limits of the best
possible reconstruction error. Our first result provides a tight geometric
characterization of the optimal error in terms of the Chebyshev radius, a
classical concept from geometry. This characterization applies to all compact
metric spaces (in fact, even to all totally bounded spaces) and yields explicit
formulas for natural metric spaces. Our second result addresses the asymptotic
behavior of reconstruction, distinguishing between pseudo-finite spaces --
where the optimal error is attained after finitely many queries -- and spaces
where the approximation curve exhibits nontrivial decay. We characterize
pseudo-finiteness for convex Euclidean spaces.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06461v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06461v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MULTIBENCH++: A Unified and Comprehensive Multimodal Fusion Benchmarking Across Specialized Domains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Although multimodal fusion has made significant progress, its advancement is
severely hindered by the lack of adequate evaluation benchmarks. Current fusion
methods are typically evaluated on a small selection of public datasets, a
limited scope that inadequately represents the complexity and diversity of
real-world scenarios, potentially leading to biased evaluations. This issue
presents a twofold challenge. On one hand, models may overfit to the biases of
specific datasets, hindering their generalization to broader practical
applications. On the other hand, the absence of a unified evaluation standard
makes fair and objective comparisons between different fusion methods
difficult. Consequently, a truly universal and high-performance fusion model
has yet to emerge. To address these challenges, we have developed a
large-scale, domain-adaptive benchmark for multimodal evaluation. This
benchmark integrates over 30 datasets, encompassing 15 modalities and 20
predictive tasks across key application domains. To complement this, we have
also developed an open-source, unified, and automated evaluation pipeline that
includes standardized implementations of state-of-the-art models and diverse
fusion paradigms. Leveraging this platform, we have conducted large-scale
experiments, successfully establishing new performance baselines across
multiple tasks. This work provides the academic community with a crucial
platform for rigorous and reproducible assessment of multimodal models, aiming
to propel the field of multimodal artificial intelligence to new heights.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06452v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06452v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Risk-Neutral Neural Operator for Arbitrage-Free SPX-VIX Term Structures
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose ARBITER, a risk-neutral neural operator for learning joint SPX-VIX
term structures under no-arbitrage constraints. ARBITER maps market states to
an operator that outputs implied volatility and variance curves while enforcing
static arbitrage (calendar, vertical, butterfly), Lipschitz bounds, and
monotonicity. The model couples operator learning with constrained decoders and
is trained with extragradient-style updates plus projection. We introduce
evaluation metrics for derivatives term structures (NAS, CNAS, NI, Dual-Gap,
Stability Rate) and show gains over Fourier Neural Operator, DeepONet, and
state-space sequence models on historical SPX and VIX data. Ablation studies
indicate that tying the SPX and VIX legs reduces Dual-Gap and improves NI,
Lipschitz projection stabilizes calibration, and selective state updates
improve long-horizon generalization. We provide identifiability and
approximation results and describe practical recipes for arbitrage-free
interpolation and extrapolation across maturities and strikes.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06451v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06451v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Countering Multi-modal Representation Collapse through Rank-targeted Fusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06450v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06450v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        How Wide and How Deep? Mitigating Over-Squashing of GNNs via Channel Capacity Constrained Estimation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Existing graph neural networks typically rely on heuristic choices for hidden
dimensions and propagation depths, which often lead to severe information loss
during propagation, known as over-squashing. To address this issue, we propose
Channel Capacity Constrained Estimation (C3E), a novel framework that
formulates the selection of hidden dimensions and depth as a nonlinear
programming problem grounded in information theory. Through modeling spectral
graph neural networks as communication channels, our approach directly connects
channel capacity to hidden dimensions, propagation depth, propagation
mechanism, and graph structure. Extensive experiments on nine public datasets
demonstrate that hidden dimensions and depths estimated by C3E can mitigate
over-squashing and consistently improve representation learning. Experimental
results show that over-squashing occurs due to the cumulative compression of
information in representation matrices. Furthermore, our findings show that
increasing hidden dimensions indeed mitigate information compression, while the
role of propagation depth is more nuanced, uncovering a fundamental balance
between information compression and representation complexity.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06443v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06443v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards Resource-Efficient Multimodal Intelligence: Learned Routing among Specialized Expert Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As AI moves beyond text, large language models (LLMs) increasingly power
vision, audio, and document understanding; however, their high inference costs
hinder real-time, scalable deployment. Conversely, smaller open-source models
offer cost advantages but struggle with complex or multimodal queries. We
introduce a unified, modular framework that intelligently routes each query -
textual, multimodal, or complex - to the most fitting expert model, using a
learned routing network that balances cost and quality. For vision tasks, we
employ a two-stage open-source pipeline optimized for efficiency and reviving
efficient classical vision components where they remain SOTA for sub-tasks. On
benchmarks such as Massive Multitask Language Understanding (MMLU) and Visual
Question Answering (VQA), we match or exceed the performance of always-premium
LLM (monolithic systems with one model serving all query types) performance,
yet reduce the reliance on costly models by over 67%. With its extensible,
multi-agent orchestration, we deliver high-quality, resource-efficient AI at
scale.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06441v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06441v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CG-TTRL: Context-Guided Test-Time Reinforcement Learning for On-Device Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Test-time Reinforcement Learning (TTRL) has shown promise in adapting
foundation models for complex tasks at test-time, resulting in large
performance improvements. TTRL leverages an elegant two-phase sampling
strategy: first, multi-sampling derives a pseudo-label via majority voting,
while subsequent downsampling and reward-based fine-tuning encourages the model
to explore and learn diverse valid solutions, with the pseudo-label modulating
the reward signal. Meanwhile, in-context learning has been widely explored at
inference time and demonstrated the ability to enhance model performance
without weight updates. However, TTRL's two-phase sampling strategy
under-utilizes contextual guidance, which can potentially improve pseudo-label
accuracy in the initial exploitation phase while regulating exploration in the
second. To address this, we propose context-guided TTRL (CG-TTRL), integrating
context dynamically into both sampling phases and propose a method for
efficient context selection for on-device applications. Our evaluations on
mathematical and scientific QA benchmarks show CG-TTRL outperforms TTRL (e.g.
additional 7% relative accuracy improvement over TTRL), while boosting
efficiency by obtaining strong performance after only a few steps of test-time
training (e.g. 8% relative improvement rather than 1% over TTRL after 3 steps).
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06430v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06430v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Interpretable representation learning is a central challenge in modern
machine learning, particularly in high-dimensional settings such as
neuroimaging, genomics, and text analysis. Current methods often struggle to
balance the competing demands of interpretability and model flexibility,
limiting their effectiveness in extracting meaningful insights from complex
data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a
general-purpose matrix estimation framework that unifies ideas from sparse
matrix factorization, orthogonalization, and constrained manifold learning.
NSA-Flow enforces structured sparsity through a continuous balance between
reconstruction fidelity and column-wise decorrelation, parameterized by a
single tunable weight. The method operates as a smooth flow near the Stiefel
manifold with proximal updates for non-negativity and adaptive gradient
control, yielding representations that are simultaneously sparse, stable, and
interpretable. Unlike classical regularization schemes, NSA-Flow provides an
intuitive geometric mechanism for manipulating sparsity at the level of global
structure while simplifying latent features. We demonstrate that the NSA-Flow
objective can be optimized smoothly and integrates seamlessly with existing
pipelines for dimensionality reduction while improving interpretability and
generalization in both simulated and real biomedical data. Empirical validation
on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the
NSA-Flow constraints can maintain or improve performance over related methods
with little additional methodological effort. NSA-Flow offers a scalable,
general-purpose tool for interpretable ML, applicable across data science
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06425v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06425v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Fast Riemannian-manifold Hamiltonian Monte Carlo for hierarchical Gaussian-process models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Hierarchical Bayesian models based on Gaussian processes are considered
useful for describing complex nonlinear statistical dependencies among
variables in real-world data. However, effective Monte Carlo algorithms for
inference with these models have not yet been established, except for several
simple cases. In this study, we show that, compared with the slow inference
achieved with existing program libraries, the performance of
Riemannian-manifold Hamiltonian Monte Carlo (RMHMC) can be drastically improved
by optimising the computation order according to the model structure and
dynamically programming the eigendecomposition. This improvement cannot be
achieved when using an existing library based on a naive automatic
differentiator. We numerically demonstrate that RMHMC effectively samples from
the posterior, allowing the calculation of model evidence, in a Bayesian
logistic regression on simulated data and in the estimation of propensity
functions for the American national medical expenditure data using several
Bayesian multiple-kernel models. These results lay a foundation for
implementing effective Monte Carlo algorithms for analysing real-world data
with Gaussian processes, and highlight the need to develop a customisable
library set that allows users to incorporate dynamically programmed objects and
finely optimises the mode of automatic differentiation depending on the model
structure.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06407v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06407v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning the Inverse Ryu--Takayanagi Formula with Transformers
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the inverse problem of holographic entanglement entropy in AdS$_3$
using a data-driven generative model. Training data consist of randomly
generated geometries and their holographic entanglement entropies using the
Ryu--Takayanagi formula. After training, the Transformer reconstructs the
blackening function within our metric ansatz from previously unseen inputs. The
Transformer achieves accurate reconstructions on smooth black hole geometries
and extrapolates to horizonless backgrounds. We describe the architecture and
data generation process, and we quantify accuracy on both $f(z)$ and the
reconstructed $S(\ell)$. Code and evaluation scripts are available at the
provided repository.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06387v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06387v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Vocabulary In-Context Learning in Transformers: Benefits of Positional Encoding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Numerous studies have demonstrated that the Transformer architecture
possesses the capability for in-context learning (ICL). In scenarios involving
function approximation, context can serve as a control parameter for the model,
endowing it with the universal approximation property (UAP). In practice,
context is represented by tokens from a finite set, referred to as a
vocabulary, which is the case considered in this paper, \emph{i.e.}, vocabulary
in-context learning (VICL). We demonstrate that VICL in single-layer
Transformers, without positional encoding, does not possess the UAP; however,
it is possible to achieve the UAP when positional encoding is included. Several
sufficient conditions for the positional encoding are provided. Our findings
reveal the benefits of positional encoding from an approximation theory
perspective in the context of ICL.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06376v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06376v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptive Regularization for Large-Scale Sparse Feature Embedding Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The one-epoch overfitting problem has drawn widespread attention, especially
in CTR and CVR estimation models in search, advertising, and recommendation
domains. These models which rely heavily on large-scale sparse categorical
features, often suffer a significant decline in performance when trained for
multiple epochs. Although recent studies have proposed heuristic solutions,
they have not clearly identified the fundamental cause of this phenomenon. In
this work, we provide a theoretical analysis that explains why overfitting
occurs in models that use large-scale sparse categorical features. Based on
this analysis, we propose an adaptive regularization method to address it. Our
approach not only prevents the severe performance degradation observed during
multi-epoch training, but also improves model performance within a single
epoch. This method has already been deployed in online production systems.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06374v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06374v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scalable Verification of Neural Control Barrier Functions Using Linear Bound Propagation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Control barrier functions (CBFs) are a popular tool for safety certification
of nonlinear dynamical control systems. Recently, CBFs represented as neural
networks have shown great promise due to their expressiveness and applicability
to a broad class of dynamics and safety constraints. However, verifying that a
trained neural network is indeed a valid CBF is a computational bottleneck that
limits the size of the networks that can be used. To overcome this limitation,
we present a novel framework for verifying neural CBFs based on piecewise
linear upper and lower bounds on the conditions required for a neural network
to be a CBF. Our approach is rooted in linear bound propagation (LBP) for
neural networks, which we extend to compute bounds on the gradients of the
network. Combined with McCormick relaxation, we derive linear upper and lower
bounds on the CBF conditions, thereby eliminating the need for computationally
expensive verification procedures. Our approach applies to arbitrary
control-affine systems and a broad range of nonlinear activation functions. To
reduce conservatism, we develop a parallelizable refinement strategy that
adaptively refines the regions over which these bounds are computed. Our
approach scales to larger neural networks than state-of-the-art verification
procedures for CBFs, as demonstrated by our numerical experiments.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06341v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06341v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DRIVE: Data Curation Best Practices for Reinforcement Learning with Verifiable Reward in Competitive Code Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent reasoning-first models (e.g., OpenAI o1, DeepSeek R1) have spurred a
resurgence of interest in RLVR. Nevertheless, advances are dominated by
mathematics (e.g., AIME), with competitive-programming code generation
underexplored and data curation receiving less attention than RL algorithm
design. We investigate how to construct RLVR datasets (i.e., RL prompts) and
present practical training techniques that yield strong performance on
competitive-programming code generation. Our pipeline begins with supervised
fine-tuning (SFT) distilled from strong open-source models, augmented with
general-purpose and reasoning-intensive data. RL then follows a two-stage
process with executable, testcase-driven rewards: first, training on a large,
uniformly distributed set of competitive-programming problems using Group
Relative Policy Optimization (GRPO) with 8 rollouts per prompt and a relatively
short response-generation window (e.g., 32k during SFT and 24k in this stage)
to expand entropy and mitigate repetition and truncation; second, we perform
\textbf{Pre-GRPO}: updating on a small, high-quality set of challenging
problems with a large rollout budget (64 rollouts per prompt) under a
hard-focus curriculum that continuously retains the most difficult instances
throughout training. We implement our method on Qwen2.5-32B and evaluate on
LeetCode and Codeforces weekly contests to avoid data leakage. The resulting
model achieves state-of-the-art performance among models of similar scale and
is comparable to leading systems such as DeepSeek v3.1 and Doubao-1.5-Thinking.
We also examine scaling trends and observe strong RL scaling on an internal
large-scale MoE model. Our study distills concise best practices for data
curation, entropy expansion, and curriculum design in RLVR for
competitive-programming code generation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06307v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06307v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Setting $\varepsilon$ is not the Issue in Differential Privacy
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This position paper argues that setting the privacy budget in differential
privacy should not be viewed as an important limitation of differential privacy
compared to alternative methods for privacy-preserving machine learning. The
so-called problem of interpreting the privacy budget is often presented as a
major hindrance to the wider adoption of differential privacy in real-world
deployments and is sometimes used to promote alternative mitigation techniques
for data protection. We believe this misleads decision-makers into choosing
unsafe methods. We argue that the difficulty in interpreting privacy budgets
does not stem from the definition of differential privacy itself, but from the
intrinsic difficulty of estimating privacy risks in context, a challenge that
any rigorous method for privacy risk assessment face. Moreover, we claim that
any sound method for estimating privacy risks should, given the current state
of research, be expressible within the differential privacy framework or
justify why it cannot.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06305v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06305v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        3dSAGER: Geospatial Entity Resolution over 3D Objects (Technical Report)
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Urban environments are continuously mapped and modeled by various data
collection platforms, including satellites, unmanned aerial vehicles and street
cameras. The growing availability of 3D geospatial data from multiple
modalities has introduced new opportunities and challenges for integrating
spatial knowledge at scale, particularly in high-impact domains such as urban
planning and rapid disaster management. Geospatial entity resolution is the
task of identifying matching spatial objects across different datasets, often
collected independently under varying conditions. Existing approaches typically
rely on spatial proximity, textual metadata, or external identifiers to
determine correspondence. While useful, these signals are often unavailable,
unreliable, or misaligned, especially in cross-source scenarios. To address
these limitations, we shift the focus to the intrinsic geometry of 3D spatial
objects and present 3dSAGER (3D Spatial-Aware Geospatial Entity Resolution), an
end-to-end pipeline for geospatial entity resolution over 3D objects. 3dSAGER
introduces a novel, spatial-reference-independent featurization mechanism that
captures intricate geometric characteristics of matching pairs, enabling robust
comparison even across datasets with incompatible coordinate systems where
traditional spatial methods fail. As a key component of 3dSAGER, we also
propose a new lightweight and interpretable blocking method, BKAFI, that
leverages a trained model to efficiently generate high-recall candidate sets.
We validate 3dSAGER through extensive experiments on real-world urban datasets,
demonstrating significant gains in both accuracy and efficiency over strong
baselines. Our empirical study further dissects the contributions of each
component, providing insights into their impact and the overall design choices.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06300v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06300v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Achieving Fairness Without Harm via Selective Demographic Experts
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As machine learning systems become increasingly integrated into
human-centered domains such as healthcare, ensuring fairness while maintaining
high predictive performance is critical. Existing bias mitigation techniques
often impose a trade-off between fairness and accuracy, inadvertently degrading
performance for certain demographic groups. In high-stakes domains like
clinical diagnosis, such trade-offs are ethically and practically unacceptable.
In this study, we propose a fairness-without-harm approach by learning distinct
representations for different demographic groups and selectively applying
demographic experts consisting of group-specific representations and
personalized classifiers through a no-harm constrained selection. We evaluate
our approach on three real-world medical datasets -- covering eye disease, skin
cancer, and X-ray diagnosis -- as well as two face datasets. Extensive
empirical results demonstrate the effectiveness of our approach in achieving
fairness without harm.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06293v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06293v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM$^3$-DTI: A Large Language Model and Multi-modal data co-powered framework for Drug-Target Interaction prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Drug-target interaction (DTI) prediction is of great significance for drug
discovery and drug repurposing. With the accumulation of a large volume of
valuable data, data-driven methods have been increasingly harnessed to predict
DTIs, reducing costs across various dimensions. Therefore, this paper proposes
a $\textbf{L}$arge $\textbf{L}$anguage $\textbf{M}$odel and
$\textbf{M}$ulti-$\textbf{M}$odel data co-powered $\textbf{D}$rug
$\textbf{T}$arget $\textbf{I}$nteraction prediction framework, named
LLM$^3$-DTI. LLM$^3$-DTI constructs multi-modal data embedding to enhance DTI
prediction performance. In this framework, the text semantic embeddings of
drugs and targets are encoded by a domain-specific LLM. To effectively align
and fuse multi-modal embedding. We propose the dual cross-attention mechanism
and the TSFusion module. Finally, these multi-modal data are utilized for the
DTI task through an output network. The experimental results indicate that
LLM$^3$-DTI can proficiently identify validated DTIs, surpassing the
performance of the models employed for comparison across diverse scenarios.
Consequently, LLM$^3$-DTI is adept at fulfilling the task of DTI prediction
with excellence. The data and code are available at
https://github.com/chaser-gua/LLM3DTI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06269v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06269v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CAMP-HiVe: Cyclic Pair Merging based Efficient DNN Pruning with Hessian-Vector Approximation for Resource-Constrained Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep learning algorithms are becoming an essential component of many
artificial intelligence (AI) driven applications, many of which run on
resource-constrained and energy-constrained systems. For efficient deployment
of these algorithms, although different techniques for the compression of
neural network models are proposed, neural pruning is one of the fastest and
effective methods, which can provide a high compression gain with minimal cost.
To harness enhanced performance gain with respect to model complexity, we
propose a novel neural network pruning approach utilizing Hessian-vector
products that approximate crucial curvature information in the loss function,
which significantly reduces the computation demands. By employing a power
iteration method, our algorithm effectively identifies and preserves the
essential information, ensuring a balanced trade-off between model accuracy and
computational efficiency. Herein, we introduce CAMP-HiVe, a cyclic pair
merging-based pruning with Hessian Vector approximation by iteratively
consolidating weight pairs, combining significant and less significant weights,
thus effectively streamlining the model while preserving its performance. This
dynamic, adaptive framework allows for real-time adjustment of weight
significance, ensuring that only the most critical parameters are retained. Our
experimental results demonstrate that our proposed method achieves significant
reductions in computational requirements while maintaining high performance
across different neural network architectures, e.g., ResNet18, ResNet56, and
MobileNetv2, on standard benchmark datasets, e.g., CIFAR10, CIFAR-100, and
ImageNet, and it outperforms the existing state-of-the-art neural pruning
methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06265v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06265v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Test-Time Iterative Error Correction for Efficient Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the growing demand for high-quality image generation on
resource-constrained devices, efficient diffusion models have received
increasing attention. However, such models suffer from approximation errors
introduced by efficiency techniques, which significantly degrade generation
quality. Once deployed, these errors are difficult to correct, as modifying the
model is typically infeasible in deployment environments. Through an analysis
of error propagation across diffusion timesteps, we reveal that these
approximation errors can accumulate exponentially, severely impairing output
quality. Motivated by this insight, we propose Iterative Error Correction
(IEC), a novel test-time method that mitigates inference-time errors by
iteratively refining the model's output. IEC is theoretically proven to reduce
error propagation from exponential to linear growth, without requiring any
retraining or architectural changes. IEC can seamlessly integrate into the
inference process of existing diffusion models, enabling a flexible trade-off
between performance and efficiency. Extensive experiments show that IEC
consistently improves generation quality across various datasets, efficiency
techniques, and model architectures, establishing it as a practical and
generalizable solution for test-time enhancement of efficient diffusion models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06250v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06250v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Functional Adjoint Sampler: Scalable Sampling on Infinite Dimensional Spaces
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Learning-based methods for sampling from the Gibbs distribution in
finite-dimensional spaces have progressed quickly, yet theory and algorithmic
design for infinite-dimensional function spaces remain limited. This gap
persists despite their strong potential for sampling the paths of conditional
diffusion processes, enabling efficient simulation of trajectories of diffusion
processes that respect rare events or boundary constraints. In this work, we
present the adjoint sampler for infinite-dimensional function spaces, a
stochastic optimal control-based diffusion sampler that operates in function
space and targets Gibbs-type distributions on infinite-dimensional Hilbert
spaces. Our Functional Adjoint Sampler (FAS) generalizes Adjoint Sampling
(Havens et al., 2025) to Hilbert spaces based on a SOC theory called stochastic
maximum principle, yielding a simple and scalable matching-type objective for a
functional representation. We show that FAS achieves superior transition path
sampling performance across synthetic potential and real molecular systems,
including Alanine Dipeptide and Chignolin.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06239v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06239v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sparsity via Hyperpriors: A Theoretical and Algorithmic Study under Empirical Bayes Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper presents a comprehensive analysis of hyperparameter estimation
within the empirical Bayes framework (EBF) for sparse learning. By studying the
influence of hyperpriors on the solution of EBF, we establish a theoretical
connection between the choice of the hyperprior and the sparsity as well as the
local optimality of the resulting solutions. We show that some strictly
increasing hyperpriors, such as half-Laplace and half-generalized Gaussian with
the power in $(0,1)$, effectively promote sparsity and improve solution
stability with respect to measurement noise. Based on this analysis, we adopt a
proximal alternating linearized minimization (PALM) algorithm with convergence
guaranties for both convex and concave hyperpriors. Extensive numerical tests
on two-dimensional image deblurring problems demonstrate that introducing
appropriate hyperpriors significantly promotes the sparsity of the solution and
enhances restoration accuracy. Furthermore, we illustrate the influence of the
noise level and the ill-posedness of inverse problems to EBF solutions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06235v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06235v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Synheart Emotion: Privacy-Preserving On-Device Emotion Recognition from Biosignals
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Human-computer interaction increasingly demands systems that recognize not
only explicit user inputs but also implicit emotional states. While substantial
progress has been made in affective computing, most emotion recognition systems
rely on cloud-based inference, introducing privacy vulnerabilities and latency
constraints unsuitable for real-time applications. This work presents a
comprehensive evaluation of machine learning architectures for on-device
emotion recognition from wrist-based photoplethysmography (PPG), systematically
comparing different models spanning classical ensemble methods, deep neural
networks, and transformers on the WESAD stress detection dataset. Results
demonstrate that classical ensemble methods substantially outperform deep
learning on small physiological datasets, with ExtraTrees achieving F1 = 0.826
on combined features and F1 = 0.623 on wrist-only features, compared to
transformers achieving only F1 = 0.509-0.577. We deploy the wrist-only
ExtraTrees model optimized via ONNX conversion, achieving a 4.08 MB footprint,
0.05 ms inference latency, and 152x speedup over the original implementation.
Furthermore, ONNX optimization yields a 30.5% average storage reduction and
40.1x inference speedup, highlighting the feasibility of privacy-preserving
on-device emotion recognition for real-world wearables.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06231v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06231v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Deep Reinforcement Learning for Dynamic Origin-Destination Matrix Estimation in Microscopic Traffic Simulations Considering Credit Assignment
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper focuses on dynamic origin-destination matrix estimation (DODE), a
crucial calibration process necessary for the effective application of
microscopic traffic simulations. The fundamental challenge of the DODE problem
in microscopic simulations stems from the complex temporal dynamics and
inherent uncertainty of individual vehicle dynamics. This makes it highly
challenging to precisely determine which vehicle traverses which link at any
given moment, resulting in intricate and often ambiguous relationships between
origin-destination (OD) matrices and their contributions to resultant link
flows. This phenomenon constitutes the credit assignment problem, a central
challenge addressed in this study. We formulate the DODE problem as a Markov
Decision Process (MDP) and propose a novel framework that applies model-free
deep reinforcement learning (DRL). Within our proposed framework, the agent
learns an optimal policy to sequentially generate OD matrices, refining its
strategy through direct interaction with the simulation environment. The
proposed method is validated on the Nguyen-Dupuis network using SUMO, where its
performance is evaluated against ground-truth link flows aggregated at 5-minute
intervals over a 30-minute horizon. Experimental results demonstrate that our
approach achieves a 43.2% reduction in mean squared error (MSE) compared to the
best-performing conventional baseline. By reframing DODE as a sequential
decision-making problem, our approach addresses the credit assignment challenge
through its learned policy, thereby overcoming the limitations of conventional
methods and proposing a novel framework for calibration of microscopic traffic
simulations.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06229v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06229v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptive Multi-view Graph Contrastive Learning via Fractional-order Neural Diffusion Networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Graph contrastive learning (GCL) learns node and graph representations by
contrasting multiple views of the same graph. Existing methods typically rely
on fixed, handcrafted views-usually a local and a global perspective, which
limits their ability to capture multi-scale structural patterns. We present an
augmentation-free, multi-view GCL framework grounded in fractional-order
continuous dynamics. By varying the fractional derivative order $\alpha \in
(0,1]$, our encoders produce a continuous spectrum of views: small $\alpha$
yields localized features, while large $\alpha$ induces broader, global
aggregation. We treat $\alpha$ as a learnable parameter so the model can adapt
diffusion scales to the data and automatically discover informative views. This
principled approach generates diverse, complementary representations without
manual augmentations. Extensive experiments on standard benchmarks demonstrate
that our method produces more robust and expressive embeddings and outperforms
state-of-the-art GCL baselines.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06216v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06216v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Time Matters: A Novel Real-Time Long- and Short-term User Interest Model for Click-Through Rate Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Click-Through Rate (CTR) prediction is a core task in online personalization
platform. A key step for CTR prediction is to learn accurate user
representation to capture their interests. Generally, the interest expressed by
a user is time-variant, i.e., a user activates different interests at different
time. However, most previous CTR prediction methods overlook the correlation
between the activated interest and the occurrence time, resulting in what they
actually learn is the mixture of the interests expressed by the user at all
time, rather than the real-time interest at the certain prediction time. To
capture the correlation between the activated interest and the occurrence time,
in this paper we investigate users' interest evolution from the perspective of
the whole time line and develop two regular patterns: periodic pattern and
time-point pattern. Based on the two patterns, we propose a novel time-aware
long- and short-term user interest modeling method to model users' dynamic
interests at different time. Extensive experiments on public datasets as well
as an industrial dataset verify the effectiveness of exploiting the two
patterns and demonstrate the superiority of our proposed method compared with
other state-of-the-art ones.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06213v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06213v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sparse Linear Regression is Easy on Random Supports
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sparse linear regression is one of the most basic questions in machine
learning and statistics. Here, we are given as input a design matrix $X \in
\mathbb{R}^{N \times d}$ and measurements or labels ${y} \in \mathbb{R}^N$
where ${y} = {X} {w}^* + {\xi}$, and ${\xi}$ is the noise in the measurements.
Importantly, we have the additional constraint that the unknown signal vector
${w}^*$ is sparse: it has $k$ non-zero entries where $k$ is much smaller than
the ambient dimension. Our goal is to output a prediction vector
$\widehat{{w}}$ that has small prediction error: $\frac{1}{N}\cdot \|{X} {w}^*
- {X} \widehat{{w}}\|^2_2$.
  Information-theoretically, we know what is best possible in terms of
measurements: under most natural noise distributions, we can get prediction
error at most $\epsilon$ with roughly $N = O(k \log d/\epsilon)$ samples.
Computationally, this currently needs $d^{\Omega(k)}$ run-time. Alternately,
with $N = O(d)$, we can get polynomial-time. Thus, there is an exponential gap
(in the dependence on $d$) between the two and we do not know if it is possible
to get $d^{o(k)}$ run-time and $o(d)$ samples.
  We give the first generic positive result for worst-case design matrices
${X}$: For any ${X}$, we show that if the support of ${w}^*$ is chosen at
random, we can get prediction error $\epsilon$ with $N = \text{poly}(k, \log d,
1/\epsilon)$ samples and run-time $\text{poly}(d,N)$. This run-time holds for
any design matrix ${X}$ with condition number up to $2^{\text{poly}(d)}$.
  Previously, such results were known for worst-case ${w}^*$, but only for
random design matrices from well-behaved families, matrices that have a very
low condition number ($\text{poly}(\log d)$; e.g., as studied in compressed
sensing), or those with special structural properties.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06211v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06211v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Local K-Similarity Constraint for Federated Learning with Label Noise
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Federated learning on clients with noisy labels is a challenging problem, as
such clients can infiltrate the global model, impacting the overall
generalizability of the system. Existing methods proposed to handle noisy
clients assume that a sufficient number of clients with clean labels are
available, which can be leveraged to learn a robust global model while
dampening the impact of noisy clients. This assumption fails when a high number
of heterogeneous clients contain noisy labels, making the existing approaches
ineffective. In such scenarios, it is important to locally regularize the
clients before communication with the global model, to ensure the global model
isn't corrupted by noisy clients. While pre-trained self-supervised models can
be effective for local regularization, existing centralized approaches relying
on pretrained initialization are impractical in a federated setting due to the
potentially large size of these models, which increases communication costs. In
that line, we propose a regularization objective for client models that
decouples the pre-trained and classification models by enforcing similarity
between close data points within the client. We leverage the representation
space of a self-supervised pretrained model to evaluate the closeness among
examples. This regularization, when applied with the standard objective
function for the downstream task in standard noisy federated settings,
significantly improves performance, outperforming existing state-of-the-art
federated methods in multiple computer vision and medical image classification
benchmarks. Unlike other techniques that rely on self-supervised pretrained
initialization, our method does not require the pretrained model and classifier
backbone to share the same architecture, making it architecture-agnostic.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06169v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06169v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-8" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Vision
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 13 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前视觉技术方向的研究动态主要集中在提升模型对复杂视觉内容的理解能力和多模态信息处理的效率。研究者们通过引入新的基准和架构，提升了视觉-语言模型在漫画理解、目光理解和红外-可见物体检测等任务中的表现。此外，针对生成图像的检测问题，研究者们也在探索更有效的检测方法，显示出对AI生成内容的关注和应对策略。这些趋势表明，未来的研究将更加注重模型的灵活性和适应性，以应对多样化的视觉任务。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Zooming into Comics: Region-Aware RL Improves Fine-Grained Comic Understanding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Complex visual narratives, such as comics, present a significant challenge to
Vision-Language Models (VLMs). Despite excelling on natural images, VLMs often
struggle with stylized line art, onomatopoeia, and densely packed multi-panel
layouts. To address this gap, we introduce AI4VA-FG, the first fine-grained and
comprehensive benchmark for VLM-based comic understanding. It spans tasks from
foundational recognition and detection to high-level character reasoning and
narrative construction, supported by dense annotations for characters, poses,
and depth. Beyond that, we evaluate state-of-the-art proprietary models,
including GPT-4o and Gemini-2.5, and open-source models such as Qwen2.5-VL,
revealing substantial performance deficits across core tasks of our benchmarks
and underscoring that comic understanding remains an unsolved challenge. To
enhance VLMs' capabilities in this domain, we systematically investigate
post-training strategies, including supervised fine-tuning on solutions
(SFT-S), supervised fine-tuning on reasoning trajectories (SFT-R), and
reinforcement learning (RL). Beyond that, inspired by the emerging "Thinking
with Images" paradigm, we propose Region-Aware Reinforcement Learning (RARL)
for VLMs, which trains models to dynamically attend to relevant regions through
zoom-in operations. We observe that when applied to the Qwen2.5-VL model, RL
and RARL yield significant gains in low-level entity recognition and high-level
storyline ordering, paving the way for more accurate and efficient VLM
applications in the comics domain.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06490v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06490v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Personality over Precision: Exploring the Influence of Human-Likeness on ChatGPT Use for Search
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Conversational search interfaces, like ChatGPT, offer an interactive,
personalized, and engaging user experience compared to traditional search. On
the downside, they are prone to cause overtrust issues where users rely on
their responses even when they are incorrect. What aspects of the
conversational interaction paradigm drive people to adopt it, and how it
creates personalized experiences that lead to overtrust, is not clear. To
understand the factors influencing the adoption of conversational interfaces,
we conducted a survey with 173 participants. We examined user perceptions
regarding trust, human-likeness (anthropomorphism), and design preferences
between ChatGPT and Google. To better understand the overtrust phenomenon, we
asked users about their willingness to trade off factuality for constructs like
ease of use or human-likeness. Our analysis identified two distinct user
groups: those who use both ChatGPT and Google daily (DUB), and those who
primarily rely on Google (DUG). The DUB group exhibited higher trust in
ChatGPT, perceiving it as more human-like, and expressed greater willingness to
trade factual accuracy for enhanced personalization and conversational flow.
Conversely, the DUG group showed lower trust toward ChatGPT but still
appreciated aspects like ad-free experiences and responsive interactions.
Demographic analysis further revealed nuanced patterns, with middle-aged adults
using ChatGPT less frequently yet trusting it more, suggesting potential
vulnerability to misinformation. Our findings contribute to understanding user
segmentation, emphasizing the critical roles of personalization and
human-likeness in conversational IR systems, and reveal important implications
regarding users' willingness to compromise factual accuracy for more engaging
interactions.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06447v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06447v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Turbo-DDCM: Fast and Flexible Zero-Shot Diffusion-Based Image Compression
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While zero-shot diffusion-based compression methods have seen significant
progress in recent years, they remain notoriously slow and computationally
demanding. This paper presents an efficient zero-shot diffusion-based
compression method that runs substantially faster than existing methods, while
maintaining performance that is on par with the state-of-the-art techniques.
Our method builds upon the recently proposed Denoising Diffusion Codebook
Models (DDCMs) compression scheme. Specifically, DDCM compresses an image by
sequentially choosing the diffusion noise vectors from reproducible random
codebooks, guiding the denoiser's output to reconstruct the target image. We
modify this framework with Turbo-DDCM, which efficiently combines a large
number of noise vectors at each denoising step, thereby significantly reducing
the number of required denoising operations. This modification is also coupled
with an improved encoding protocol. Furthermore, we introduce two flexible
variants of Turbo-DDCM, a priority-aware variant that prioritizes
user-specified regions and a distortion-controlled variant that compresses an
image based on a target PSNR rather than a target BPP. Comprehensive
experiments position Turbo-DDCM as a compelling, practical, and flexible image
compression scheme.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06424v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06424v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On Modality Incomplete Infrared-Visible Object Detection: An Architecture Compatibility Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Infrared and visible object detection (IVOD) is essential for numerous
around-the-clock applications. Despite notable advancements, current IVOD
models exhibit notable performance declines when confronted with incomplete
modality data, particularly if the dominant modality is missing. In this paper,
we take a thorough investigation on modality incomplete IVOD problem from an
architecture compatibility perspective. Specifically, we propose a
plug-and-play Scarf Neck module for DETR variants, which introduces a
modality-agnostic deformable attention mechanism to enable the IVOD detector to
flexibly adapt to any single or double modalities during training and
inference. When training Scarf-DETR, we design a pseudo modality dropout
strategy to fully utilize the multi-modality information, making the detector
compatible and robust to both working modes of single and double modalities.
Moreover, we introduce a comprehensive benchmark for the modality-incomplete
IVOD task aimed at thoroughly assessing situations where the absent modality is
either dominant or secondary. Our proposed Scarf-DETR not only performs
excellently in missing modality scenarios but also achieves superior
performances on the standard IVOD modality complete benchmarks. Our code will
be available at https://github.com/YinghuiXing/Scarf-DETR.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06406v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06406v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GazeVLM: A Vision-Language Model for Multi-Task Gaze Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Gaze understanding unifies the detection of people, their gaze targets, and
objects of interest into a single framework, offering critical insight into
visual attention and intent estimation. Although prior research has modelled
gaze cues in visual scenes, a unified system is still needed for gaze
understanding using both visual and language prompts. This paper introduces
GazeVLM, a novel Vision-Language Model (VLM) for multi-task gaze understanding
in images, addressing person detection, gaze target detection, and gaze object
identification. While other transformer-based methods exist for gaze analysis,
GazeVLM represents, to our knowledge, the first application of a VLM to these
combined tasks, allowing for selective execution of each task. Through the
integration of visual (RGB and depth) and textual modalities, our ablation
study on visual input combinations revealed that a fusion of RGB images with
HHA-encoded depth maps, guided by text prompts, yields superior performance. We
also introduce an object-level gaze detection metric for gaze object
identification ($AP_{ob}$). Through experiments, GazeVLM demonstrates
significant improvements, notably achieving state-of-the-art evaluation scores
on GazeFollow and VideoAttentionTarget datasets.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06348v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06348v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CINEMAE: Leveraging Frozen Masked Autoencoders for Cross-Generator AI Image Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While context-based detectors have achieved strong generalization for
AI-generated text by measuring distributional inconsistencies, image-based
detectors still struggle with overfitting to generator-specific artifacts. We
introduce CINEMAE, a novel paradigm for AIGC image detection that adapts the
core principles of text detection methods to the visual domain. Our key insight
is that Masked AutoEncoder (MAE), trained to reconstruct masked patches
conditioned on visible context, naturally encodes semantic consistency
expectations. We formalize this reconstruction process probabilistically,
computing conditional Negative Log-Likelihood (NLL, p(masked | visible)) to
quantify local semantic anomalies. By aggregating these patch-level statistics
with global MAE features through learned fusion, CINEMAE achieves strong
cross-generator generalization. Trained exclusively on Stable Diffusion v1.4,
our method achieves over 95% accuracy on all eight unseen generators in the
GenImage benchmark, substantially outperforming state-of-the-art detectors.
This demonstrates that context-conditional reconstruction uncertainty provides
a robust, transferable signal for AIGC detection.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06325v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06325v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LaneDiffusion: Improving Centerline Graph Learning via Prior Injected BEV Feature Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Centerline graphs, crucial for path planning in autonomous driving, are
traditionally learned using deterministic methods. However, these methods often
lack spatial reasoning and struggle with occluded or invisible centerlines.
Generative approaches, despite their potential, remain underexplored in this
domain. We introduce LaneDiffusion, a novel generative paradigm for centerline
graph learning. LaneDiffusion innovatively employs diffusion models to generate
lane centerline priors at the Bird's Eye View (BEV) feature level, instead of
directly predicting vectorized centerlines. Our method integrates a Lane Prior
Injection Module (LPIM) and a Lane Prior Diffusion Module (LPDM) to effectively
construct diffusion targets and manage the diffusion process. Furthermore,
vectorized centerlines and topologies are then decoded from these
prior-injected BEV features. Extensive evaluations on the nuScenes and
Argoverse2 datasets demonstrate that LaneDiffusion significantly outperforms
existing methods, achieving improvements of 4.2%, 4.6%, 4.7%, 6.4% and 1.8% on
fine-grained point-level metrics (GEO F1, TOPO F1, JTOPO F1, APLS and SDA) and
2.3%, 6.4%, 6.8% and 2.1% on segment-level metrics (IoU, mAP_cf, DET_l and
TOP_ll). These results establish state-of-the-art performance in centerline
graph learning, offering new insights into generative models for this task.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06272v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06272v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MambaOVSR: Multiscale Fusion with Global Motion Modeling for Chinese Opera Video Super-Resolution
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chinese opera is celebrated for preserving classical art. However, early
filming equipment limitations have degraded videos of last-century performances
by renowned artists (e.g., low frame rates and resolution), hindering archival
efforts. Although space-time video super-resolution (STVSR) has advanced
significantly, applying it directly to opera videos remains challenging. The
scarcity of datasets impedes the recovery of high frequency details, and
existing STVSR methods lack global modeling capabilities, compromising visual
quality when handling opera's characteristic large motions. To address these
challenges, we pioneer a large scale Chinese Opera Video Clip (COVC) dataset
and propose the Mamba-based multiscale fusion network for space-time Opera
Video Super-Resolution (MambaOVSR). Specifically, MambaOVSR involves three
novel components: the Global Fusion Module (GFM) for motion modeling through a
multiscale alternating scanning mechanism, and the Multiscale Synergistic Mamba
Module (MSMM) for alignment across different sequence lengths. Additionally,
our MambaVR block resolves feature artifacts and positional information loss
during alignment. Experimental results on the COVC dataset show that MambaOVSR
significantly outperforms the SOTA STVSR method by an average of 1.86 dB in
terms of PSNR. Dataset and Code will be publicly released.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06172v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06172v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Countering Multi-modal Representation Collapse through Rank-targeted Fusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-modal fusion methods often suffer from two types of representation
collapse: feature collapse where individual dimensions lose their
discriminative power (as measured by eigenspectra), and modality collapse where
one dominant modality overwhelms the other. Applications like human action
anticipation that require fusing multifarious sensor data are hindered by both
feature and modality collapse. However, existing methods attempt to counter
feature collapse and modality collapse separately. This is because there is no
unifying framework that efficiently addresses feature and modality collapse in
conjunction. In this paper, we posit the utility of effective rank as an
informative measure that can be utilized to quantify and counter both the
representation collapses. We propose \textit{Rank-enhancing Token Fuser}, a
theoretically grounded fusion framework that selectively blends less
informative features from one modality with complementary features from another
modality. We show that our method increases the effective rank of the fused
representation. To address modality collapse, we evaluate modality combinations
that mutually increase each others' effective rank. We show that depth
maintains representational balance when fused with RGB, avoiding modality
collapse. We validate our method on action anticipation, where we present
\texttt{R3D}, a depth-informed fusion framework. Extensive experiments on
NTURGBD, UTKinect, and DARai demonstrate that our approach significantly
outperforms prior state-of-the-art methods by up to 3.74\%. Our code is
available at:
\href{https://github.com/olivesgatech/R3D}{https://github.com/olivesgatech/R3D}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06450v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06450v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Non-Negative Stiefel Approximating Flow: Orthogonalish Matrix Optimization for Interpretable Embeddings
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Interpretable representation learning is a central challenge in modern
machine learning, particularly in high-dimensional settings such as
neuroimaging, genomics, and text analysis. Current methods often struggle to
balance the competing demands of interpretability and model flexibility,
limiting their effectiveness in extracting meaningful insights from complex
data. We introduce Non-negative Stiefel Approximating Flow (NSA-Flow), a
general-purpose matrix estimation framework that unifies ideas from sparse
matrix factorization, orthogonalization, and constrained manifold learning.
NSA-Flow enforces structured sparsity through a continuous balance between
reconstruction fidelity and column-wise decorrelation, parameterized by a
single tunable weight. The method operates as a smooth flow near the Stiefel
manifold with proximal updates for non-negativity and adaptive gradient
control, yielding representations that are simultaneously sparse, stable, and
interpretable. Unlike classical regularization schemes, NSA-Flow provides an
intuitive geometric mechanism for manipulating sparsity at the level of global
structure while simplifying latent features. We demonstrate that the NSA-Flow
objective can be optimized smoothly and integrates seamlessly with existing
pipelines for dimensionality reduction while improving interpretability and
generalization in both simulated and real biomedical data. Empirical validation
on the Golub leukemia dataset and in Alzheimer's disease demonstrate that the
NSA-Flow constraints can maintain or improve performance over related methods
with little additional methodological effort. NSA-Flow offers a scalable,
general-purpose tool for interpretable ML, applicable across data science
domains.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06425v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06425v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CAMP-HiVe: Cyclic Pair Merging based Efficient DNN Pruning with Hessian-Vector Approximation for Resource-Constrained Systems
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep learning algorithms are becoming an essential component of many
artificial intelligence (AI) driven applications, many of which run on
resource-constrained and energy-constrained systems. For efficient deployment
of these algorithms, although different techniques for the compression of
neural network models are proposed, neural pruning is one of the fastest and
effective methods, which can provide a high compression gain with minimal cost.
To harness enhanced performance gain with respect to model complexity, we
propose a novel neural network pruning approach utilizing Hessian-vector
products that approximate crucial curvature information in the loss function,
which significantly reduces the computation demands. By employing a power
iteration method, our algorithm effectively identifies and preserves the
essential information, ensuring a balanced trade-off between model accuracy and
computational efficiency. Herein, we introduce CAMP-HiVe, a cyclic pair
merging-based pruning with Hessian Vector approximation by iteratively
consolidating weight pairs, combining significant and less significant weights,
thus effectively streamlining the model while preserving its performance. This
dynamic, adaptive framework allows for real-time adjustment of weight
significance, ensuring that only the most critical parameters are retained. Our
experimental results demonstrate that our proposed method achieves significant
reductions in computational requirements while maintaining high performance
across different neural network architectures, e.g., ResNet18, ResNet56, and
MobileNetv2, on standard benchmark datasets, e.g., CIFAR10, CIFAR-100, and
ImageNet, and it outperforms the existing state-of-the-art neural pruning
methods.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06265v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06265v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Test-Time Iterative Error Correction for Efficient Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the growing demand for high-quality image generation on
resource-constrained devices, efficient diffusion models have received
increasing attention. However, such models suffer from approximation errors
introduced by efficiency techniques, which significantly degrade generation
quality. Once deployed, these errors are difficult to correct, as modifying the
model is typically infeasible in deployment environments. Through an analysis
of error propagation across diffusion timesteps, we reveal that these
approximation errors can accumulate exponentially, severely impairing output
quality. Motivated by this insight, we propose Iterative Error Correction
(IEC), a novel test-time method that mitigates inference-time errors by
iteratively refining the model's output. IEC is theoretically proven to reduce
error propagation from exponential to linear growth, without requiring any
retraining or architectural changes. IEC can seamlessly integrate into the
inference process of existing diffusion models, enabling a flexible trade-off
between performance and efficiency. Extensive experiments show that IEC
consistently improves generation quality across various datasets, efficiency
techniques, and model architectures, establishing it as a practical and
generalizable solution for test-time enhancement of efficient diffusion models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06250v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06250v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Multimodal Misinformation Detection by Replaying the Whole Story from Image Modality Perspective
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multimodal Misinformation Detection (MMD) refers to the task of detecting
social media posts involving misinformation, where the post often contains text
and image modalities. However, by observing the MMD posts, we hold that the
text modality may be much more informative than the image modality because the
text generally describes the whole event/story of the current post but the
image often presents partial scenes only. Our preliminary empirical results
indicate that the image modality exactly contributes less to MMD. Upon this
idea, we propose a new MMD method named RETSIMD. Specifically, we suppose that
each text can be divided into several segments, and each text segment describes
a partial scene that can be presented by an image. Accordingly, we split the
text into a sequence of segments, and feed these segments into a pre-trained
text-to-image generator to augment a sequence of images. We further incorporate
two auxiliary objectives concerning text-image and image-label mutual
information, and further post-train the generator over an auxiliary
text-to-image generation benchmark dataset. Additionally, we propose a graph
structure by defining three heuristic relationships between images, and use a
graph neural network to generate the fused features. Extensive empirical
results validate the effectiveness of RETSIMD.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-11-09/2511_06284v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06284v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
        </div>
        
        

        <div class="footer">
            <p>生成时间: 2025-11-20 17:52:55</p>
            <p>访问地址: <a href="https://jycarlos1019.pp.ua">https://jycarlos1019.pp.ua</a></p>
        </div>
    </div>
</body>
</html>