<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>主流技术方向 - 2025-11-09</title>
    <style>
        :root {
            /* 系统配色 */
            --primary-color: #4f46e5;   /* Indigo */
            --direction-color: #f97316; /* Orange 500 */
            --direction-dark: #c2410c;  /* Orange 700 */
            --direction-light: #fff7ed; /* Orange 50 */
            --direction-border: #fdba74;/* Orange 300 */
            
            --bg-body: #f8fafc;
            --bg-card: #ffffff;
            --text-main: #0f172a;
            --text-secondary: #64748b;
            --text-light: #94a3b8;
            --border-color: #e2e8f0;
            
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            
            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        /* SVG 图标 */
        .icon {
            width: 1.1em;
            height: 1.1em;
            display: inline-block;
            vertical-align: middle;
            stroke-width: 2;
            stroke: currentColor;
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
        }

        /* 导航栏 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            font-size: 14px;
        }

        .back-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: flex;
            align-items: center;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--primary-color); }
        .back-link .icon { margin-right: 6px; }

        .date-badge {
            background-color: #e0e7ff;
            color: var(--primary-color);
            padding: 4px 12px;
            border-radius: 99px;
            font-weight: 600;
            font-size: 13px;
        }

        /* 头部 */
        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .header h1 {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
            margin-bottom: 12px;
        }
        
        .header h1 .icon { color: var(--direction-color); width: 36px; height: 36px; }
        
        .header .subtitle { 
            color: var(--text-secondary);
            font-size: 16px;
        }

        /* --- Tab 导航栏 --- */
        .tabs-container {
            position: sticky;
            top: 20px;
            z-index: 100;
            margin-bottom: 30px;
            /* 磨砂玻璃效果 */
            background: rgba(248, 250, 252, 0.9);
            backdrop-filter: blur(8px);
            padding: 10px 0;
            border-radius: 16px;
        }

        .tabs-scroll {
            display: flex;
            gap: 12px;
            overflow-x: auto;
            padding: 4px 4px 12px 4px; /* 底部留空间给滚动条或阴影 */
            
            /* 隐藏滚动条但保持可滚动 */
            scrollbar-width: none; /* Firefox */
            -ms-overflow-style: none;  /* IE 10+ */
            
            /* 鼠标交互优化 */
            cursor: grab; /* 提示可拖拽 */
            user-select: none; /* 防止拖拽时选中文字 */
        }
        
        .tabs-scroll::-webkit-scrollbar { 
            display: none; /* Chrome/Safari */
        }

        /* 拖拽时的光标状态 */
        .tabs-scroll.active {
            cursor: grabbing;
        }

        .tab-btn {
            flex-shrink: 0;
            background-color: var(--bg-card);
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
            padding: 10px 20px;
            border-radius: 99px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: var(--shadow-sm);
            display: flex;
            align-items: center;
            gap: 8px;
            /* 防止图片/文字干扰拖拽 */
            pointer-events: auto; 
        }

        .tab-btn:hover {
            border-color: var(--direction-border);
            color: var(--direction-color);
            transform: translateY(-2px);
        }

        .tab-btn.active {
            background-color: var(--direction-color);
            color: white;
            border-color: var(--direction-color);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3); /* Orange glow */
        }

        .tab-count {
            background-color: rgba(0,0,0,0.1);
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 11px;
        }
        
        .tab-btn.active .tab-count {
            background-color: rgba(255,255,255,0.2);
        }

        /* --- 内容区域 --- */
        .tab-pane {
            display: none; /* 默认隐藏 */
            animation: fadeIn 0.3s ease-out;
        }
        
        .tab-pane.active {
            display: block;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .direction-block {
            background-color: var(--bg-card);
            border-radius: 20px;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-md);
            overflow: hidden;
        }

        .direction-header-info {
            background-color: var(--direction-light);
            padding: 24px 30px;
            border-bottom: 1px solid var(--direction-border);
        }
        
        .direction-title-lg {
            font-size: 22px;
            font-weight: 800;
            color: var(--direction-dark);
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .direction-desc-lg {
            font-size: 15px;
            color: #9a3412;
            opacity: 0.9;
        }

        /* --- 手风琴论文列表 --- */
        .paper-list {
            padding: 10px 30px 30px;
        }

        .paper-item {
            border-bottom: 1px solid var(--border-color);
        }
        
        .paper-item:last-child { border-bottom: none; }

        /* 折叠头部 (可点击) */
        .paper-header {
            padding: 20px 0;
            cursor: pointer;
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            gap: 16px;
            group: paper-header;
        }

        .paper-title-row {
            flex-grow: 1;
        }

        .paper-title {
            font-size: 17px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            transition: color 0.2s;
            margin-bottom: 6px;
        }
        
        .paper-header:hover .paper-title {
            color: var(--direction-color);
        }

        .paper-meta-preview {
            font-size: 13px;
            color: var(--text-light);
            display: flex;
            align-items: center;
            gap: 12px;
        }
        
        .score-badge {
            background-color: #f3f4f6;
            color: var(--text-secondary);
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            font-size: 12px;
        }
        
        .toggle-icon {
            color: var(--text-light);
            transition: transform 0.3s ease, color 0.2s;
            flex-shrink: 0;
            margin-top: 4px;
        }
        
        /* 激活状态样式 */
        .paper-item.expanded .toggle-icon {
            transform: rotate(180deg);
            color: var(--direction-color);
        }
        
        .paper-item.expanded .paper-title {
            color: var(--direction-color);
        }

        /* 折叠内容区 */
        .paper-body {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
            padding-left: 4px; /* 微调对齐 */
        }
        
        .paper-content-inner {
            padding-bottom: 24px;
            color: var(--text-secondary);
            font-size: 15px;
            line-height: 1.7;
            text-align: justify;
        }
        
        .paper-links {
            margin-top: 12px;
            padding-top: 12px;
            border-top: 1px dashed var(--border-color);
            display: flex;
            gap: 16px;
            font-size: 13px;
        }
        
        .action-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        
        .action-link:hover { text-decoration: underline; }

        /* 空状态 */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            background-color: var(--bg-card);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .footer {
            margin-top: 60px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }
        
        .footer a { color: var(--text-secondary); text-decoration: none; }

        @media (max-width: 640px) {
            .container { padding: 20px 15px; }
            .direction-header-info { padding: 20px; }
            .paper-list { padding: 0 20px 20px; }
            .paper-title { font-size: 16px; }
        }
    </style>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 1. Tab 栏鼠标拖拽滚动 (Draggable Scroll)
            const slider = document.querySelector('.tabs-scroll');
            let isDown = false;
            let startX;
            let scrollLeft;
            let isDragging = false; // 区分是“点击”还是“拖拽”

            slider.addEventListener('mousedown', (e) => {
                isDown = true;
                isDragging = false;
                slider.classList.add('active');
                startX = e.pageX - slider.offsetLeft;
                scrollLeft = slider.scrollLeft;
            });

            slider.addEventListener('mouseleave', () => {
                isDown = false;
                slider.classList.remove('active');
            });

            slider.addEventListener('mouseup', () => {
                isDown = false;
                slider.classList.remove('active');
                // 如果是拖拽结束，为了防止触发 click，我们在 click 事件中做判断
                setTimeout(() => { isDragging = false; }, 0);
            });

            slider.addEventListener('mousemove', (e) => {
                if (!isDown) return;
                e.preventDefault(); // 防止选中文字
                const x = e.pageX - slider.offsetLeft;
                const walk = (x - startX) * 2; // 滚动速度系数
                slider.scrollLeft = scrollLeft - walk;
                
                // 如果移动距离超过 5px，则视为拖拽，不是点击
                if (Math.abs(walk) > 5) {
                    isDragging = true;
                }
            });

            // 2. Tab 切换逻辑
            const tabs = document.querySelectorAll('.tab-btn');
            const panes = document.querySelectorAll('.tab-pane');

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    // 如果刚才是在拖拽，则拦截点击，不切换 Tab
                    if (isDragging) {
                        e.preventDefault();
                        e.stopPropagation();
                        return;
                    }

                    // 移除所有激活状态
                    tabs.forEach(t => t.classList.remove('active'));
                    panes.forEach(p => p.classList.remove('active'));

                    // 激活当前
                    tab.classList.add('active');
                    const targetId = tab.getAttribute('data-target');
                    document.getElementById(targetId).classList.add('active');
                    
                    // 滚动 Tab 到可见区域 (移动端/拖拽后体验优化)
                    tab.scrollIntoView({ behavior: 'smooth', block: 'nearest', inline: 'center' });
                });
            });

            // 3. 论文折叠/展开逻辑
            const paperHeaders = document.querySelectorAll('.paper-header');

            paperHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const item = this.parentElement;
                    const body = item.querySelector('.paper-body');
                    
                    // 切换状态
                    item.classList.toggle('expanded');
                    
                    if (item.classList.contains('expanded')) {
                        body.style.maxHeight = body.scrollHeight + "px";
                    } else {
                        body.style.maxHeight = null;
                    }
                });
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <!-- 导航 -->
        <div class="nav-bar">
            <a href="index.html" class="back-link">
                <svg class="icon" viewBox="0 0 24 24"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
                返回每日简报
            </a>
            <div class="date-badge">2025-11-09</div>
        </div>

        <!-- 头部 -->
        <div class="header">
            <h1>
                <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="10"></circle><polygon points="16.24 7.76 14.12 14.12 7.76 16.24 9.88 9.88 16.24 7.76"></polygon></svg>
                每日主流技术方向
            </h1>
            <div class="subtitle">聚合 RAG / LLM / Agent 等核心赛道，点击下方标签切换</div>
        </div>

        
        
        
        
        <!-- Tab 导航栏 (可拖拽) -->
        <div class="tabs-container">
            <div class="tabs-scroll">
                
                <button class="tab-btn active" data-target="tab-1">
                    Agent
                    <span class="tab-count">6</span>
                </button>
                
                <button class="tab-btn " data-target="tab-2">
                    Alignment
                    <span class="tab-count">11</span>
                </button>
                
                <button class="tab-btn " data-target="tab-3">
                    LLM
                    <span class="tab-count">47</span>
                </button>
                
                <button class="tab-btn " data-target="tab-4">
                    Multimodal
                    <span class="tab-count">28</span>
                </button>
                
                <button class="tab-btn " data-target="tab-5">
                    Optimization
                    <span class="tab-count">17</span>
                </button>
                
                <button class="tab-btn " data-target="tab-6">
                    RAG
                    <span class="tab-count">5</span>
                </button>
                
                <button class="tab-btn " data-target="tab-7">
                    RL
                    <span class="tab-count">111</span>
                </button>
                
                <button class="tab-btn " data-target="tab-8">
                    Vision
                    <span class="tab-count">22</span>
                </button>
                
            </div>
        </div>

        <!-- 内容区域 -->
        <div class="content-wrapper">
            
            <div id="tab-1" class="tab-pane active">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Agent
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 6 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前多智能体系统（MAS）研究主要集中在提升复杂规划和协作能力上，尤其是通过大语言模型（LLMs）来优化多智能体的决策过程。研究者们正努力解决在多智能体环境中探索与利用之间的平衡问题，同时关注如何通过自我抽象和经验学习来提升智能体的自我改进能力。此外，针对模型生成过程中的错误传播问题，研究者们也在开发新方法以提高智能体在执行任务时的鲁棒性。这些趋势表明，多智能体系统在教育、软件工程等领域具有广泛的应用潜力。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for
Trees (UCTs) to balance exploration and exploitation through randomized
sampling, is instrumental to solving complex planning problems. However, for
multi-agent planning, MCTS is confronted with a large combinatorial action
space that often grows exponentially with the number of agents. As a result,
the branching factor of MCTS during tree expansion also increases
exponentially, making it very difficult to efficiently explore and exploit
during tree search. To this end, we propose MALinZero, a new approach to
leverage low-dimensional representational structures on joint-action returns
and enable efficient MCTS in complex multi-agent planning. Our solution can be
viewed as projecting the joint-action returns into the low-dimensional space
representable using a contextual linear bandit problem formulation. We solve
the contextual linear bandit problem with convex and $\mu$-smooth loss
functions -- in order to place more importance on better joint actions and
mitigate potential representational limitations -- and derive a linear Upper
Confidence Bound applied to trees (LinUCT) to enable novel multi-agent
exploration and exploitation in the low-dimensional space. We analyze the
regret of MALinZero for low-dimensional reward functions and propose an
$(1-\tfrac1e)$-approximation algorithm for the joint action selection by
maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art
performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2,
outperforming both model-based and model-free multi-agent reinforcement
learning baselines with faster learning speed and better performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06142v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06142v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-agent systems (MAS) built on Large Language Models (LLMs) are being
used to approach complex problems and can surpass single model inference.
However, their success hinges on navigating a fundamental cognitive tension:
the need to balance broad, divergent exploration of the solution space with a
principled, convergent synthesis to the optimal solution. Existing paradigms
often struggle to manage this duality, leading to premature consensus, error
propagation, and a critical credit assignment problem that fails to distinguish
between genuine reasoning and superficially plausible arguments. To resolve
this core challenge, we propose the Multi-Agent Exploration-Synthesis framework
Through Role Orchestration (Maestro), a principled paradigm for collaboration
that structurally decouples these cognitive modes. Maestro uses a collective of
parallel Execution Agents for diverse exploration and a specialized Central
Agent for convergent, evaluative synthesis. To operationalize this critical
synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO),
a reinforcement learning objective that disentangles signals for strategic
decisions and tactical rationales. By combining decision-focused policy
gradients with a list-wise ranking loss over justifications, CLPO achieves
clean credit assignment and stronger comparative supervision. Experiments on
mathematical reasoning and general problem-solving benchmarks demonstrate that
Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art
multi-agent approaches, delivering absolute accuracy gains of 6% on average and
up to 10% at best.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06134v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06134v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Simulated Students offer a valuable methodological framework for evaluating
pedagogical approaches and modelling diverse learner profiles, tasks which are
otherwise challenging to undertake systematically in real-world settings.
Recent research has increasingly focused on developing such simulated agents to
capture a range of learning styles, cognitive development pathways, and social
behaviours. Among contemporary simulation techniques, the integration of large
language models (LLMs) into educational research has emerged as a particularly
versatile and scalable paradigm. LLMs afford a high degree of linguistic
realism and behavioural adaptability, enabling agents to approximate cognitive
processes and engage in contextually appropriate pedagogical dialogues. This
paper presents a thematic review of empirical and methodological studies
utilising LLMs to simulate student behaviour across educational environments.
We synthesise current evidence on the capacity of LLM-based agents to emulate
learner archetypes, respond to instructional inputs, and interact within
multi-agent classroom scenarios. Furthermore, we examine the implications of
such systems for curriculum development, instructional evaluation, and teacher
training. While LLMs surpass rule-based systems in natural language generation
and situational flexibility, ongoing concerns persist regarding algorithmic
bias, evaluation reliability, and alignment with educational objectives. The
review identifies existing technological and methodological gaps and proposes
future research directions for integrating generative AI into adaptive learning
systems and instructional design.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06078v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06078v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the proliferation of powerful agentic models, the lack of critical
post-training details hinders the development of strong counterparts in the
open-source community. In this study, we present a comprehensive and fully
open-source pipeline for training a high-performance agentic model for
interacting with external tools and environments, named Klear-Qwen3-AgentForge,
starting from the Qwen3-8B base model. We design effective supervised
fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement
learning (RL) to unlock the potential for multiple diverse agentic tasks. We
perform exclusive experiments on various agentic benchmarks in both tool use
and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art
performance among LLMs of similar size and remains competitive with
significantly larger models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05951v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05951v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language model (LLM) based agents are increasingly used to tackle
software engineering tasks that require multi-step reasoning and code
modification, demonstrating promising yet limited performance. However, most
existing LLM agents typically operate within static execution frameworks,
lacking a principled mechanism to learn and self-improve from their own
experience and past rollouts. As a result, their performance remains bounded by
the initial framework design and the underlying LLM's capabilities. We propose
Self-Abstraction from Grounded Experience (SAGE), a framework that enables
agents to learn from their own task executions and refine their behavior
through self-abstraction. After an initial rollout, the agent induces a concise
plan abstraction from its grounded experience, distilling key steps,
dependencies, and constraints. This learned abstraction is then fed back as
contextual guidance, refining the agent's policy and supporting more
structured, informed subsequent executions. Empirically, SAGE delivers
consistent performance gains across diverse LLM backbones and agent
architectures. Notably, it yields a 7.2% relative performance improvement over
the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone.
SAGE further achieves strong overall performance on SWE-Bench Verified
benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent
and OpenHands CodeAct agent framework, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05931v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05931v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Catching Contamination Before Generation: Spectral Kill Switches for Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Agentic language models compose multi step reasoning chains, yet intermediate
steps can be corrupted by inconsistent context, retrieval errors, or
adversarial inputs, which makes post hoc evaluation too late because errors
propagate before detection. We introduce a diagnostic that requires no
additional training and uses only the forward pass to emit a binary accept or
reject signal during agent execution. The method analyzes token graphs induced
by attention and computes two spectral statistics in early layers, namely the
high frequency energy ratio and spectral entropy. We formalize these signals,
establish invariances, and provide finite sample estimators with uncertainty
quantification. Under a two regime mixture assumption with a monotone
likelihood ratio property, we show that a single threshold on the high
frequency energy ratio is optimal in the Bayes sense for detecting context
inconsistency. Empirically, the high frequency energy ratio exhibits robust
bimodality during context verification across multiple model families, which
enables gating decisions with overhead below one millisecond on our hardware
and configurations. We demonstrate integration into retrieval augmented agent
pipelines and discuss deployment as an inline safety monitor. The approach
detects contamination while the model is still processing the text, before
errors commit to the reasoning chain.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05804v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05804v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-2" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Alignment
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 11 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在对齐技术（Alignment）领域的研究主要集中在以下几个趋势：一是对大型语言模型（LLM）中隐性偏见的评估，尤其是在复杂推理任务中，提出了新的评估框架以揭示社会刻板印象的影响；二是针对合成监督的应用，旨在提升网络代理在新环境中的适应能力，解决数据质量问题；三是关注人工智能在海湾合作委员会国家的劳动力适应性，强调技能和治理的建设；四是探讨在不确定性下的物联网新鲜农产品供应链优化；五是研究对LLM的对抗性攻击，评估其在事实记忆中的脆弱性；六是应用物理信息神经网络进行实时气体交叉预测，以支持绿色氢气生产。这些研究共同指向提升AI系统的安全性、可靠性和适应性，具有重要的社会和经济价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While recent safety guardrails effectively suppress overtly biased outputs,
subtler forms of social bias emerge during complex logical reasoning tasks that
evade current evaluation benchmarks. To fill this gap, we introduce a new
evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model
Evaluation), that uses logic grid puzzles to systematically probe the influence
of social stereotypes on logical reasoning and decision making in LLMs. Our use
of logic puzzles enables automatic generation and verification, as well as
variability in complexity and biased settings. PRIME includes stereotypical,
anti-stereotypical, and neutral puzzle variants generated from a shared puzzle
structure, allowing for controlled and fine-grained comparisons. We evaluate
multiple model families across puzzle sizes and test the effectiveness of
prompt-based mitigation strategies. Focusing our experiments on gender
stereotypes, our findings highlight that models consistently reason more
accurately when solutions align with stereotypical associations. This
demonstrates the significance of PRIME for diagnosing and quantifying social
biases perpetuated in the deductive reasoning of LLMs, where fairness is
critical.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06160v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06160v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapting Web Agents with Synthetic Supervision
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Web agents struggle to adapt to new websites due to the scarcity of
environment specific tasks and demonstrations. Recent works have explored
synthetic data generation to address this challenge, however, they suffer from
data quality issues where synthesized tasks contain hallucinations that cannot
be executed, and collected trajectories are noisy with redundant or misaligned
actions. In this paper, we propose SynthAgent, a fully synthetic supervision
framework that aims at improving synthetic data quality via dual refinement of
both tasks and trajectories. Our approach begins by synthesizing diverse tasks
through categorized exploration of web elements, ensuring efficient coverage of
the target environment. During trajectory collection, we refine tasks when
conflicts with actual observations are detected, mitigating hallucinations
while maintaining task consistency. After collection, we conduct trajectory
refinement with a global context to mitigate potential noise or misalignments.
Finally, we fine-tune open-source web agents on the refined synthetic data to
adapt them to the target environment. Experimental results demonstrate that
SynthAgent outperforms existing synthetic data methods, validating the
importance of high-quality synthetic supervision. The code will be publicly
available at https://github.com/aiming-lab/SynthAgent.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06101v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06101v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Artificial intelligence and the Gulf Cooperation Council workforce adapting to the future of work
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid expansion of artificial intelligence (AI) in the Gulf Cooperation
Council (GCC) raises a central question: are investments in compute
infrastructure matched by an equally robust build-out of skills, incentives,
and governance? Grounded in socio-technical systems (STS) theory, this
mixed-methods study audits workforce preparedness across Kingdom of Saudi
Arabia (KSA), the United Arab Emirates (UAE), Qatar, Kuwait, Bahrain, and Oman.
We combine term frequency--inverse document frequency (TF--IDF) analysis of six
national AI strategies (NASs), an inventory of 47 publicly disclosed AI
initiatives (January 2017--April 2025), paired case studies, the Mohamed bin
Zayed University of Artificial Intelligence (MBZUAI) and the Saudi Data &
Artificial Intelligence Authority (SDAIA) Academy, and a scenario matrix
linking oil-revenue slack (technical capacity) to regulatory coherence (social
alignment). Across the corpus, 34/47 initiatives (0.72; 95% Wilson CI
0.58--0.83) exhibit joint social--technical design; country-level indices span
0.57--0.90 (small n; intervals overlap). Scenario results suggest that, under
our modeled conditions, regulatory convergence plausibly binds outcomes more
than fiscal capacity: fragmented rules can offset high oil revenues, while
harmonized standards help preserve progress under austerity. We also identify
an emerging two-track talent system, research elites versus rapidly trained
practitioners, that risks labor-market bifurcation without bridging mechanisms.
By extending STS inquiry to oil-rich, state-led economies, the study refines
theory and sets a research agenda focused on longitudinal coupling metrics,
ethnographies of coordination, and outcome-based performance indicators.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05927v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05927v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        IoT-based Fresh Produce Supply Chain Under Uncertainty: An Adaptive Optimization Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Fruits and vegetables form a vital component of the global economy; however,
their distribution poses complex logistical challenges due to high
perishability, supply fluctuations, strict quality and safety standards, and
environmental sensitivity. In this paper, we propose an adaptive optimization
model that accounts for delays, travel time, and associated temperature changes
impacting produce shelf life, and compare it against traditional approaches
such as Robust Optimization, Distributionally Robust Optimization, and
Stochastic Programming. Additionally, we conduct a series of computational
experiments using Internet of Things (IoT) sensor data to evaluate the
performance of our proposed model. Our study demonstrates that the proposed
adaptive model achieves a higher shelf life, extending it by over 18\% compared
to traditional optimization models, by dynamically mitigating temperature
deviations through a temperature feedback mechanism. The promising results
demonstrate the potential of this approach to improve both the freshness and
efficiency of logistics systems an aspect often neglected in previous works.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05920v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05920v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    LLMs are now an integral part of information retrieval. As such, their role
as question answering chatbots raises significant concerns due to their shown
vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose
the first principled attack evaluation on LLM factual memory under prompt
injection via Xmera, our novel, theory-grounded MitM framework. By perturbing
the input given to "victim" LLMs in three closed-book and fact-based QA
settings, we undermine the correctness of the responses and assess the
uncertainty of their generation process. Surprisingly, trivial
instruction-based attacks report the highest success rate (up to ~85.3%) while
simultaneously having a high uncertainty for incorrectly answered questions. To
provide a simple defense mechanism against Xmera, we train Random Forest
classifiers on the response uncertainty levels to distinguish between attacked
and unattacked queries (average AUC of up to ~96%). We believe that signaling
users to be cautious about the answers they receive from black-box and
potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05919v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05919v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Green hydrogen production via polymer electrolyte membrane (PEM) water
electrolysis is pivotal for energy transition, yet hydrogen crossover through
membranes threatens safety and economic viability-approaching explosive limits
(4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current
physics-based models require extensive calibration and computational resources
that preclude real-time implementation, while purely data-driven approaches
fail to extrapolate beyond training conditions-critical for dynamic
electrolyzer operation. Here we present the first application of
physics-informed neural networks (PINNs) for hydrogen crossover prediction,
integrating mass conservation, Fick's diffusion law, and Henry's solubility law
within a compact architecture (17,793 parameters). Validated across six
membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar,
25-85{\deg}C), our PINN achieves exceptional accuracy (R$^2$ = 99.84%, RMSE =
0.0348%) with sub-millisecond inference times suitable for real-time control.
Remarkably, the model maintains R$^2$ > 86% when predicting crossover at
pressures 2.5x beyond training range-substantially outperforming pure neural
networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs
to edge devices (Raspberry Pi 4), enables distributed safety monitoring
essential for gigawatt-scale installations. By bridging physical rigor and
computational efficiency, this work establishes a new paradigm for real-time
electrolyzer monitoring, accelerating deployment of safe, efficient green
hydrogen infrastructure crucial for net-zero emissions targets.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05879v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05879v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Emotion recognition from EEG signals is essential for affective computing and
has been widely explored using deep learning. While recent deep learning
approaches have achieved strong performance on single EEG emotion datasets,
their generalization across datasets remains limited due to the heterogeneity
in annotation schemes and data formats. Existing models typically require
dataset-specific architectures tailored to input structure and lack semantic
alignment across diverse emotion labels. To address these challenges, we
propose EMOD: A Unified EEG Emotion Representation Framework Leveraging
Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and
emotion-aware representations from heterogeneous datasets by bridging both
semantic and structural gaps. Specifically, we project discrete and continuous
emotion labels into a unified V-A space and formulate a soft-weighted
supervised contrastive loss that encourages emotionally similar samples to
cluster in the latent space. To accommodate variable EEG formats, EMOD employs
a flexible backbone comprising a Triple-Domain Encoder followed by a
Spatial-Temporal Transformer, enabling robust extraction and integration of
temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG
datasets and evaluate its performance on three benchmark datasets. Experimental
results show that EMOD achieves state-of-the-art performance, demonstrating
strong adaptability and generalization across diverse EEG-based emotion
recognition scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05863v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05863v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Quantifying Edits Decay in Fine-tuned LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Knowledge editing has emerged as a lightweight alternative to retraining for
correcting or injecting specific facts in large language models (LLMs).
Meanwhile, fine-tuning remains the default operation for adapting LLMs to new
domains and tasks. Despite their widespread adoption, these two post-training
interventions have been studied in isolation, leaving open a crucial question:
if we fine-tune an edited model, do the edits survive? This question is
motivated by two practical scenarios: removing covert or malicious edits, and
preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1,
current KE methods become less useful, as every fine-tuned model would require
re-editing, which significantly increases the cost; if edits persist,
fine-tuned models risk propagating hidden malicious edits, raising serious
safety concerns. To this end, we systematically quantify edits decay after
fine-tuning, investigating how fine-tuning affects knowledge editing. We
evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three
fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three
datasets, yielding 232 experimental configurations. Our results show that edits
decay after fine-tuning, with survival varying across configurations, e.g.,
AlphaEdit edits decay more than MEMIT edits. Further, we propose
selective-layer fine-tuning and find that fine-tuning edited layers only can
effectively remove edits, though at a slight cost to downstream performance.
Surprisingly, fine-tuning non-edited layers impairs more edits than full
fine-tuning. Overall, our study establishes empirical baselines and actionable
strategies for integrating knowledge editing with fine-tuning, and underscores
that evaluating model editing requires considering the full LLM application
pipeline.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05852v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05852v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In multi-objective decision-making with hierarchical preferences,
lexicographic bandits provide a natural framework for optimizing multiple
objectives in a prioritized order. In this setting, a learner repeatedly
selects arms and observes reward vectors, aiming to maximize the reward for the
highest-priority objective, then the next, and so on. While previous studies
have primarily focused on regret minimization, this work bridges the gap
between \textit{regret minimization} and \textit{best arm identification} under
lexicographic preferences. We propose two elimination-based algorithms to
address this joint objective. The first algorithm eliminates suboptimal arms
sequentially, layer by layer, in accordance with the objective priorities, and
achieves sample complexity and regret bounds comparable to those of the best
single-objective algorithms. The second algorithm simultaneously leverages
reward information from all objectives in each round, effectively exploiting
cross-objective dependencies. Remarkably, it outperforms the known lower bound
for the single-objective bandit problem, highlighting the benefit of
cross-objective information sharing in the multi-objective setting. Empirical
results further validate their superior performance over baselines.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05802v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05802v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the sequential decision-making problem for automated weaning of
mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS
devices are percutaneous micro-axial flow pumps that provide left ventricular
unloading and forward blood flow, but current weaning strategies vary
significantly across care teams and lack data-driven approaches. Offline
reinforcement learning (RL) has proven to be successful in sequential
decision-making tasks, but our setting presents challenges for training and
evaluating traditional offline RL methods: prohibition of online patient
interaction, highly uncertain circulatory dynamics due to concurrent
treatments, and limited data availability. We developed an end-to-end machine
learning framework with two key contributions (1) Clinically-aware
OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized
offline RL algorithm for out-of-distribution suppression that also incorporates
clinically-informed reward shaping and (2) a Transformer-based probabilistic
digital twin that models MCS circulatory dynamics for policy evaluation with
rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves
theoretical performance guarantees under mild assumptions. CORMPO attains a
higher reward than the offline RL baselines by 28% and higher scores in
clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a
principled framework for safe offline policy learning in high-stakes medical
applications where domain expertise and safety constraints are essential.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06111v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06111v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Approximating Shapley Explanations in Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning has achieved remarkable success in complex
decision-making environments, yet its lack of transparency limits its
deployment in practice, especially in safety-critical settings. Shapley values
from cooperative game theory provide a principled framework for explaining
reinforcement learning; however, the computational cost of Shapley explanations
is an obstacle to their use. We introduce FastSVERL, a scalable method for
explaining reinforcement learning by approximating Shapley values. FastSVERL is
designed to handle the unique challenges of reinforcement learning, including
temporal dependencies across multi-step trajectories, learning from off-policy
data, and adapting to evolving agent behaviours in real time. FastSVERL
introduces a practical, scalable approach for principled and rigorous
interpretability in reinforcement learning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06094v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06094v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-3" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            LLM
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 47 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前大语言模型（LLM）领域的研究动态主要集中在转移学习、偏见评估、空间推理和多智能体协作等方面。研究者们探索如何利用LLM提升异构表格数据的迁移学习效果，同时关注LLM在复杂推理任务中潜在的隐性偏见，提出了新的评估框架以应对这一挑战。此外，研究还强调了LLM在多智能体系统中的协作能力以及在检索增强生成任务中的应用潜力，显示出LLM在处理复杂问题时的广泛适用性和价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Transfer learning of tabular data is non-trivial due to heterogeneity in the
feature space across disparate domains. The limited success of traditional deep
learning in tabular knowledge transfer can be advanced by leveraging large
language models (LLMs). However, the efficacy of LLMs often stagnates for mixed
data types structured in tables due to the limitations of text prompts and
in-context learning. We propose a lightweight transfer learning framework that
fine-tunes an LLM using source tabular data and transplants the LLM's selective
$key$ and $value$ projection weights into a gated feature tokenized transformer
(gFTT) built for tabular data. The gFTT model with cross-domain attention is
fine-tuned using target tabular data for transfer learning, eliminating the
need for shared features, LLM prompt engineering, and large-scale pretrained
models. Our experiments using ten pairs of source-target data sets and 12
baselines demonstrate the superiority of the proposed LLM-attention transplant
for transfer learning (LATTLE) method over traditional ML models,
state-of-the-art deep tabular architectures, and transfer learning models
trained on thousands to billions of tabular samples. The proposed attention
transfer demonstrates an effective solution to learning relationships between
data tables using an LLM in a low-resource learning environment. The source
code for the proposed method is publicly available.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06161v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06161v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While recent safety guardrails effectively suppress overtly biased outputs,
subtler forms of social bias emerge during complex logical reasoning tasks that
evade current evaluation benchmarks. To fill this gap, we introduce a new
evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model
Evaluation), that uses logic grid puzzles to systematically probe the influence
of social stereotypes on logical reasoning and decision making in LLMs. Our use
of logic puzzles enables automatic generation and verification, as well as
variability in complexity and biased settings. PRIME includes stereotypical,
anti-stereotypical, and neutral puzzle variants generated from a shared puzzle
structure, allowing for controlled and fine-grained comparisons. We evaluate
multiple model families across puzzle sizes and test the effectiveness of
prompt-based mitigation strategies. Focusing our experiments on gender
stereotypes, our findings highlight that models consistently reason more
accurately when solutions align with stereotypical associations. This
demonstrates the significance of PRIME for diagnosing and quantifying social
biases perpetuated in the deductive reasoning of LLMs, where fairness is
critical.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06160v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06160v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Large Language Models Develop Novel Social Biases Through Adaptive Exploration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As large language models (LLMs) are adopted into frameworks that grant them
the capacity to make real decisions, it is increasingly important to ensure
that they are unbiased. In this paper, we argue that the predominant approach
of simply removing existing biases from models is not enough. Using a paradigm
from the psychology literature, we demonstrate that LLMs can spontaneously
develop novel social biases about artificial demographic groups even when no
inherent differences exist. These biases result in highly stratified task
allocations, which are less fair than assignments by human participants and are
exacerbated by newer and larger models. In social science, emergent biases like
these have been shown to result from exploration-exploitation trade-offs, where
the decision-maker explores too little, allowing early observations to strongly
influence impressions about entire demographic groups. To alleviate this
effect, we examine a series of interventions targeting model inputs, problem
structure, and explicit steering. We find that explicitly incentivizing
exploration most robustly reduces stratification, highlighting the need for
better multifaceted objectives to mitigate bias. These results reveal that LLMs
are not merely passive mirrors of human social biases, but can actively create
new ones from experience, raising urgent questions about how these systems will
shape societies over time.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06148v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06148v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Spatial Reasoning is an important component of human cognition and is an area
in which the latest Vision-language models (VLMs) show signs of difficulty. The
current analysis works use image captioning tasks and visual question
answering. In this work, we propose using the Referring Expression
Comprehension task instead as a platform for the evaluation of spatial
reasoning by VLMs. This platform provides the opportunity for a deeper analysis
of spatial comprehension and grounding abilities when there is 1) ambiguity in
object detection, 2) complex spatial expressions with a longer sentence
structure and multiple spatial relations, and 3) expressions with negation
('not'). In our analysis, we use task-specific architectures as well as large
VLMs and highlight their strengths and weaknesses in dealing with these
specific situations. While all these models face challenges with the task at
hand, the relative behaviors depend on the underlying models and the specific
categories of spatial semantics (topological, directional, proximal, etc.). Our
results highlight these challenges and behaviors and provide insight into
research gaps and future directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06146v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06146v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-agent systems (MAS) built on Large Language Models (LLMs) are being
used to approach complex problems and can surpass single model inference.
However, their success hinges on navigating a fundamental cognitive tension:
the need to balance broad, divergent exploration of the solution space with a
principled, convergent synthesis to the optimal solution. Existing paradigms
often struggle to manage this duality, leading to premature consensus, error
propagation, and a critical credit assignment problem that fails to distinguish
between genuine reasoning and superficially plausible arguments. To resolve
this core challenge, we propose the Multi-Agent Exploration-Synthesis framework
Through Role Orchestration (Maestro), a principled paradigm for collaboration
that structurally decouples these cognitive modes. Maestro uses a collective of
parallel Execution Agents for diverse exploration and a specialized Central
Agent for convergent, evaluative synthesis. To operationalize this critical
synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO),
a reinforcement learning objective that disentangles signals for strategic
decisions and tactical rationales. By combining decision-focused policy
gradients with a list-wise ranking loss over justifications, CLPO achieves
clean credit assignment and stronger comparative supervision. Experiments on
mathematical reasoning and general problem-solving benchmarks demonstrate that
Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art
multi-agent approaches, delivering absolute accuracy gains of 6% on average and
up to 10% at best.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06134v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06134v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluation of retrieval-based QA on QUEST-LOFT
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the popularity of retrieval-augmented generation (RAG) as a solution
for grounded QA in both academia and industry, current RAG methods struggle
with questions where the necessary information is distributed across many
documents or where retrieval needs to be combined with complex reasoning.
Recently, the LOFT study has shown that this limitation also applies to
approaches based on long-context language models, with the QUEST benchmark
exhibiting particularly large headroom. In this paper, we provide an in-depth
analysis of the factors contributing to the poor performance on QUEST-LOFT,
publish updated numbers based on a thorough human evaluation, and demonstrate
that RAG can be optimized to significantly outperform long-context approaches
when combined with a structured output format containing reasoning and
evidence, optionally followed by answer re-verification.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06125v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06125v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapting Web Agents with Synthetic Supervision
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Web agents struggle to adapt to new websites due to the scarcity of
environment specific tasks and demonstrations. Recent works have explored
synthetic data generation to address this challenge, however, they suffer from
data quality issues where synthesized tasks contain hallucinations that cannot
be executed, and collected trajectories are noisy with redundant or misaligned
actions. In this paper, we propose SynthAgent, a fully synthetic supervision
framework that aims at improving synthetic data quality via dual refinement of
both tasks and trajectories. Our approach begins by synthesizing diverse tasks
through categorized exploration of web elements, ensuring efficient coverage of
the target environment. During trajectory collection, we refine tasks when
conflicts with actual observations are detected, mitigating hallucinations
while maintaining task consistency. After collection, we conduct trajectory
refinement with a global context to mitigate potential noise or misalignments.
Finally, we fine-tune open-source web agents on the refined synthetic data to
adapt them to the target environment. Experimental results demonstrate that
SynthAgent outperforms existing synthetic data methods, validating the
importance of high-quality synthetic supervision. The code will be publicly
available at https://github.com/aiming-lab/SynthAgent.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06101v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06101v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Simulated Students offer a valuable methodological framework for evaluating
pedagogical approaches and modelling diverse learner profiles, tasks which are
otherwise challenging to undertake systematically in real-world settings.
Recent research has increasingly focused on developing such simulated agents to
capture a range of learning styles, cognitive development pathways, and social
behaviours. Among contemporary simulation techniques, the integration of large
language models (LLMs) into educational research has emerged as a particularly
versatile and scalable paradigm. LLMs afford a high degree of linguistic
realism and behavioural adaptability, enabling agents to approximate cognitive
processes and engage in contextually appropriate pedagogical dialogues. This
paper presents a thematic review of empirical and methodological studies
utilising LLMs to simulate student behaviour across educational environments.
We synthesise current evidence on the capacity of LLM-based agents to emulate
learner archetypes, respond to instructional inputs, and interact within
multi-agent classroom scenarios. Furthermore, we examine the implications of
such systems for curriculum development, instructional evaluation, and teacher
training. While LLMs surpass rule-based systems in natural language generation
and situational flexibility, ongoing concerns persist regarding algorithmic
bias, evaluation reliability, and alignment with educational objectives. The
review identifies existing technological and methodological gaps and proposes
future research directions for integrating generative AI into adaptive learning
systems and instructional design.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06078v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06078v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Stemming Hallucination in Language Models Using a Licensing Oracle
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language models exhibit remarkable natural language generation capabilities
but remain prone to hallucinations, generating factually incorrect information
despite producing syntactically coherent responses. This study introduces the
Licensing Oracle, an architectural solution designed to stem hallucinations in
LMs by enforcing truth constraints through formal validation against structured
knowledge graphs. Unlike statistical approaches that rely on data scaling or
fine-tuning, the Licensing Oracle embeds a deterministic validation step into
the model's generative process, ensuring that only factually accurate claims
are made. We evaluated the effectiveness of the Licensing Oracle through
experiments comparing it with several state-of-the-art methods, including
baseline language model generation, fine-tuning for factual recall, fine-tuning
for abstention behavior, and retrieval-augmented generation (RAG). Our results
demonstrate that although RAG and fine-tuning improve performance, they fail to
eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect
abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring
that only valid claims were generated with 89.1% accuracy in factual responses.
This work shows that architectural innovations, such as the Licensing Oracle,
offer a necessary and sufficient solution for hallucinations in domains with
structured knowledge representations, offering guarantees that statistical
methods cannot match. Although the Licensing Oracle is specifically designed to
address hallucinations in fact-based domains, its framework lays the groundwork
for truth-constrained generation in future AI systems, providing a new path
toward reliable, epistemically grounded models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06073v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06073v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ScRPO: From Errors to Insights
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose Self-correction Relative Policy Optimization (ScRPO), a novel
reinforcement learning framework designed to enhance large language models on
challenging mathemati- cal problems by leveraging self-reflection and error
correction. Our approach consists of two stages: (1) Trial-and-error learning
stage: training the model with GRPO and collect- ing incorrect answers along
with their cor- responding questions in an error pool; (2) Self-correction
learning stage: guiding the model to reflect on why its previous an- swers were
wrong. Extensive experiments across multiple math reasoning benchmarks,
including AIME, AMC, Olympiad, MATH- 500, GSM8k, using Deepseek-Distill-Qwen-
1.5B and Deepseek-Distill-Qwen-7B. The ex- perimental results demonstrate that
ScRPO consistently outperforms several post-training methods. These findings
highlight ScRPO as a promising paradigm for enabling language models to
self-improve on difficult tasks with limited external feedback, paving the way
to- ward more reliable and capable AI systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06065v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06065v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The escalating context length in Large Language Models (LLMs) creates a
severe performance bottleneck around the Key-Value (KV) cache, whose
memory-bound nature leads to significant GPU under-utilization. This paper
introduces Mixture of Shared KV Attention (MoSKA), an architecture that
addresses this challenge by exploiting the heterogeneity of context data. It
differentiates between per-request unique and massively reused shared
sequences. The core of MoSKA is a novel Shared KV Attention mechanism that
transforms the attention on shared data from a series of memory-bound GEMV
operations into a single, compute-bound GEMM by batching concurrent requests.
This is supported by an MoE-inspired sparse attention strategy that prunes the
search space and a tailored Disaggregated Infrastructure that specializes
hardware for unique and shared data. This comprehensive approach demonstrates a
throughput increase of up to 538.7x over baselines in workloads with high
context sharing, offering a clear architectural path toward scalable LLM
inference.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06010v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06010v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Revisiting Entropy in Reinforcement Learning for Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
predominant approach for enhancing the reasoning capabilities of large language
models (LLMs). However, the entropy of LLMs usually collapses during RLVR
training, causing premature convergence to suboptimal local minima and hinder
further performance improvement. Although various approaches have been proposed
to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains
lacking. To address this gap, we conduct extensive experiments to investigate
the entropy dynamics of LLMs trained with RLVR and analyze how model entropy
correlates with response diversity, calibration, and performance across various
benchmarks. Our findings reveal that the number of off-policy updates, the
diversity of training data, and the clipping thresholds in the optimization
objective are critical factors influencing the entropy of LLMs trained with
RLVR. Moreover, we theoretically and empirically demonstrate that tokens with
positive advantages are the primary contributors to entropy collapse, and that
model entropy can be effectively regulated by adjusting the relative loss
weights of tokens with positive and negative advantages during training.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05993v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05993v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieval-Augmented Generation (RAG) systems combine Large Language Models
(LLMs) with external knowledge, and their performance depends heavily on how
that knowledge is represented. This study investigates how different Knowledge
Graph (KG) construction strategies influence RAG performance. We compare a
variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over
KGs built from ontologies derived either from relational databases or textual
corpora. Results show that ontology-guided KGs incorporating chunk information
achieve competitive performance with state-of-the-art frameworks, substantially
outperforming vector retrieval baselines. Moreover, the findings reveal that
ontology-guided KGs built from relational databases perform competitively to
ones built with ontologies extracted from text, with the benefit of offering a
dual advantage: they require a one-time-only ontology learning process,
substantially reducing LLM usage costs; and avoid the complexity of ontology
merging inherent to text-based approaches.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05991v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05991v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Anomaly troubleshooting for large model distributed inference (LMDI) remains
a critical challenge. Resolving anomalies such as inference performance
degradation or latency jitter in distributed system demands significant manual
efforts from domain experts, resulting in extremely time-consuming diagnosis
processes with relatively low accuracy. In this paper, we introduce Kunlun
Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework
tailored for LMDI. KAT addresses this problem through two core innovations.
First, KAT exploits the synchronicity and consistency of GPU workers,
innovatively leverages function trace data to precisely detect kernel-level
anomalies and associated hardware components at nanosecond resolution. Second,
KAT integrates these detection results into a domain-adapted LLM, delivering
systematic causal reasoning and natural language interpretation of complex
anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production
environment indicate that KAT achieves over 0.884 precision and 0.936 recall in
anomaly detection, providing detail anomaly insights that significantly narrow
down the diagnostic scope and improve both the efficiency and success rate of
troubleshooting.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05978v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05978v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Interpretable Recognition of Cognitive Distortions in Natural Language Texts
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose a new approach to multi-factor classification of natural language
texts based on weighted structured patterns such as N-grams, taking into
account the heterarchical relationships between them, applied to solve such a
socially impactful problem as the automation of detection of specific cognitive
distortions in psychological care, relying on an interpretable, robust and
transparent artificial intelligence model. The proposed recognition and
learning algorithms improve the current state of the art in this field. The
improvement is tested on two publicly available datasets, with significant
improvements over literature-known F1 scores for the task, with optimal
hyper-parameters determined, having code and models available for future use by
the community.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05969v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05969v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05968v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05968v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the proliferation of powerful agentic models, the lack of critical
post-training details hinders the development of strong counterparts in the
open-source community. In this study, we present a comprehensive and fully
open-source pipeline for training a high-performance agentic model for
interacting with external tools and environments, named Klear-Qwen3-AgentForge,
starting from the Qwen3-8B base model. We design effective supervised
fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement
learning (RL) to unlock the potential for multiple diverse agentic tasks. We
perform exclusive experiments on various agentic benchmarks in both tool use
and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art
performance among LLMs of similar size and remains competitive with
significantly larger models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05951v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05951v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        10 Open Challenges Steering the Future of Vision-Language-Action Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Due to their ability of follow natural language instructions,
vision-language-action (VLA) models are increasingly prevalent in the embodied
AI arena, following the widespread success of their precursors -- LLMs and
VLMs. In this paper, we discuss 10 principal milestones in the ongoing
development of VLA models -- multimodality, reasoning, data, evaluation,
cross-robot action generalization, efficiency, whole-body coordination, safety,
agents, and coordination with humans. Furthermore, we discuss the emerging
trends of using spatial understanding, modeling world dynamics, post training,
and data synthesis -- all aiming to reach these milestones. Through these
discussions, we hope to bring attention to the research avenues that may
accelerate the development of VLA models into wider acceptability.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05936v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05936v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning (RL) is often credited with improving language model
reasoning and generalization at the expense of degrading memorized knowledge.
We challenge this narrative by observing that RL-enhanced models consistently
outperform their base and supervised fine-tuned (SFT) counterparts on pure
knowledge recall tasks, particularly those requiring traversal of hierarchical,
structured knowledge (e.g., medical codes). We hypothesize these gains stem not
from newly acquired data, but from improved procedural skills in navigating and
searching existing knowledge hierarchies within the model parameters. To
support this hypothesis, we show that structured prompting, which explicitly
guides SFTed models through hierarchical traversal, recovers most of the
performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We
further find that while prompting improves final-answer accuracy, RL-enhanced
models retain superior ability to recall correct procedural paths on
deep-retrieval tasks. Finally our layer-wise internal activation analysis
reveals that while factual representations (e.g., activations for the statement
"code 57.95 refers to urinary infection") maintain high cosine similarity
between SFT and RL models, query representations (e.g., "what is code 57.95")
diverge noticeably, indicating that RL primarily transforms how models traverse
knowledge rather than the knowledge representation itself.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05933v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05933v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language model (LLM) based agents are increasingly used to tackle
software engineering tasks that require multi-step reasoning and code
modification, demonstrating promising yet limited performance. However, most
existing LLM agents typically operate within static execution frameworks,
lacking a principled mechanism to learn and self-improve from their own
experience and past rollouts. As a result, their performance remains bounded by
the initial framework design and the underlying LLM's capabilities. We propose
Self-Abstraction from Grounded Experience (SAGE), a framework that enables
agents to learn from their own task executions and refine their behavior
through self-abstraction. After an initial rollout, the agent induces a concise
plan abstraction from its grounded experience, distilling key steps,
dependencies, and constraints. This learned abstraction is then fed back as
contextual guidance, refining the agent's policy and supporting more
structured, informed subsequent executions. Empirically, SAGE delivers
consistent performance gains across diverse LLM backbones and agent
architectures. Notably, it yields a 7.2% relative performance improvement over
the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone.
SAGE further achieves strong overall performance on SWE-Bench Verified
benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent
and OpenHands CodeAct agent framework, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05931v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05931v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Voice-controlled dialog systems have become immensely popular due to their
ability to perform a wide range of actions in response to diverse user queries.
These agents possess a predefined set of skills or intents to fulfill specific
user tasks. But every system has its own limitations. There are instances
where, even for known intents, if any model exhibits low confidence, it results
in rejection of utterances that necessitate manual annotation. Additionally, as
time progresses, there may be a need to retrain these agents with new intents
from the system-rejected queries to carry out additional tasks. Labeling all
these emerging intents and rejected utterances over time is impractical, thus
calling for an efficient mechanism to reduce annotation costs. In this paper,
we introduce IDALC (Intent Detection and Active Learning based Correction), a
semi-supervised framework designed to detect user intents and rectify
system-rejected utterances while minimizing the need for human annotation.
Empirical findings on various benchmark datasets demonstrate that our system
surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8%
improvement in macro-F1. Remarkably, we maintain the overall annotation cost at
just 6-10% of the unlabelled data available to the system. The overall
framework of IDALC is shown in Fig. 1
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05921v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05921v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    LLMs are now an integral part of information retrieval. As such, their role
as question answering chatbots raises significant concerns due to their shown
vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose
the first principled attack evaluation on LLM factual memory under prompt
injection via Xmera, our novel, theory-grounded MitM framework. By perturbing
the input given to "victim" LLMs in three closed-book and fact-based QA
settings, we undermine the correctness of the responses and assess the
uncertainty of their generation process. Surprisingly, trivial
instruction-based attacks report the highest success rate (up to ~85.3%) while
simultaneously having a high uncertainty for incorrectly answered questions. To
provide a simple defense mechanism against Xmera, we train Random Forest
classifiers on the response uncertainty levels to distinguish between attacked
and unattacked queries (average AUC of up to ~96%). We believe that signaling
users to be cautious about the answers they receive from black-box and
potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05919v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05919v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        NILC: Discovering New Intents with LLM-assisted Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    New intent discovery (NID) seeks to recognize both new and known intents from
unlabeled user utterances, which finds prevalent use in practical dialogue
systems. Existing works towards NID mainly adopt a cascaded architecture,
wherein the first stage focuses on encoding the utterances into informative
text embeddings beforehand, while the latter is to group similar embeddings
into clusters (i.e., intents), typically by K-Means. However, such a cascaded
pipeline fails to leverage the feedback from both steps for mutual refinement,
and, meanwhile, the embedding-only clustering overlooks nuanced textual
semantics, leading to suboptimal performance. To bridge this gap, this paper
proposes NILC, a novel clustering framework specially catered for effective
NID. Particularly, NILC follows an iterative workflow, in which clustering
assignments are judiciously updated by carefully refining cluster centroids and
text embeddings of uncertain utterances with the aid of large language models
(LLMs). Specifically, NILC first taps into LLMs to create additional semantic
centroids for clusters, thereby enriching the contextual semantics of the
Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment
hard samples (ambiguous or terse utterances) identified from clusters via
rewriting for subsequent cluster correction. Further, we inject supervision
signals through non-trivial techniques seeding and soft must links for more
accurate NID in the semi-supervised setting. Extensive experiments comparing
NILC against multiple recent baselines under both unsupervised and
semi-supervised settings showcase that NILC can achieve significant performance
improvements over six benchmark datasets of diverse domains consistently.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05913v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05913v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    User simulation is important for developing and evaluating human-centered AI,
yet current student simulation in educational applications has significant
limitations. Existing approaches focus on single learning experiences and do
not account for students' gradual knowledge construction and evolving skill
sets. Moreover, large language models are optimized to produce direct and
accurate responses, making it challenging to represent the incomplete
understanding and developmental constraints that characterize real learners. In
this paper, we introduce a novel framework for memory-based student simulation
that incorporates developmental trajectories through a hierarchical memory
mechanism with structured knowledge representation. The framework also
integrates metacognitive processes and personality traits to enrich the
individual learner profiling, through dynamical consolidation of both cognitive
development and personal learning characteristics. In practice, we implement a
curriculum-aligned simulator grounded on the Next Generation Science Standards.
Experimental results show that our approach can effectively reflect the gradual
nature of knowledge development and the characteristic difficulties students
face, providing a more accurate representation of learning processes.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05903v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05903v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid growth of medical knowledge and increasing complexity of clinical
practice pose challenges. In this context, large language models (LLMs) have
demonstrated value; however, inherent limitations remain. Retrieval-augmented
generation (RAG) technologies show potential to enhance their clinical
applicability. This study reviewed RAG applications in medicine. We found that
research primarily relied on publicly available data, with limited application
in private data. For retrieval, approaches commonly relied on English-centric
embedding models, while LLMs were mostly generic, with limited use of
medical-specific LLMs. For evaluation, automated metrics evaluated generation
quality and task performance, whereas human evaluation focused on accuracy,
completeness, relevance, and fluency, with insufficient attention to bias and
safety. RAG applications were concentrated on question answering, report
generation, text summarization, and information extraction. Overall, medical
RAG remains at an early stage, requiring advances in clinical validation,
cross-linguistic adaptation, and support for low-resource settings to enable
trustworthy and responsible global use.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05901v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05901v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this paper, we proposed Speeder, a remarkably efficient paradigm to
multimodal large language models for sequential recommendation. Speeder
introduces 3 key components: (1) Multimodal Representation Compression (MRC),
which efficiently reduces redundancy in item descriptions; (2) Sequential
Position Awareness Enhancement (SPAE), which strengthens the model's ability to
capture complex sequential dependencies; (3) Modality-aware Progressive
Optimization (MPO), which progressively integrates different modalities to
improve the model's understanding and reduce cognitive biases. Through
extensive experiments, Speeder demonstrates superior performance over baselines
in terms of VHR@1 and computational efficiency. Specifically, Speeder achieved
250% of the training speed and 400% of the inference speed compared to the
state-of-the-art MLLM-based SR models. Future work could focus on incorporating
real-time feedback from real-world systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05885v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05885v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        An Empirical Study of Reasoning Steps in Thinking Code LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Thinking Large Language Models (LLMs) generate explicit intermediate
reasoning traces before final answers, potentially improving transparency,
interpretability, and solution accuracy for code generation. However, the
quality of these reasoning chains remains underexplored. We present a
comprehensive empirical study examining the reasoning process and quality of
thinking LLMs for code generation. We evaluate six state-of-the-art reasoning
LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking,
Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code
generation tasks of varying difficulty from BigCodeBench. We quantify
reasoning-chain structure through step counts and verbosity, conduct controlled
step-budget adjustments, and perform a 21-participant human evaluation across
three dimensions: efficiency, logical correctness, and completeness. Our
step-count interventions reveal that targeted step increases can improve
resolution rates for certain models/tasks, while modest reductions often
preserve success on standard tasks, rarely on hard ones. Through systematic
analysis, we develop a reasoning-problematic taxonomy, identifying completeness
as the dominant failure mode. Task complexity significantly impacts reasoning
quality; hard problems are substantially more prone to incompleteness than
standard tasks. Our stability analysis demonstrates that thinking LLMs maintain
consistent logical structures across computational effort levels and can
self-correct previous errors. This study provides new insights into the
strengths and limitations of current thinking LLMs in software engineering.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05874v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05874v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Hallucination in large language models (LLMs) remains a critical barrier to
their safe deployment. Existing tool-augmented hallucination detection methods
require pre-defined fixed verification strategies, which are crucial to the
quality and effectiveness of tool calls. Some methods directly employ powerful
closed-source LLMs such as GPT-4 as detectors, which are effective but too
costly. To mitigate the cost issue, some methods adopt the teacher-student
architecture and finetune open-source small models as detectors via agent
tuning. However, these methods are limited by fixed strategies. When faced with
a dynamically changing execution environment, they may lack adaptability and
inappropriately call tools, ultimately leading to detection failure. To address
the problem of insufficient strategy adaptability, we propose the innovative
``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an
efficient student model with the dynamic learning and proactive correction
capabilities of the teacher model. Specifically, our method formulates the
hallucination detection problem as a dynamic strategy learning problem. We
first employ a teacher model to generate trajectories within the dynamic
learning loop and dynamically adjust the strategy based on execution failures.
We then distill this dynamic planning capability into an efficient student
model via agent tuning. Finally, during strategy execution, the student model
adopts a proactive correction mechanism, enabling it to propose, review, and
optimize its own verification strategies before execution. We demonstrate
through experiments on three challenging benchmarks that our LEAP-tuned model
outperforms existing state-of-the-art methods.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05854v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05854v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Quantifying Edits Decay in Fine-tuned LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Knowledge editing has emerged as a lightweight alternative to retraining for
correcting or injecting specific facts in large language models (LLMs).
Meanwhile, fine-tuning remains the default operation for adapting LLMs to new
domains and tasks. Despite their widespread adoption, these two post-training
interventions have been studied in isolation, leaving open a crucial question:
if we fine-tune an edited model, do the edits survive? This question is
motivated by two practical scenarios: removing covert or malicious edits, and
preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1,
current KE methods become less useful, as every fine-tuned model would require
re-editing, which significantly increases the cost; if edits persist,
fine-tuned models risk propagating hidden malicious edits, raising serious
safety concerns. To this end, we systematically quantify edits decay after
fine-tuning, investigating how fine-tuning affects knowledge editing. We
evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three
fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three
datasets, yielding 232 experimental configurations. Our results show that edits
decay after fine-tuning, with survival varying across configurations, e.g.,
AlphaEdit edits decay more than MEMIT edits. Further, we propose
selective-layer fine-tuning and find that fine-tuning edited layers only can
effectively remove edits, though at a slight cost to downstream performance.
Surprisingly, fine-tuning non-edited layers impairs more edits than full
fine-tuning. Overall, our study establishes empirical baselines and actionable
strategies for integrating knowledge editing with fine-tuning, and underscores
that evaluating model editing requires considering the full LLM application
pipeline.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05852v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05852v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Retrieval Quality at Context Limit
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The ability of large language models (LLMs) to recall and retrieve
information from long contexts is critical for many real-world applications.
Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in
retrieval accuracy for facts placed in the middle of large contexts, an effect
known as "Lost in the Middle" (LITM). We find the model Gemini 2.5 Flash can
answer needle-in-a-haystack questions with great accuracy regardless of
document position including when the document is nearly at the input context
limit. Our results suggest that the "Lost in the Middle" effect is not present
for simple factoid Q\&A in Gemini 2.5 Flash, indicating substantial
improvements in long-context retrieval.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05850v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05850v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Symbolic regression seeks to uncover physical laws from experimental data by
searching for closed-form expressions, which is an important task in AI-driven
scientific discovery. Yet the exponential growth of the search space of
expression renders the task computationally challenging. A promising yet
underexplored direction for reducing the effective search space and
accelerating training lies in symbolic equivalence: many expressions, although
syntactically different, define the same function -- for example,
$\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$.
Existing algorithms treat such variants as distinct outputs, leading to
redundant exploration and slow learning. We introduce EGG-SR, a unified
framework that integrates equality graphs (e-graphs) into diverse symbolic
regression algorithms, including Monte Carlo Tree Search (MCTS), deep
reinforcement learning (DRL), and large language models (LLMs). EGG-SR
compactly represents equivalent expressions through the proposed EGG module,
enabling more efficient learning by: (1) pruning redundant subtree exploration
in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and
(3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that
embedding e-graphs tightens the regret bound of MCTS and reduces the variance
of the DRL gradient estimator. Empirically, EGG-SR consistently enhances
multiple baselines across challenging benchmarks, discovering equations with
lower normalized mean squared error than state-of-the-art methods. Code
implementation is available at: https://www.github.com/jiangnanhugo/egg-sr.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05849v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05849v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WAR-Re: Web API Recommendation with Semantic Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the development of cloud computing, the number of Web APIs has increased
dramatically, further intensifying the demand for efficient Web API
recommendation. Despite the demonstrated success of previous Web API
recommendation solutions, two critical challenges persist: 1) a fixed top-N
recommendation that cannot accommodate the varying API cardinality requirements
of different mashups, and 2) these methods output only ranked API lists without
accompanying reasons, depriving users of understanding the recommendation. To
address these challenges, we propose WAR-Re, an LLM-based model for Web API
recommendation with semantic reasoning for justification. WAR-Re leverages
special start and stop tokens to handle the first challenge and uses two-stage
training: supervised fine-tuning and reinforcement learning via Group Relative
Policy Optimization (GRPO) to enhance the model's ability in both tasks.
Comprehensive experimental evaluations on the ProgrammableWeb dataset
demonstrate that WAR-Re achieves a gain of up to 21.59\% over the
state-of-the-art baseline model in recommendation accuracy, while consistently
producing high-quality semantic reasons for recommendations.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05820v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05820v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Training large language models with FP8 formats offers significant efficiency
gains. However, the reduced numerical precision of FP8 poses challenges for
stable and accurate training. Current frameworks preserve training performance
using mixed-granularity quantization, i.e., applying per-group quantization for
activations and per-tensor/block quantization for weights. While effective,
per-group quantization requires scaling along the inner dimension of matrix
multiplication, introducing additional dequantization overhead. Moreover, these
frameworks often rely on just-in-time scaling to dynamically adjust scaling
factors based on the current data distribution. However, this online
quantization is inefficient for FP8 training, as it involves multiple memory
reads and writes that negate the performance benefits of FP8. To overcome these
limitations, we propose MOSS, a novel FP8 training framework that ensures both
efficiency and numerical stability. MOSS introduces two key innovations: (1) a
two-level microscaling strategy for quantizing sensitive activations, which
balances precision and dequantization cost by combining a high-precision global
scale with compact, power-of-two local scales; and (2) automatic scaling for
weights in linear layers, which eliminates the need for costly max-reduction
operations by predicting and adjusting scaling factors during training.
Leveraging these techniques, MOSS enables efficient FP8 training of a 7B
parameter model, achieving performance comparable to the BF16 baseline while
achieving up to 34% higher training throughput.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05811v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05811v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Building trustworthy clinical AI systems requires not only accurate
predictions but also transparent, biologically grounded explanations. We
present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian
deconvolution, eQTL-guided deep learning, and LLM-based narrative generation
for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian
Process-based hierarchical model that infers cell-type-specific gene expression
profiles from bulk and single-cell RNA-seq data while modeling biological
uncertainty. These features, combined with regulatory priors from eQTL
analysis, power a neural classifier that achieves high predictive performance
in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human
understanding and trust, we introduce an LLM-based reasoning module that
translates model outputs into audience-specific diagnostic reports, grounded in
clinical features, attribution signals, and domain knowledge. Human evaluations
confirm that these reports are accurate, actionable, and appropriately tailored
for both physicians and patients. Our findings show that LLMs, when deployed as
post-hoc reasoners rather than end-to-end predictors, can serve as effective
communicators within hybrid diagnostic pipelines.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05810v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05810v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Prompt injection attacks pose a critical threat to large language models
(LLMs), with prior work focusing on cutting-edge LLM applications like personal
copilots. In contrast, simpler LLM applications, such as customer service
chatbots, are widespread on the web, yet their security posture and exposure to
such attacks remain poorly understood. These applications often rely on
third-party chatbot plugins that act as intermediaries to commercial LLM APIs,
offering non-expert website builders intuitive ways to customize chatbot
behaviors. To bridge this gap, we present the first large-scale study of 17
third-party chatbot plugins used by over 10,000 public websites, uncovering
previously unknown prompt injection risks in practice. First, 8 of these
plugins (used by 8,000 websites) fail to enforce the integrity of the
conversation history transmitted in network requests between the website
visitor and the chatbot. This oversight amplifies the impact of direct prompt
injection attacks by allowing adversaries to forge conversation histories
(including fake system messages), boosting their ability to elicit unintended
behavior (e.g., code generation) by 3 to 8x. Second, 15 plugins offer tools,
such as web-scraping, to enrich the chatbot's context with website-specific
content. However, these tools do not distinguish the website's trusted content
(e.g., product descriptions) from untrusted, third-party content (e.g.,
customer reviews), introducing a risk of indirect prompt injection. Notably, we
found that ~13% of e-commerce websites have already exposed their chatbots to
third-party content. We systematically evaluate both vulnerabilities through
controlled experiments grounded in real-world observations, focusing on factors
such as system prompt design and the underlying LLM. Our findings show that
many plugins adopt insecure practices that undermine the built-in LLM
safeguards.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05797v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05797v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unlearning in Large Language Models (LLMs) is crucial for protecting private
data and removing harmful knowledge. Most existing approaches rely on
fine-tuning to balance unlearning efficiency with general language
capabilities. However, these methods typically require training or access to
retain data, which is often unavailable in real world scenarios. Although these
methods can perform well when both forget and retain data are available, few
works have demonstrated equivalent capability in more practical, data-limited
scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented
GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes
in-context chain-of-thought (CoT) instructions to guard deployed LLMs before
inference. Instead of modifying the base model, DRAGON leverages the inherent
instruction-following ability of LLMs and introduces a lightweight detection
module to identify forget-worthy prompts without any retain data. These are
then routed through a dedicated CoT guard model to enforce safe and accurate
in-context intervention. To robustly evaluate unlearning performance, we
introduce novel metrics for unlearning performance and the continual unlearning
setting. Extensive experiments across three representative unlearning tasks
validate the effectiveness of DRAGON, demonstrating its strong unlearning
capability, scalability, and applicability in practical scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05784v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05784v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MuonAll: Muon Variant for Efficient Finetuning of Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Muon optimizer has demonstrated robust results in pretraining of language
models but its performance in finetuning of existing public pretrained models
is not yet explored. Currently, Muon is used along with AdamW introducing a
scope of improvement for adopting all parameters inside Muon. We introduce
MuonAll, which incorporates all the parameters inside Muon by transforming into
2D matrices. We conduct extensive finetuning experiments across publicly
available language models with model sizes upto half billion parameters. Muon
and MuonAll perform at par with AdamW across major benchmarks, highlighting
their effectiveness as alternative optimizers. We open-source the distributed
implementations of Muon and MuonAll, available at
https://github.com/Saurabh750/optimizer
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06086v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06086v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Short-video recommenders such as Douyin must exploit extremely long user
histories without breaking latency or cost budgets. We present an end-to-end
system that scales long-sequence modeling to 10k-length histories in
production. First, we introduce Stacked Target-to-History Cross Attention
(STCA), which replaces history self-attention with stacked cross-attention from
the target to the history, reducing complexity from quadratic to linear in
sequence length and enabling efficient end-to-end training. Second, we propose
Request Level Batching (RLB), a user-centric batching scheme that aggregates
multiple targets for the same user/request to share the user-side encoding,
substantially lowering sequence-related storage, communication, and compute
without changing the learning objective. Third, we design a
length-extrapolative training strategy -- train on shorter windows, infer on
much longer ones -- so the model generalizes to 10k histories without
additional training cost. Across offline and online experiments, we observe
predictable, monotonic gains as we scale history length and model capacity,
mirroring the scaling law behavior observed in large language models. Deployed
at full traffic on Douyin, our system delivers significant improvements on key
engagement metrics while meeting production latency, demonstrating a practical
path to scaling end-to-end long-sequence recommendation to the 10k regime.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06077v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06077v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering
interpretable features in large language models (LLMs) through the sparse
directions they learn. However, the sheer number of extracted directions makes
comprehensive exploration intractable. While conventional embedding techniques
such as UMAP can reveal global structure, they suffer from limitations
including high-dimensional compression artifacts, overplotting, and misleading
neighborhood distortions. In this work, we propose a focused exploration
framework that prioritizes curated concepts and their corresponding SAE
features over attempts to visualize all available features simultaneously. We
present an interactive visualization system that combines topology-based visual
encoding with dimensionality reduction to faithfully represent both local and
global relationships among selected features. This hybrid approach enables
users to investigate SAE behavior through targeted, interpretable subsets,
facilitating deeper and more nuanced analysis of concept representation in
latent space.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06048v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06048v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Generative reasoning with large language models (LLMs) often involves long
decoding sequences, leading to substantial memory and latency overheads from
accumulating key-value (KV) caches. While existing KV compression methods
primarily focus on reducing prefill memory from long input sequences, they fall
short in addressing the dynamic and layer-sensitive nature of long-form
generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV
cache management framework that introduces adaptivity along both the spatial
and temporal dimensions of decoding. Along the spatial dimension, Lethe
performs layerwise sparsity-aware allocation, assigning token pruning budgets
to each transformer layer based on estimated attention redundancy. Along the
temporal dimension, Lethe conducts multi-round token pruning during generation,
driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends
traditional recency-based heuristics by also considering token relevance
derived from evolving attention patterns, enabling informed decisions about
which tokens to retain or evict. Empirical results demonstrate that Lethe
achieves a favorable balance between efficiency and generation quality across
diverse models and tasks, increases throughput by up to 2.56x.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06029v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06029v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Log-based anomaly detection is critical for ensuring the stability and
reliability of web systems. One of the key problems in this task is the lack of
sufficient labeled logs, which limits the rapid deployment in new systems.
Existing works usually leverage large-scale labeled logs from a mature web
system and a small amount of labeled logs from a new system, using transfer
learning to extract and generalize general knowledge across both domains.
However, these methods focus solely on the transfer of general knowledge and
neglect the disparity and potential mismatch between such knowledge and the
proprietary knowledge of target system, thus constraining performance. To
address this limitation, we propose FusionLog, a novel zero-label cross-system
log-based anomaly detection method that effectively achieves the fusion of
general and proprietary knowledge, enabling cross-system generalization without
any labeled target logs. Specifically, we first design a training-free router
based on semantic similarity that dynamically partitions unlabeled target logs
into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs
a small model based on system-agnostic representation meta-learning for direct
training and inference, inheriting the general anomaly patterns shared between
the source and target systems. For proprietary logs, we iteratively generate
pseudo-labels and fine-tune the small model using multi-round collaborative
knowledge distillation and fusion based on large language model (LLM) and small
model (SM) to enhance its capability to recognize anomaly patterns specific to
the target system. Experimental results on three public log datasets from
different systems show that FusionLog achieves over 90% F1-score under a fully
zero-label setting, significantly outperforming state-of-the-art cross-system
log-based anomaly detection methods.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05878v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05878v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Automating Hardware Design and Verification from Architectural Papers via a Neural-Symbolic Graph Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The reproduction of hardware architectures from academic papers remains a
significant challenge due to the lack of publicly available source code and the
complexity of hardware description languages (HDLs). To this end, we propose
\textbf{ArchCraft}, a Framework that converts abstract architectural
descriptions from academic papers into synthesizable Verilog projects with
register-transfer level (RTL) verification. ArchCraft introduces a structured
workflow, which uses formal graphs to capture the Architectural Blueprint and
symbols to define the Functional Specification, translating unstructured
academic papers into verifiable, hardware-aware designs. The framework then
generates RTL and testbench (TB) code decoupled via these symbols to facilitate
verification and debugging, ultimately reporting the circuit's Power, Area, and
Performance (PPA). Moreover, we propose the first benchmark,
\textbf{ArchSynthBench}, for synthesizing hardware from architectural
descriptions, with a complete set of evaluation indicators, 50 project-level
circuits, and around 600 circuit blocks. We systematically assess ArchCraft on
ArchSynthBench, where the experiment results demonstrate the superiority of our
proposed method, surpassing direct generation methods and the VerilogCoder
framework in both paper understanding and code completion. Furthermore,
evaluation and physical implementation of the generated executable RTL code
show that these implementations meet all timing constraints without violations,
and their performance metrics are consistent with those reported in the
original papers.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06067v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06067v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multimodal Stance Detection (MSD) is a crucial task for understanding public
opinion on social media. Existing work simply fuses information from various
modalities to learn stance representations, overlooking the varying
contributions of stance expression from different modalities. Therefore, stance
misunderstanding noises may be drawn into the stance learning process due to
the risk of learning errors by rough modality combination. To address this, we
get inspiration from the dual-process theory of human cognition and propose
**ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance
expression through a **D**ual-reasoning paradigm. ReMoD integrates
*experience-driven intuitive reasoning* to capture initial stance cues with
*deliberate reflective reasoning* to adjust for modality biases, refine stance
judgments, and thereby dynamically weight modality contributions based on their
actual expressive power for the target stance. Specifically, the intuitive
stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool
(SEP) to form an initial stance hypothesis, prioritizing historically impactful
modalities. This hypothesis is then refined in the reflective stage via two
reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to
amplify relevant modalities, while Semantic-CoT refines SEP with deeper
contextual insights of stance semantics. These dual experience structures are
continuously refined during training and recalled at inference to guide robust
and context-aware stance decisions. Extensive experiments on the public MMSD
benchmark demonstrate that our ReMoD significantly outperforms most baseline
models and exhibits strong generalization capabilities.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06057v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06057v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Efficient Hate Speech Detection: A Three-Layer LoRA-Tuned BERTweet Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper addresses the critical challenge of developing computationally
efficient hate speech detection systems that maintain competitive performance
while being practical for real-time deployment. We propose a novel three-layer
framework that combines rule-based pre-filtering with a parameter-efficient
LoRA-tuned BERTweet model and continuous learning capabilities. Our approach
achieves 0.85 macro F1 score - representing 94% of the performance of
state-of-the-art large language models like SafePhi (Phi-4 based) while using a
base model that is 100x smaller (134M vs 14B parameters). Compared to
traditional BERT-based approaches with similar computational requirements, our
method demonstrates superior performance through strategic dataset unification
and optimized fine-tuning. The system requires only 1.87M trainable parameters
(1.37% of full fine-tuning) and trains in approximately 2 hours on a single T4
GPU, making robust hate speech detection accessible in resource-constrained
environments while maintaining competitive accuracy for real-world deployment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06051v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06051v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) often exhibit implicit biases and discriminatory
tendencies that reflect underlying social stereotypes. While recent alignment
techniques such as RLHF and DPO have mitigated some of these issues, they
remain limited in addressing culturally specific and multi-dimensional forms of
discrimination. This paper proposes a Multi-Reward Group Relative Policy
Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free
behavior. Our approach constructs a synthetic English-language dataset derived
from Chinese-context discrimination categories, including regional, ethnic, and
occupational biases. Each instance is paired with both neutral and biased
responses to train a reward model based on DeBERTa-v3, which provides
multi-dimensional reward signals capturing fairness, neutrality, and linguistic
quality. The trained reward model then guides GRPO fine-tuning to optimize
model outputs along these ethical dimensions. Experimental results demonstrate
significant reductions in bias intensity and improved alignment with
non-discriminatory standards without compromising fluency or informativeness.
This study highlights the effectiveness of GRPO-based multi-reward optimization
for de-biasing LLMs and offers a replicable framework for cultural-contextual
ethical alignment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06023v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06023v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLMs Do Not See Age: Assessing Demographic Bias in Automated Systematic Review Synthesis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Clinical interventions often hinge on age: medications and procedures safe
for adults may be harmful to children or ineffective for older adults. However,
as language models are increasingly integrated into biomedical evidence
synthesis workflows, it remains uncertain whether these systems preserve such
crucial demographic distinctions. To address this gap, we evaluate how well
state-of-the-art language models retain age-related information when generating
abstractive summaries of biomedical studies. We construct DemogSummary, a novel
age-stratified dataset of systematic review primary studies, covering child,
adult, and older adult populations. We evaluate three prominent
summarisation-capable LLMs, Qwen (open-source), Longformer (open-source) and
GPT-4.1 Nano (proprietary), using both standard metrics and a newly proposed
Demographic Salience Score (DSS), which quantifies age-related entity retention
and hallucination. Our results reveal systematic disparities across models and
age groups: demographic fidelity is lowest for adult-focused summaries, and
under-represented populations are more prone to hallucinations. These findings
highlight the limitations of current LLMs in faithful and bias-free
summarisation and point to the need for fairness-aware evaluation frameworks
and summarisation pipelines in biomedical NLP.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06000v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06000v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) demonstrate strong capabilities in solving
complex tasks when integrated with external tools. The Model Context Protocol
(MCP) has become a standard interface for enabling such tool-based
interactions. However, these interactions introduce substantial security
concerns, particularly when the MCP server is compromised or untrustworthy.
While prior benchmarks primarily focus on prompt injection attacks or analyze
the vulnerabilities of LLM MCP interaction trajectories, limited attention has
been given to the underlying system logs associated with malicious MCP servers.
To address this gap, we present the first synthetic benchmark for evaluating
LLMs ability to identify security risks from system logs. We define nine
categories of MCP server risks and generate 1,800 synthetic system logs using
ten state-of-the-art LLMs. These logs are embedded in the return values of 243
curated MCP servers, yielding a dataset of 2,421 chat histories for training
and 471 queries for evaluation. Our pilot experiments reveal that smaller
models often fail to detect risky system logs, leading to high false negatives.
While models trained with supervised fine-tuning (SFT) tend to over-flag benign
logs, resulting in elevated false positives, Reinforcement Learning from
Verifiable Reward (RLVR) offers a better precision-recall balance. In
particular, after training with Group Relative Policy Optimization (GRPO),
Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing
large remote model by 9 percentage points. Fine-grained, per-category analysis
further underscores the effectiveness of reinforcement learning in enhancing
LLM safety within the MCP framework. Code and data are available at:
https://github.com/PorUna-byte/MCP-Guard/tree/master
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05867v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05867v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-4" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Multimodal
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 28 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前多模态技术研究主要集中在空间推理、图像和视频处理以及知识迁移等方面。研究者们通过引入新的评估平台和深度学习框架，提升了视觉-语言模型在空间理解、运动模糊文本恢复和视频帧插值中的表现。此外，针对边缘计算和人脸重识别的需求，提出了一种高效的知识转移方法，显示出在不同资源条件下的可扩展性。这些研究不仅推动了多模态技术的发展，也为实际应用提供了更强的支持。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Spatial Reasoning is an important component of human cognition and is an area
in which the latest Vision-language models (VLMs) show signs of difficulty. The
current analysis works use image captioning tasks and visual question
answering. In this work, we propose using the Referring Expression
Comprehension task instead as a platform for the evaluation of spatial
reasoning by VLMs. This platform provides the opportunity for a deeper analysis
of spatial comprehension and grounding abilities when there is 1) ambiguity in
object detection, 2) complex spatial expressions with a longer sentence
structure and multiple spatial relations, and 3) expressions with negation
('not'). In our analysis, we use task-specific architectures as well as large
VLMs and highlight their strengths and weaknesses in dealing with these
specific situations. While all these models face challenges with the task at
hand, the relative behaviors depend on the underlying models and the specific
categories of spatial semantics (topological, directional, proximal, etc.). Our
results highlight these challenges and behaviors and provide insight into
research gaps and future directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06146v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06146v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06087v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06087v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S2ML: Spatio-Spectral Mutual Learning for Depth Completion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or
structured light often suffer from incomplete depth values due to weak
reflections, boundary shadows, and artifacts, which limit their applications in
downstream vision tasks. Existing methods address this problem through depth
completion in the image domain, but they overlook the physical characteristics
of raw depth images. It has been observed that the presence of invalid depth
areas alters the frequency distribution pattern. In this work, we propose a
Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of
both spatial and frequency domains for depth completion. Specifically, we
consider the distinct properties of amplitude and phase spectra and devise a
dedicated spectral fusion module. Meanwhile, the local and global correlations
between spatial-domain and frequency-domain features are calculated in a
unified embedding space. The gradual mutual representation and refinement
encourage the network to fully explore complementary physical characteristics
and priors for more accurate depth completion. Extensive experiments
demonstrate the effectiveness of our proposed S2ML method, outperforming the
state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2
and SUN RGB-D datasets, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06033v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06033v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Video Frame Interpolation (VFI) remains a cornerstone in video enhancement,
enabling temporal upscaling for tasks like slow-motion rendering, frame rate
conversion, and video restoration. While classical methods rely on optical flow
and learning-based models assume access to dense ground-truth, both struggle
with occlusions, domain shifts, and ambiguous motion. This article introduces
MiVID, a lightweight, self-supervised, diffusion-based framework for video
interpolation. Our model eliminates the need for explicit motion estimation by
combining a 3D U-Net backbone with transformer-style temporal attention,
trained under a hybrid masking regime that simulates occlusions and motion
uncertainty. The use of cosine-based progressive masking and adaptive loss
scheduling allows our network to learn robust spatiotemporal representations
without any high-frame-rate supervision. Our framework is evaluated on UCF101-7
and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and
9-frame video segments, making it a low-resource yet highly effective pipeline.
Despite these constraints, our model achieves optimal results at just 50
epochs, competitive with several supervised baselines.This work demonstrates
the power of self-supervised diffusion priors for temporally coherent frame
synthesis and provides a scalable path toward accessible and generalizable VFI
systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06019v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06019v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        One-Shot Knowledge Transfer for Scalable Person Re-Identification
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06016v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06016v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05996v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05996v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep neural networks (DNNs) are widely used in perception systems for
safety-critical applications, such as autonomous driving and robotics. However,
DNNs remain vulnerable to various safety concerns, including generalization
errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can
lead to hazardous failures. This survey provides a comprehensive overview of
runtime safety monitoring approaches, which operate in parallel to DNNs during
inference to detect these safety concerns without modifying the DNN itself. We
categorize existing methods into three main groups: Monitoring inputs, internal
representations, and outputs. We analyze the state-of-the-art for each
category, identify strengths and limitations, and map methods to the safety
concerns they address. In addition, we highlight open challenges and future
research directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05982v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05982v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05968v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05968v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05967v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05967v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Typical detection-free methods for image-to-point cloud registration leverage
transformer-based architectures to aggregate cross-modal features and establish
correspondences. However, they often struggle under challenging conditions,
where noise disrupts similarity computation and leads to incorrect
correspondences. Moreover, without dedicated designs, it remains difficult to
effectively select informative and correlated representations across
modalities, thereby limiting the robustness and accuracy of registration. To
address these challenges, we propose a novel cross-modal registration framework
composed of two key modules: the Iterative Agents Selection (IAS) module and
the Reliable Agents Interaction (RAI) module. IAS enhances structural feature
awareness with phase maps and employs reinforcement learning principles to
efficiently select reliable agents. RAI then leverages these selected agents to
guide cross-modal interactions, effectively reducing mismatches and improving
overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes
benchmarks demonstrate that our method consistently achieves state-of-the-art
performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05965v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05965v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        10 Open Challenges Steering the Future of Vision-Language-Action Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Due to their ability of follow natural language instructions,
vision-language-action (VLA) models are increasingly prevalent in the embodied
AI arena, following the widespread success of their precursors -- LLMs and
VLMs. In this paper, we discuss 10 principal milestones in the ongoing
development of VLA models -- multimodality, reasoning, data, evaluation,
cross-robot action generalization, efficiency, whole-body coordination, safety,
agents, and coordination with humans. Furthermore, we discuss the emerging
trends of using spatial understanding, modeling world dynamics, post training,
and data synthesis -- all aiming to reach these milestones. Through these
discussions, we hope to bring attention to the research avenues that may
accelerate the development of VLA models into wider acceptability.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05936v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05936v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05929v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05929v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the effectiveness of quantization-aware training (QAT) in compressing
deep neural networks, its performance on multi-task architectures often
degrades significantly due to task-specific feature discrepancies and gradient
conflicts. To address these challenges, we propose Gradient-Aware Balanced
Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and
fuses task-specific features in a quantization-friendly manner. We further
introduce Attention Distribution Alignment (ADA), a feature-level distillation
strategy tailored for quantized models. Our method demonstrates strong
generalization across network architectures and QAT algorithms, with
theoretical guarantees on gradient bias reduction. Extensive experiments
demonstrate that our strategy consistently enhances a variety of QAT methods
across different network architectures and bit-widths. On PASCAL VOC and COCO
datasets, the proposed approach achieves average mAP improvements of
approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit
quantization, our method narrows the accuracy gap with the full-precision model
to only 1.7% on VOC, showcasing its effectiveness in preserving performance
under low-bit constraints. Notably, the proposed framework is modular, easy to
integrate, and compatible with any existing QAT technique-enhancing the
performance of quantized models without requiring modifications to the original
network architecture.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05898v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05898v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this paper, we proposed Speeder, a remarkably efficient paradigm to
multimodal large language models for sequential recommendation. Speeder
introduces 3 key components: (1) Multimodal Representation Compression (MRC),
which efficiently reduces redundancy in item descriptions; (2) Sequential
Position Awareness Enhancement (SPAE), which strengthens the model's ability to
capture complex sequential dependencies; (3) Modality-aware Progressive
Optimization (MPO), which progressively integrates different modalities to
improve the model's understanding and reduce cognitive biases. Through
extensive experiments, Speeder demonstrates superior performance over baselines
in terms of VHR@1 and computational efficiency. Specifically, Speeder achieved
250% of the training speed and 400% of the inference speed compared to the
state-of-the-art MLLM-based SR models. Future work could focus on incorporating
real-time feedback from real-world systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05885v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05885v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Numerous multimodal misinformation benchmarks exhibit bias toward specific
modalities, allowing detectors to make predictions based solely on one
modality. While previous research has quantified bias at the dataset level or
manually identified spurious correlations between modalities and labels, these
approaches lack meaningful insights at the sample level and struggle to scale
to the vast amount of online information. In this paper, we investigate the
design for automated recognition of modality bias at the sample level.
Specifically, we propose three bias quantification methods based on
theories/views of different levels of granularity: 1) a coarse-grained
evaluation of modality benefit; 2) a medium-grained quantification of
information flow; and 3) a fine-grained causality analysis. To verify the
effectiveness, we conduct a human evaluation on two popular benchmarks.
Experimental results reveal three interesting findings that provide potential
direction toward future research: 1)~Ensembling multiple views is crucial for
reliable automated analysis; 2)~Automated analysis is prone to detector-induced
fluctuations; and 3)~Different views produce a higher agreement on
modality-balanced samples but diverge on biased ones.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05883v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05883v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Social platforms connect billions of people, yet their engagement-first
algorithms often work on users rather than with them, amplifying stress,
misinformation, and a loss of control. We propose Human-Layer AI
(HL-AI)--user-owned, explainable intermediaries that sit in the browser between
platform logic and the interface. HL-AI gives people practical,
moment-to-moment control without requiring platform cooperation. We contribute
a working Chrome/Edge prototype implementing five representative pattern
frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed
Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying
mathematical formulation balancing user utility, autonomy costs, and risk
thresholds. Evaluation spans technical accuracy, usability, and behavioral
outcomes. The result is a suite of humane controls that help users rewrite
before harm, read with integrity cues, tune feeds with intention, pause
compulsive loops, and seek shelter during harassment, all while preserving
agency through explanations and override options. This prototype offers a
practical path to retrofit today's feeds with safety, agency, and well-being,
inviting rigorous cross-cultural user evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05875v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05875v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Endoscopic images often suffer from diverse and co-occurring degradations
such as low lighting, smoke, and bleeding, which obscure critical clinical
details. Existing restoration methods are typically task-specific and often
require prior knowledge of the degradation type, limiting their robustness in
real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic
diffusion-based framework that restores multiple degradation types using a
single model. EndoIR introduces a Dual-Domain Prompter that extracts joint
spatial-frequency features, coupled with an adaptive embedding that encodes
both shared and task-specific cues as conditioning for denoising. To mitigate
feature confusion in conventional concatenation-based conditioning, we design a
Dual-Stream Diffusion architecture that processes clean and degraded inputs
separately, with a Rectified Fusion Block integrating them in a structured,
degradation-aware manner. Furthermore, Noise-Aware Routing Block improves
efficiency by dynamically selecting only noise-relevant features during
denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR
achieves state-of-the-art performance across multiple degradation scenarios
while using fewer parameters than strong baselines, and downstream segmentation
experiments confirm its clinical utility.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05873v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05873v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CGCE: Classifier-Guided Concept Erasure in Generative Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05865v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05865v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Diffusion Model Guidance through Calibration and Regularization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05844v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05844v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05841v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05841v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hilbert-Guided Block-Sparse Local Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05832v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05832v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Robotic grasping is a fundamental capability for autonomous manipulation;
however, most existing methods rely on large-scale expert annotations and
necessitate retraining to handle new objects. We present VLAD-Grasp, a
Vision-Language model Assisted zero-shot approach for Detecting grasps. From a
single RGB-D image, our method (1) prompts a large vision-language model to
generate a goal image where a straight rod "impales" the object, representing
an antipodal grasp, (2) predicts depth and segmentation to lift this generated
image into 3D, and (3) aligns generated and observed object point clouds via
principal component analysis and correspondence-free optimization to recover an
executable grasp pose. Unlike prior work, our approach is training-free and
does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves
performance that is competitive with or superior to that of state-of-the-art
supervised models on the Cornell and Jacquard datasets. We further demonstrate
zero-shot generalization to novel real-world objects on a Franka Research 3
robot, highlighting vision-language foundation models as powerful priors for
robotic manipulation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05791v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05791v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sign language recognition from skeletal data using graph and recurrent neural networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This work presents an approach for recognizing isolated sign language
gestures using skeleton-based pose data extracted from video sequences. A
Graph-GRU temporal network is proposed to model both spatial and temporal
dependencies between frames, enabling accurate classification. The model is
trained and evaluated on the AUTSL (Ankara university Turkish sign language)
dataset, achieving high accuracy. Experimental results demonstrate the
effectiveness of integrating graph-based spatial representations with temporal
modeling, providing a scalable framework for sign language recognition. The
results of this approach highlight the potential of pose-driven methods for
sign language understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05772v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05772v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in
children plays a crucial role in improving outcomes in education and mental
health. Diagnosing ADHD using neuroimaging data, however, remains challenging
due to heterogeneous presentations and overlapping symptoms with other
conditions. To address this, we propose a novel parameter-efficient transfer
learning approach that adapts a large-scale 3D convolutional foundation model,
pre-trained on CT images, to an MRI-based ADHD classification task. Our method
introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional
kernels into 2D low-rank updates, dramatically reducing trainable parameters
while achieving superior performance. In a five-fold cross-validated evaluation
on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved
state-of-the-art results, with one model variant reaching 71.9% accuracy and
another attaining an AUC of 0.716. Both variants use only 1.64 million
trainable parameters (over 113x fewer than a fully fine-tuned foundation
model). Our results represent one of the first successful cross-modal
(CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a
new benchmark for ADHD classification while greatly improving efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06163v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06163v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CSGaze: Context-aware Social Gaze Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05955v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05955v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In recent years, the advancement of Graph Neural Networks (GNNs) has
significantly propelled progress in Multi-View Clustering (MVC). However,
existing methods face the problem of coarse-grained graph fusion. Specifically,
current approaches typically generate a separate graph structure for each view
and then perform weighted fusion of graph structures at the view level, which
is a relatively rough strategy. To address this limitation, we present a novel
Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly
consists of two modules. In particular, we propose an innovative Mixture of
Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a
Mixture-of-Experts network to implement fine-grained fusion of ego graphs at
the sample level, rather than the conventional view-level fusion. Additionally,
we present the Ego Graph Contrastive Learning (EGCL) module to align the fused
representation with the view-specific representation. The EGCL module enhances
the representation similarity of samples from the same cluster, not merely from
the same sample, further boosting fine-grained graph representation. Extensive
experiments demonstrate that MoEGCL achieves state-of-the-art results in deep
multi-view clustering tasks. The source code is publicly available at
https://github.com/HackerHyper/MoEGCL.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05876v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05876v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ambient sensor-based human activity recognition (HAR) in smart homes remains
challenging due to the need for real-time inference, spatially grounded
reasoning, and context-aware temporal modeling. Existing approaches often rely
on pre-segmented, within-activity data and overlook the physical layout of the
environment, limiting their robustness in continuous, real-world deployments.
In this paper, we propose MARAuder's Map, a novel framework for real-time
activity recognition from raw, unsegmented sensor streams. Our method projects
sensor activations onto the physical floorplan to generate trajectory-aware,
image-like sequences that capture the spatial flow of human movement. These
representations are processed by a hybrid deep learning model that jointly
captures spatial structure and temporal dependencies. To enhance temporal
awareness, we introduce a learnable time embedding module that encodes
contextual cues such as hour-of-day and day-of-week. Additionally, an
attention-based encoder selectively focuses on informative segments within each
observation window, enabling accurate recognition even under cross-activity
transitions and temporal ambiguity. Extensive experiments on multiple
real-world smart home datasets demonstrate that our method outperforms strong
baselines, offering a practical solution for real-time HAR in ambient sensor
environments.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05773v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05773v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ReMoD: Rethinking Modality Contribution in Multimodal Stance Detection via Dual Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multimodal Stance Detection (MSD) is a crucial task for understanding public
opinion on social media. Existing work simply fuses information from various
modalities to learn stance representations, overlooking the varying
contributions of stance expression from different modalities. Therefore, stance
misunderstanding noises may be drawn into the stance learning process due to
the risk of learning errors by rough modality combination. To address this, we
get inspiration from the dual-process theory of human cognition and propose
**ReMoD**, a framework that **Re**thinks **Mo**dality contribution of stance
expression through a **D**ual-reasoning paradigm. ReMoD integrates
*experience-driven intuitive reasoning* to capture initial stance cues with
*deliberate reflective reasoning* to adjust for modality biases, refine stance
judgments, and thereby dynamically weight modality contributions based on their
actual expressive power for the target stance. Specifically, the intuitive
stage queries the Modality Experience Pool (MEP) and Semantic Experience Pool
(SEP) to form an initial stance hypothesis, prioritizing historically impactful
modalities. This hypothesis is then refined in the reflective stage via two
reasoning chains: Modality-CoT updates MEP with adaptive fusion strategies to
amplify relevant modalities, while Semantic-CoT refines SEP with deeper
contextual insights of stance semantics. These dual experience structures are
continuously refined during training and recalled at inference to guide robust
and context-aware stance decisions. Extensive experiments on the public MMSD
benchmark demonstrate that our ReMoD significantly outperforms most baseline
models and exhibits strong generalization capabilities.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06057v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06057v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-5" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Optimization
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 17 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前优化技术领域的研究动态主要集中在多智能体规划、软件性能优化、强化学习以及生成模型等方面。研究者们普遍关注如何在复杂环境中提高算法的效率与准确性，尤其是在面对大规模数据和不确定性时的表现。此外，结合自我纠错和强化学习的策略也逐渐成为提升模型性能的重要趋势，显示出在实际应用中优化算法的潜在价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for
Trees (UCTs) to balance exploration and exploitation through randomized
sampling, is instrumental to solving complex planning problems. However, for
multi-agent planning, MCTS is confronted with a large combinatorial action
space that often grows exponentially with the number of agents. As a result,
the branching factor of MCTS during tree expansion also increases
exponentially, making it very difficult to efficiently explore and exploit
during tree search. To this end, we propose MALinZero, a new approach to
leverage low-dimensional representational structures on joint-action returns
and enable efficient MCTS in complex multi-agent planning. Our solution can be
viewed as projecting the joint-action returns into the low-dimensional space
representable using a contextual linear bandit problem formulation. We solve
the contextual linear bandit problem with convex and $\mu$-smooth loss
functions -- in order to place more importance on better joint actions and
mitigate potential representational limitations -- and derive a linear Upper
Confidence Bound applied to trees (LinUCT) to enable novel multi-agent
exploration and exploitation in the low-dimensional space. We analyze the
regret of MALinZero for low-dimensional reward functions and propose an
$(1-\tfrac1e)$-approximation algorithm for the joint action selection by
maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art
performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2,
outperforming both model-based and model-free multi-agent reinforcement
learning baselines with faster learning speed and better performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06142v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06142v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Optimizing the performance of large-scale software repositories demands
expertise in code reasoning and software engineering (SWE) to reduce runtime
while preserving program correctness. However, most benchmarks emphasize what
to fix rather than how to fix code. We introduce \textsc{SWE-fficiency}, a
benchmark for evaluating repository-level performance optimization on real
workloads. Our suite contains 498 tasks across nine widely used data-science,
machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a
complete codebase and a slow workload, an agent must investigate code
semantics, localize bottlenecks and relevant tests, and produce a patch that
matches or exceeds expert speedup while passing the same unit tests. To enable
this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests
for performance-improving edits, combining keyword filtering, static analysis,
coverage tooling, and execution validation to both confirm expert speedup
baselines and identify relevant repository unit tests. Empirical evaluation of
state-of-the-art agents reveals significant underperformance. On average,
agents achieve less than 0.15x the expert speedup: agents struggle in
localizing optimization opportunities, reasoning about execution across
functions, and maintaining correctness in proposed edits. We release the
benchmark and accompanying data pipeline to facilitate research on automated
performance engineering and long-horizon software reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06090v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06090v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ScRPO: From Errors to Insights
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose Self-correction Relative Policy Optimization (ScRPO), a novel
reinforcement learning framework designed to enhance large language models on
challenging mathemati- cal problems by leveraging self-reflection and error
correction. Our approach consists of two stages: (1) Trial-and-error learning
stage: training the model with GRPO and collect- ing incorrect answers along
with their cor- responding questions in an error pool; (2) Self-correction
learning stage: guiding the model to reflect on why its previous an- swers were
wrong. Extensive experiments across multiple math reasoning benchmarks,
including AIME, AMC, Olympiad, MATH- 500, GSM8k, using Deepseek-Distill-Qwen-
1.5B and Deepseek-Distill-Qwen-7B. The ex- perimental results demonstrate that
ScRPO consistently outperforms several post-training methods. These findings
highlight ScRPO as a promising paradigm for enabling language models to
self-improve on difficult tasks with limited external feedback, paving the way
to- ward more reliable and capable AI systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06065v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06065v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Revisiting Entropy in Reinforcement Learning for Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
predominant approach for enhancing the reasoning capabilities of large language
models (LLMs). However, the entropy of LLMs usually collapses during RLVR
training, causing premature convergence to suboptimal local minima and hinder
further performance improvement. Although various approaches have been proposed
to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains
lacking. To address this gap, we conduct extensive experiments to investigate
the entropy dynamics of LLMs trained with RLVR and analyze how model entropy
correlates with response diversity, calibration, and performance across various
benchmarks. Our findings reveal that the number of off-policy updates, the
diversity of training data, and the clipping thresholds in the optimization
objective are critical factors influencing the entropy of LLMs trained with
RLVR. Moreover, we theoretically and empirically demonstrate that tokens with
positive advantages are the primary contributors to entropy collapse, and that
model entropy can be effectively regulated by adjusting the relative loss
weights of tokens with positive and negative advantages during training.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05993v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05993v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A PDE Perspective on Generative Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Score-based diffusion models have emerged as a powerful class of generative
methods, achieving state-of-the-art performance across diverse domains. Despite
their empirical success, the mathematical foundations of those models remain
only partially understood, particularly regarding the stability and consistency
of the underlying stochastic and partial differential equations governing their
dynamics.
  In this work, we develop a rigorous partial differential equation (PDE)
framework for score-based diffusion processes. Building on the Li--Yau
differential inequality for the heat flow, we prove well-posedness and derive
sharp $L^p$-stability estimates for the associated score-based Fokker--Planck
dynamics, providing a mathematically consistent description of their temporal
evolution. Through entropy stability methods, we further show that the
reverse-time dynamics of diffusion models concentrate on the data manifold for
compactly supported data distributions and a broad class of initialization
schemes, with a concentration rate of order $\sqrt{t}$ as $t \to 0$.
  These results yield a theoretical guarantee that, under exact score guidance,
diffusion trajectories return to the data manifold while preserving imitation
fidelity. Our findings also provide practical insights for designing diffusion
models, including principled criteria for score-function construction, loss
formulation, and stopping-time selection. Altogether, this framework provides a
quantitative understanding of the trade-off between generative capacity and
imitation fidelity, bridging rigorous analysis and model design within a
unified mathematical perspective.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05940v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05940v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        IoT-based Fresh Produce Supply Chain Under Uncertainty: An Adaptive Optimization Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Fruits and vegetables form a vital component of the global economy; however,
their distribution poses complex logistical challenges due to high
perishability, supply fluctuations, strict quality and safety standards, and
environmental sensitivity. In this paper, we propose an adaptive optimization
model that accounts for delays, travel time, and associated temperature changes
impacting produce shelf life, and compare it against traditional approaches
such as Robust Optimization, Distributionally Robust Optimization, and
Stochastic Programming. Additionally, we conduct a series of computational
experiments using Internet of Things (IoT) sensor data to evaluate the
performance of our proposed model. Our study demonstrates that the proposed
adaptive model achieves a higher shelf life, extending it by over 18\% compared
to traditional optimization models, by dynamically mitigating temperature
deviations through a temperature feedback mechanism. The promising results
demonstrate the potential of this approach to improve both the freshness and
efficiency of logistics systems an aspect often neglected in previous works.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05920v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05920v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for
small to medium-sized tabular data, which has attracted much attention
recently. This paper investigates the application of TabPFN in Combinatorial
Optimization (CO) problems. The aim is to lessen challenges in time and
data-intensive training requirements often observed in using traditional
methods including exact and heuristic algorithms, Machine Learning (ML)-based
models, to solve CO problems. Proposing possibly the first ever application of
TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the
Travelling Salesman Problem (TSP), one of the most well-known CO problems.
Specifically, we adopt the node-based approach and the node-predicting
adaptation strategy to construct the entire TSP route. Our evaluation with
varying instance sizes confirms that TabPFN requires minimal training, adapts
to TSP using a single sample, performs better generalization across varying TSP
instance sizes, and reduces performance degradation. Furthermore, the training
process with adaptation and fine-tuning is completed within minutes. The
methodology leads to strong solution quality even without post-processing and
achieves performance comparable to other models with post-processing
refinement. Our findings suggest that the TabPFN model is a promising approach
to solve structured and CO problems efficiently under training resource
constraints and rapid deployment requirements.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05872v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05872v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WAR-Re: Web API Recommendation with Semantic Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the development of cloud computing, the number of Web APIs has increased
dramatically, further intensifying the demand for efficient Web API
recommendation. Despite the demonstrated success of previous Web API
recommendation solutions, two critical challenges persist: 1) a fixed top-N
recommendation that cannot accommodate the varying API cardinality requirements
of different mashups, and 2) these methods output only ranked API lists without
accompanying reasons, depriving users of understanding the recommendation. To
address these challenges, we propose WAR-Re, an LLM-based model for Web API
recommendation with semantic reasoning for justification. WAR-Re leverages
special start and stop tokens to handle the first challenge and uses two-stage
training: supervised fine-tuning and reinforcement learning via Group Relative
Policy Optimization (GRPO) to enhance the model's ability in both tasks.
Comprehensive experimental evaluations on the ProgrammableWeb dataset
demonstrate that WAR-Re achieves a gain of up to 21.59\% over the
state-of-the-art baseline model in recommendation accuracy, while consistently
producing high-quality semantic reasons for recommendations.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05820v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05820v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In today's landscape, Mixture of Experts (MoE) is a crucial architecture that
has been used by many of the most advanced models. One of the major challenges
of MoE models is that they usually require much more memory than their dense
counterparts due to their unique architecture, and hence are harder to deploy
in environments with limited GPU memory, such as edge devices. MoE offloading
is a promising technique proposed to overcome this challenge, especially if it
is enhanced with caching and pre-fetching, but prior work stopped at suboptimal
caching algorithm and offered limited insights. In this work, we study MoE
offloading in depth and make the following contributions: 1. We analyze the
expert activation and LRU caching behavior in detail and provide traces. 2. We
propose LFU caching optimization based on our analysis and obtain strong
improvements from LRU. 3. We implement and experiment speculative expert
pre-fetching, providing detailed trace showing its huge potential . 4. In
addition, our study extensively covers the behavior of the MoE architecture
itself, offering information on the characteristic of the gating network and
experts. This can inspire future work on the interpretation of MoE models and
the development of pruning techniques for MoE architecture with minimal
performance loss.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05814v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05814v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Deep Learning Model for Predicting Transformation Legality
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Compilers must check the legality of code transformations to guarantee the
correctness of applying a sequence of code transformations to a given code.
While such a legality check needs to be precisely computed in general, we can
use an approximate legality prediction model in certain cases, such as training
a reinforcement learning (RL) agent for schedule prediction. In this paper, we
propose an approximate method for legality checks. We propose a novel DL model
for predicting the legality of transformations. The model takes the code
representation and a list of transformations as input and predicts whether
applying those transformations to the code is legal. We implement and evaluate
the proposed model, demonstrating its effectiveness. Our evaluation shows an F1
score of 0.91 on a test set of randomly generated programs. To further evaluate
the model in a practical scenario, we used the model to replace the legality
check used during the training of an RL agent designed for automatic code
optimization. We demonstrate that such a replacement enables the agent to train
on twice as many steps, resulting in faster training and reducing resource
usage by approximately 80\% for CPU and 35\% for RAM. The agent trained using
this approach maintains comparable performance, with only a 4\% reduction on
benchmarks from the Polybench suite compared to the traditional method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06120v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06120v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the sequential decision-making problem for automated weaning of
mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS
devices are percutaneous micro-axial flow pumps that provide left ventricular
unloading and forward blood flow, but current weaning strategies vary
significantly across care teams and lack data-driven approaches. Offline
reinforcement learning (RL) has proven to be successful in sequential
decision-making tasks, but our setting presents challenges for training and
evaluating traditional offline RL methods: prohibition of online patient
interaction, highly uncertain circulatory dynamics due to concurrent
treatments, and limited data availability. We developed an end-to-end machine
learning framework with two key contributions (1) Clinically-aware
OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized
offline RL algorithm for out-of-distribution suppression that also incorporates
clinically-informed reward shaping and (2) a Transformer-based probabilistic
digital twin that models MCS circulatory dynamics for policy evaluation with
rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves
theoretical performance guarantees under mild assumptions. CORMPO attains a
higher reward than the offline RL baselines by 28% and higher scores in
clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a
principled framework for safe offline policy learning in high-stakes medical
applications where domain expertise and safety constraints are essential.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06111v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06111v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MuonAll: Muon Variant for Efficient Finetuning of Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Muon optimizer has demonstrated robust results in pretraining of language
models but its performance in finetuning of existing public pretrained models
is not yet explored. Currently, Muon is used along with AdamW introducing a
scope of improvement for adopting all parameters inside Muon. We introduce
MuonAll, which incorporates all the parameters inside Muon by transforming into
2D matrices. We conduct extensive finetuning experiments across publicly
available language models with model sizes upto half billion parameters. Muon
and MuonAll perform at par with AdamW across major benchmarks, highlighting
their effectiveness as alternative optimizers. We open-source the distributed
implementations of Muon and MuonAll, available at
https://github.com/Saurabh750/optimizer
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06086v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06086v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Physics-Informed Design of Input Convex Neural Networks for Consistency Optimal Transport Flow Matching
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose a consistency model based on the optimal-transport flow. A
physics-informed design of partially input-convex neural networks (PICNN) plays
a central role in constructing the flow field that emulates the displacement
interpolation. During the training stage, we couple the Hamilton-Jacobi (HJ)
residual in the OT formulation with the original flow matching loss function.
Our approach avoids inner optimization subproblems that are present in previous
one-step OFM approaches. During the prediction stage, our approach supports
both one-step (Brenier-map) and multi-step ODE sampling from the same learned
potential, leveraging the straightness of the OT flow. We validate scalability
and performance on standard OT benchmarks.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06042v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06042v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning solutions of parameterized stiff ODEs using Gaussian processes
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Stiff ordinary differential equations (ODEs) play an important role in many
scientific and engineering applications. Often, the dependence of the solution
of the ODE on additional parameters is of interest, e.g.\ when dealing with
uncertainty quantification or design optimization. Directly studying this
dependence can quickly become too computationally expensive, such that cheaper
surrogate models approximating the solution are of interest. One popular class
of surrogate models are Gaussian processes (GPs). They perform well when
approximating stationary functions, functions which have a similar level of
variation along any given parameter direction, however solutions to stiff ODEs
are often characterized by a mixture of regions of rapid and slow variation
along the time axis and when dealing with such nonstationary functions, GP
performance frequently degrades drastically. We therefore aim to reparameterize
stiff ODE solutions based on the available data, to make them appear more
stationary and hence recover good GP performance. This approach comes with
minimal computational overhead and requires no internal changes to the GP
implementation, as it can be seen as a separate preprocessing step. We
illustrate the achieved benefits using multiple examples.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05990v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05990v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent research has demonstrated that artificial intelligence (AI) can assist
electronic design automation (EDA) in improving both the quality and efficiency
of chip design. But current AI for EDA (AI-EDA) infrastructures remain
fragmented, lacking comprehensive solutions for the entire data pipeline from
design execution to AI integration. Key challenges include fragmented flow
engines that generate raw data, heterogeneous file formats for data exchange,
non-standardized data extraction methods, and poorly organized data storage.
This work introduces a unified open-source library for EDA (AiEDA) that
addresses these issues. AiEDA integrates multiple design-to-vector data
representation techniques that transform diverse chip design data into
universal multi-level vector representations, establishing an AI-aided design
(AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical
design flows with programmatic data extraction and standardized Python
interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA
library, we generate iDATA, a 600GB dataset of structured data derived from 50
real chip designs (28nm), and validate its effectiveness through seven
representative AAD tasks spanning prediction, generation, optimization and
analysis. The code is publicly available at
https://github.com/OSCC-Project/AiEDA, while the full iDATA dataset is being
prepared for public release, providing a foundation for future AI-EDA research.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05823v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05823v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) often exhibit implicit biases and discriminatory
tendencies that reflect underlying social stereotypes. While recent alignment
techniques such as RLHF and DPO have mitigated some of these issues, they
remain limited in addressing culturally specific and multi-dimensional forms of
discrimination. This paper proposes a Multi-Reward Group Relative Policy
Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free
behavior. Our approach constructs a synthetic English-language dataset derived
from Chinese-context discrimination categories, including regional, ethnic, and
occupational biases. Each instance is paired with both neutral and biased
responses to train a reward model based on DeBERTa-v3, which provides
multi-dimensional reward signals capturing fairness, neutrality, and linguistic
quality. The trained reward model then guides GRPO fine-tuning to optimize
model outputs along these ethical dimensions. Experimental results demonstrate
significant reductions in bias intensity and improved alignment with
non-discriminatory standards without compromising fluency or informativeness.
This study highlights the effectiveness of GRPO-based multi-reward optimization
for de-biasing LLMs and offers a replicable framework for cultural-contextual
ethical alignment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06023v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06023v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) demonstrate strong capabilities in solving
complex tasks when integrated with external tools. The Model Context Protocol
(MCP) has become a standard interface for enabling such tool-based
interactions. However, these interactions introduce substantial security
concerns, particularly when the MCP server is compromised or untrustworthy.
While prior benchmarks primarily focus on prompt injection attacks or analyze
the vulnerabilities of LLM MCP interaction trajectories, limited attention has
been given to the underlying system logs associated with malicious MCP servers.
To address this gap, we present the first synthetic benchmark for evaluating
LLMs ability to identify security risks from system logs. We define nine
categories of MCP server risks and generate 1,800 synthetic system logs using
ten state-of-the-art LLMs. These logs are embedded in the return values of 243
curated MCP servers, yielding a dataset of 2,421 chat histories for training
and 471 queries for evaluation. Our pilot experiments reveal that smaller
models often fail to detect risky system logs, leading to high false negatives.
While models trained with supervised fine-tuning (SFT) tend to over-flag benign
logs, resulting in elevated false positives, Reinforcement Learning from
Verifiable Reward (RLVR) offers a better precision-recall balance. In
particular, after training with Group Relative Policy Optimization (GRPO),
Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing
large remote model by 9 percentage points. Fine-grained, per-category analysis
further underscores the effectiveness of reinforcement learning in enhancing
LLM safety within the MCP framework. Code and data are available at:
https://github.com/PorUna-byte/MCP-Guard/tree/master
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05867v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05867v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-6" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RAG
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 5 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前RAG（检索增强生成）技术的研究动态主要集中在提升模型在复杂问答场景中的表现、减少生成内容的虚假信息、优化知识图谱的构建方法以及探索其在医学领域的应用。研究者们普遍关注如何通过改进知识表示和引入结构化知识图谱来增强RAG系统的性能，同时也在努力解决模型生成过程中的错误传播问题。这些研究的潜在价值在于提升RAG系统在多领域的应用效果，尤其是在信息密集和高风险的医疗场景中。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluation of retrieval-based QA on QUEST-LOFT
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the popularity of retrieval-augmented generation (RAG) as a solution
for grounded QA in both academia and industry, current RAG methods struggle
with questions where the necessary information is distributed across many
documents or where retrieval needs to be combined with complex reasoning.
Recently, the LOFT study has shown that this limitation also applies to
approaches based on long-context language models, with the QUEST benchmark
exhibiting particularly large headroom. In this paper, we provide an in-depth
analysis of the factors contributing to the poor performance on QUEST-LOFT,
publish updated numbers based on a thorough human evaluation, and demonstrate
that RAG can be optimized to significantly outperform long-context approaches
when combined with a structured output format containing reasoning and
evidence, optionally followed by answer re-verification.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06125v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06125v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Stemming Hallucination in Language Models Using a Licensing Oracle
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language models exhibit remarkable natural language generation capabilities
but remain prone to hallucinations, generating factually incorrect information
despite producing syntactically coherent responses. This study introduces the
Licensing Oracle, an architectural solution designed to stem hallucinations in
LMs by enforcing truth constraints through formal validation against structured
knowledge graphs. Unlike statistical approaches that rely on data scaling or
fine-tuning, the Licensing Oracle embeds a deterministic validation step into
the model's generative process, ensuring that only factually accurate claims
are made. We evaluated the effectiveness of the Licensing Oracle through
experiments comparing it with several state-of-the-art methods, including
baseline language model generation, fine-tuning for factual recall, fine-tuning
for abstention behavior, and retrieval-augmented generation (RAG). Our results
demonstrate that although RAG and fine-tuning improve performance, they fail to
eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect
abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring
that only valid claims were generated with 89.1% accuracy in factual responses.
This work shows that architectural innovations, such as the Licensing Oracle,
offer a necessary and sufficient solution for hallucinations in domains with
structured knowledge representations, offering guarantees that statistical
methods cannot match. Although the Licensing Oracle is specifically designed to
address hallucinations in fact-based domains, its framework lays the groundwork
for truth-constrained generation in future AI systems, providing a new path
toward reliable, epistemically grounded models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06073v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06073v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieval-Augmented Generation (RAG) systems combine Large Language Models
(LLMs) with external knowledge, and their performance depends heavily on how
that knowledge is represented. This study investigates how different Knowledge
Graph (KG) construction strategies influence RAG performance. We compare a
variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over
KGs built from ontologies derived either from relational databases or textual
corpora. Results show that ontology-guided KGs incorporating chunk information
achieve competitive performance with state-of-the-art frameworks, substantially
outperforming vector retrieval baselines. Moreover, the findings reveal that
ontology-guided KGs built from relational databases perform competitively to
ones built with ontologies extracted from text, with the benefit of offering a
dual advantage: they require a one-time-only ontology learning process,
substantially reducing LLM usage costs; and avoid the complexity of ontology
merging inherent to text-based approaches.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05991v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05991v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid growth of medical knowledge and increasing complexity of clinical
practice pose challenges. In this context, large language models (LLMs) have
demonstrated value; however, inherent limitations remain. Retrieval-augmented
generation (RAG) technologies show potential to enhance their clinical
applicability. This study reviewed RAG applications in medicine. We found that
research primarily relied on publicly available data, with limited application
in private data. For retrieval, approaches commonly relied on English-centric
embedding models, while LLMs were mostly generic, with limited use of
medical-specific LLMs. For evaluation, automated metrics evaluated generation
quality and task performance, whereas human evaluation focused on accuracy,
completeness, relevance, and fluency, with insufficient attention to bias and
safety. RAG applications were concentrated on question answering, report
generation, text summarization, and information extraction. Overall, medical
RAG remains at an early stage, requiring advances in clinical validation,
cross-linguistic adaptation, and support for low-resource settings to enable
trustworthy and responsible global use.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05901v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05901v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Catching Contamination Before Generation: Spectral Kill Switches for Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Agentic language models compose multi step reasoning chains, yet intermediate
steps can be corrupted by inconsistent context, retrieval errors, or
adversarial inputs, which makes post hoc evaluation too late because errors
propagate before detection. We introduce a diagnostic that requires no
additional training and uses only the forward pass to emit a binary accept or
reject signal during agent execution. The method analyzes token graphs induced
by attention and computes two spectral statistics in early layers, namely the
high frequency energy ratio and spectral entropy. We formalize these signals,
establish invariances, and provide finite sample estimators with uncertainty
quantification. Under a two regime mixture assumption with a monotone
likelihood ratio property, we show that a single threshold on the high
frequency energy ratio is optimal in the Bayes sense for detecting context
inconsistency. Empirically, the high frequency energy ratio exhibits robust
bimodality during context verification across multiple model families, which
enables gating decisions with overhead below one millisecond on our hardware
and configurations. We demonstrate integration into retrieval augmented agent
pipelines and discuss deployment as an inline safety monitor. The approach
detects contamination while the model is still processing the text, before
errors commit to the reasoning chain.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05804v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05804v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-7" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RL
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 111 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前强化学习（RL）领域的研究动态主要集中在跨领域知识迁移、模型偏见评估、低成本模型性能预测及多智能体规划等方面。研究者们逐渐认识到，利用大型语言模型（LLMs）和新颖的评估框架可以有效应对复杂的数据和推理任务，同时也在探索如何降低模型训练的计算成本。此外，针对模型在实际应用中可能产生的隐性偏见，研究者们提出了新的方法来识别和缓解这些问题，显示出对模型公平性和实用性的重视。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        LLM Attention Transplant for Transfer Learning of Tabular Data Across Disparate Domains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Transfer learning of tabular data is non-trivial due to heterogeneity in the
feature space across disparate domains. The limited success of traditional deep
learning in tabular knowledge transfer can be advanced by leveraging large
language models (LLMs). However, the efficacy of LLMs often stagnates for mixed
data types structured in tables due to the limitations of text prompts and
in-context learning. We propose a lightweight transfer learning framework that
fine-tunes an LLM using source tabular data and transplants the LLM's selective
$key$ and $value$ projection weights into a gated feature tokenized transformer
(gFTT) built for tabular data. The gFTT model with cross-domain attention is
fine-tuned using target tabular data for transfer learning, eliminating the
need for shared features, LLM prompt engineering, and large-scale pretrained
models. Our experiments using ten pairs of source-target data sets and 12
baselines demonstrate the superiority of the proposed LLM-attention transplant
for transfer learning (LATTLE) method over traditional ML models,
state-of-the-art deep tabular architectures, and transfer learning models
trained on thousands to billions of tabular samples. The proposed attention
transfer demonstrates an effective solution to learning relationships between
data tables using an LLM in a low-resource learning environment. The source
code for the proposed method is publicly available.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06161v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06161v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    While recent safety guardrails effectively suppress overtly biased outputs,
subtler forms of social bias emerge during complex logical reasoning tasks that
evade current evaluation benchmarks. To fill this gap, we introduce a new
evaluation framework, PRIME (Puzzle Reasoning for Implicit Biases in Model
Evaluation), that uses logic grid puzzles to systematically probe the influence
of social stereotypes on logical reasoning and decision making in LLMs. Our use
of logic puzzles enables automatic generation and verification, as well as
variability in complexity and biased settings. PRIME includes stereotypical,
anti-stereotypical, and neutral puzzle variants generated from a shared puzzle
structure, allowing for controlled and fine-grained comparisons. We evaluate
multiple model families across puzzle sizes and test the effectiveness of
prompt-based mitigation strategies. Focusing our experiments on gender
stereotypes, our findings highlight that models consistently reason more
accurately when solutions align with stereotypical associations. This
demonstrates the significance of PRIME for diagnosing and quantifying social
biases perpetuated in the deductive reasoning of LLMs, where fairness is
critical.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06160v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06160v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Models Got Talent: Identifying High Performing Wearable Human Activity Recognition Models Without Training
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A promising alternative to the computationally expensive Neural Architecture
Search (NAS) involves the development of \textit{Zero Cost Proxies (ZCPs)},
which correlate well to trained performance, but can be computed through a
single forward/backward pass on a randomly sampled batch of data. In this
paper, we investigate the effectiveness of ZCPs for HAR on six benchmark
datasets, and demonstrate that they discover network architectures that obtain
within 5\% of performance attained by full scale training involving 1500
randomly sampled architectures. This results in substantial computational
savings as high performing architectures can be discovered with minimal
training. Our experiments not only introduce ZCPs to sensor-based HAR, but also
demonstrate that they are robust to data noise, further showcasing their
suitability for practical scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06157v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06157v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Large Language Models Develop Novel Social Biases Through Adaptive Exploration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As large language models (LLMs) are adopted into frameworks that grant them
the capacity to make real decisions, it is increasingly important to ensure
that they are unbiased. In this paper, we argue that the predominant approach
of simply removing existing biases from models is not enough. Using a paradigm
from the psychology literature, we demonstrate that LLMs can spontaneously
develop novel social biases about artificial demographic groups even when no
inherent differences exist. These biases result in highly stratified task
allocations, which are less fair than assignments by human participants and are
exacerbated by newer and larger models. In social science, emergent biases like
these have been shown to result from exploration-exploitation trade-offs, where
the decision-maker explores too little, allowing early observations to strongly
influence impressions about entire demographic groups. To alleviate this
effect, we examine a series of interventions targeting model inputs, problem
structure, and explicit steering. We find that explicitly incentivizing
exploration most robustly reduces stratification, highlighting the need for
better multifaceted objectives to mitigate bias. These results reveal that LLMs
are not merely passive mirrors of human social biases, but can actively create
new ones from experience, raising urgent questions about how these systems will
shape societies over time.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06148v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06148v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Referring Expressions as a Lens into Spatial Language Grounding in Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Spatial Reasoning is an important component of human cognition and is an area
in which the latest Vision-language models (VLMs) show signs of difficulty. The
current analysis works use image captioning tasks and visual question
answering. In this work, we propose using the Referring Expression
Comprehension task instead as a platform for the evaluation of spatial
reasoning by VLMs. This platform provides the opportunity for a deeper analysis
of spatial comprehension and grounding abilities when there is 1) ambiguity in
object detection, 2) complex spatial expressions with a longer sentence
structure and multiple spatial relations, and 3) expressions with negation
('not'). In our analysis, we use task-specific architectures as well as large
VLMs and highlight their strengths and weaknesses in dealing with these
specific situations. While all these models face challenges with the task at
hand, the relative behaviors depend on the underlying models and the specific
categories of spatial semantics (topological, directional, proximal, etc.). Our
results highlight these challenges and behaviors and provide insight into
research gaps and future directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06146v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06146v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MALinZero: Efficient Low-Dimensional Search for Mastering Complex Multi-Agent Planning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Monte Carlo Tree Search (MCTS), which leverages Upper Confidence Bound for
Trees (UCTs) to balance exploration and exploitation through randomized
sampling, is instrumental to solving complex planning problems. However, for
multi-agent planning, MCTS is confronted with a large combinatorial action
space that often grows exponentially with the number of agents. As a result,
the branching factor of MCTS during tree expansion also increases
exponentially, making it very difficult to efficiently explore and exploit
during tree search. To this end, we propose MALinZero, a new approach to
leverage low-dimensional representational structures on joint-action returns
and enable efficient MCTS in complex multi-agent planning. Our solution can be
viewed as projecting the joint-action returns into the low-dimensional space
representable using a contextual linear bandit problem formulation. We solve
the contextual linear bandit problem with convex and $\mu$-smooth loss
functions -- in order to place more importance on better joint actions and
mitigate potential representational limitations -- and derive a linear Upper
Confidence Bound applied to trees (LinUCT) to enable novel multi-agent
exploration and exploitation in the low-dimensional space. We analyze the
regret of MALinZero for low-dimensional reward functions and propose an
$(1-\tfrac1e)$-approximation algorithm for the joint action selection by
maximizing a sub-modular objective. MALinZero demonstrates state-of-the-art
performance on multi-agent benchmarks such as matrix games, SMAC, and SMACv2,
outperforming both model-based and model-free multi-agent reinforcement
learning baselines with faster learning speed and better performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06142v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06142v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When Object-Centric World Models Meet Policy Learning: From Pixels to Policies, and Where It Breaks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Object-centric world models (OCWM) aim to decompose visual scenes into
object-level representations, providing structured abstractions that could
improve compositional generalization and data efficiency in reinforcement
learning. We hypothesize that explicitly disentangled object-level
representations, by localizing task-relevant information, can enhance policy
performance across novel feature combinations. To test this hypothesis, we
introduce DLPWM, a fully unsupervised, disentangled object-centric world model
that learns object-level latents directly from pixels. DLPWM achieves strong
reconstruction and prediction performance, including robustness to several
out-of-distribution (OOD) visual variations. However, when used for downstream
model-based control, policies trained on DLPWM latents underperform compared to
DreamerV3. Through latent-trajectory analyses, we identify representation shift
during multi-object interactions as a key driver of unstable policy learning.
Our results suggest that, although object-centric perception supports robust
visual modeling, achieving stable control requires mitigating latent drift.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06136v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06136v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Maestro: Learning to Collaborate via Conditional Listwise Policy Optimization for Multi-Agent LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Multi-agent systems (MAS) built on Large Language Models (LLMs) are being
used to approach complex problems and can surpass single model inference.
However, their success hinges on navigating a fundamental cognitive tension:
the need to balance broad, divergent exploration of the solution space with a
principled, convergent synthesis to the optimal solution. Existing paradigms
often struggle to manage this duality, leading to premature consensus, error
propagation, and a critical credit assignment problem that fails to distinguish
between genuine reasoning and superficially plausible arguments. To resolve
this core challenge, we propose the Multi-Agent Exploration-Synthesis framework
Through Role Orchestration (Maestro), a principled paradigm for collaboration
that structurally decouples these cognitive modes. Maestro uses a collective of
parallel Execution Agents for diverse exploration and a specialized Central
Agent for convergent, evaluative synthesis. To operationalize this critical
synthesis phase, we introduce Conditional Listwise Policy Optimization (CLPO),
a reinforcement learning objective that disentangles signals for strategic
decisions and tactical rationales. By combining decision-focused policy
gradients with a list-wise ranking loss over justifications, CLPO achieves
clean credit assignment and stronger comparative supervision. Experiments on
mathematical reasoning and general problem-solving benchmarks demonstrate that
Maestro, coupled with CLPO, consistently outperforms existing state-of-the-art
multi-agent approaches, delivering absolute accuracy gains of 6% on average and
up to 10% at best.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06134v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06134v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evaluation of retrieval-based QA on QUEST-LOFT
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the popularity of retrieval-augmented generation (RAG) as a solution
for grounded QA in both academia and industry, current RAG methods struggle
with questions where the necessary information is distributed across many
documents or where retrieval needs to be combined with complex reasoning.
Recently, the LOFT study has shown that this limitation also applies to
approaches based on long-context language models, with the QUEST benchmark
exhibiting particularly large headroom. In this paper, we provide an in-depth
analysis of the factors contributing to the poor performance on QUEST-LOFT,
publish updated numbers based on a thorough human evaluation, and demonstrate
that RAG can be optimized to significantly outperform long-context approaches
when combined with a structured output format containing reasoning and
evidence, optionally followed by answer re-verification.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06125v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06125v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapting Web Agents with Synthetic Supervision
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Web agents struggle to adapt to new websites due to the scarcity of
environment specific tasks and demonstrations. Recent works have explored
synthetic data generation to address this challenge, however, they suffer from
data quality issues where synthesized tasks contain hallucinations that cannot
be executed, and collected trajectories are noisy with redundant or misaligned
actions. In this paper, we propose SynthAgent, a fully synthetic supervision
framework that aims at improving synthetic data quality via dual refinement of
both tasks and trajectories. Our approach begins by synthesizing diverse tasks
through categorized exploration of web elements, ensuring efficient coverage of
the target environment. During trajectory collection, we refine tasks when
conflicts with actual observations are detected, mitigating hallucinations
while maintaining task consistency. After collection, we conduct trajectory
refinement with a global context to mitigate potential noise or misalignments.
Finally, we fine-tune open-source web agents on the refined synthetic data to
adapt them to the target environment. Experimental results demonstrate that
SynthAgent outperforms existing synthetic data methods, validating the
importance of high-quality synthetic supervision. The code will be publicly
available at https://github.com/aiming-lab/SynthAgent.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06101v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06101v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SWE-fficiency: Can Language Models Optimize Real-World Repositories on Real Workloads?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Optimizing the performance of large-scale software repositories demands
expertise in code reasoning and software engineering (SWE) to reduce runtime
while preserving program correctness. However, most benchmarks emphasize what
to fix rather than how to fix code. We introduce \textsc{SWE-fficiency}, a
benchmark for evaluating repository-level performance optimization on real
workloads. Our suite contains 498 tasks across nine widely used data-science,
machine-learning, and HPC repositories (e.g., numpy, pandas, scipy): given a
complete codebase and a slow workload, an agent must investigate code
semantics, localize bottlenecks and relevant tests, and produce a patch that
matches or exceeds expert speedup while passing the same unit tests. To enable
this how-to-fix evaluation, our automated pipeline scrapes GitHub pull requests
for performance-improving edits, combining keyword filtering, static analysis,
coverage tooling, and execution validation to both confirm expert speedup
baselines and identify relevant repository unit tests. Empirical evaluation of
state-of-the-art agents reveals significant underperformance. On average,
agents achieve less than 0.15x the expert speedup: agents struggle in
localizing optimization opportunities, reasoning about execution across
functions, and maintaining correctness in proposed edits. We release the
benchmark and accompanying data pipeline to facilitate research on automated
performance engineering and long-horizon software reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06090v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06090v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06087v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06087v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Simulating Students with Large Language Models: A Review of Architecture, Mechanisms, and Role Modelling in Education with Generative AI
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Simulated Students offer a valuable methodological framework for evaluating
pedagogical approaches and modelling diverse learner profiles, tasks which are
otherwise challenging to undertake systematically in real-world settings.
Recent research has increasingly focused on developing such simulated agents to
capture a range of learning styles, cognitive development pathways, and social
behaviours. Among contemporary simulation techniques, the integration of large
language models (LLMs) into educational research has emerged as a particularly
versatile and scalable paradigm. LLMs afford a high degree of linguistic
realism and behavioural adaptability, enabling agents to approximate cognitive
processes and engage in contextually appropriate pedagogical dialogues. This
paper presents a thematic review of empirical and methodological studies
utilising LLMs to simulate student behaviour across educational environments.
We synthesise current evidence on the capacity of LLM-based agents to emulate
learner archetypes, respond to instructional inputs, and interact within
multi-agent classroom scenarios. Furthermore, we examine the implications of
such systems for curriculum development, instructional evaluation, and teacher
training. While LLMs surpass rule-based systems in natural language generation
and situational flexibility, ongoing concerns persist regarding algorithmic
bias, evaluation reliability, and alignment with educational objectives. The
review identifies existing technological and methodological gaps and proposes
future research directions for integrating generative AI into adaptive learning
systems and instructional design.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06078v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06078v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Stemming Hallucination in Language Models Using a Licensing Oracle
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language models exhibit remarkable natural language generation capabilities
but remain prone to hallucinations, generating factually incorrect information
despite producing syntactically coherent responses. This study introduces the
Licensing Oracle, an architectural solution designed to stem hallucinations in
LMs by enforcing truth constraints through formal validation against structured
knowledge graphs. Unlike statistical approaches that rely on data scaling or
fine-tuning, the Licensing Oracle embeds a deterministic validation step into
the model's generative process, ensuring that only factually accurate claims
are made. We evaluated the effectiveness of the Licensing Oracle through
experiments comparing it with several state-of-the-art methods, including
baseline language model generation, fine-tuning for factual recall, fine-tuning
for abstention behavior, and retrieval-augmented generation (RAG). Our results
demonstrate that although RAG and fine-tuning improve performance, they fail to
eliminate hallucinations. In contrast, the Licensing Oracle achieved perfect
abstention precision (AP = 1.0) and zero false answers (FAR-NE = 0.0), ensuring
that only valid claims were generated with 89.1% accuracy in factual responses.
This work shows that architectural innovations, such as the Licensing Oracle,
offer a necessary and sufficient solution for hallucinations in domains with
structured knowledge representations, offering guarantees that statistical
methods cannot match. Although the Licensing Oracle is specifically designed to
address hallucinations in fact-based domains, its framework lays the groundwork
for truth-constrained generation in future AI systems, providing a new path
toward reliable, epistemically grounded models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06073v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06073v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ScRPO: From Errors to Insights
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose Self-correction Relative Policy Optimization (ScRPO), a novel
reinforcement learning framework designed to enhance large language models on
challenging mathemati- cal problems by leveraging self-reflection and error
correction. Our approach consists of two stages: (1) Trial-and-error learning
stage: training the model with GRPO and collect- ing incorrect answers along
with their cor- responding questions in an error pool; (2) Self-correction
learning stage: guiding the model to reflect on why its previous an- swers were
wrong. Extensive experiments across multiple math reasoning benchmarks,
including AIME, AMC, Olympiad, MATH- 500, GSM8k, using Deepseek-Distill-Qwen-
1.5B and Deepseek-Distill-Qwen-7B. The ex- perimental results demonstrate that
ScRPO consistently outperforms several post-training methods. These findings
highlight ScRPO as a promising paradigm for enabling language models to
self-improve on difficult tasks with limited external feedback, paving the way
to- ward more reliable and capable AI systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06065v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06065v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Privacy-Preserving Federated Learning Method with Homomorphic Encryption in Omics Data
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Omics data is widely employed in medical research to identify disease
mechanisms and contains highly sensitive personal information. Federated
Learning (FL) with Differential Privacy (DP) can ensure the protection of omics
data privacy against malicious user attacks. However, FL with the DP method
faces an inherent trade-off: stronger privacy protection degrades predictive
accuracy due to injected noise. On the other hand, Homomorphic Encryption (HE)
allows computations on encrypted data and enables aggregation of encrypted
gradients without DP-induced noise can increase the predictive accuracy.
However, it may increase the computation cost. To improve the predictive
accuracy while considering the computational ability of heterogeneous clients,
we propose a Privacy-Preserving Machine Learning (PPML)-Hybrid method by
introducing HE. In the proposed PPML-Hybrid method, clients distributed select
either HE or DP based on their computational resources, so that HE clients
contribute noise-free updates while DP clients reduce computational overhead.
Meanwhile, clients with high computational resources clients can flexibly adopt
HE or DP according to their privacy needs. Performance evaluation on omics
datasets show that our proposed method achieves comparable predictive accuracy
while significantly reducing computation time relative to HE-only.
Additionally, it outperforms DP-only methods under equivalent or stricter
privacy budgets.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06064v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06064v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        How Particle-System Random Batch Methods Enhance Graph Transformer: Memory Efficiency and Parallel Computing Strategy
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Attention mechanism is a significant part of Transformer models. It helps
extract features from embedded vectors by adding global information and its
expressivity has been proved to be powerful. Nevertheless, the quadratic
complexity restricts its practicability. Although several researches have
provided attention mechanism in sparse form, they are lack of theoretical
analysis about the expressivity of their mechanism while reducing complexity.
In this paper, we put forward Random Batch Attention (RBA), a linear
self-attention mechanism, which has theoretical support of the ability to
maintain its expressivity. Random Batch Attention has several significant
strengths as follows: (1) Random Batch Attention has linear time complexity.
Other than this, it can be implemented in parallel on a new dimension, which
contributes to much memory saving. (2) Random Batch Attention mechanism can
improve most of the existing models by replacing their attention mechanisms,
even many previously improved attention mechanisms. (3) Random Batch Attention
mechanism has theoretical explanation in convergence, as it comes from Random
Batch Methods on computation mathematics. Experiments on large graphs have
proved advantages mentioned above. Also, the theoretical modeling of
self-attention mechanism is a new tool for future research on
attention-mechanism analysis.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06044v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06044v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Advancing Ocean State Estimation with efficient and scalable AI
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate and efficient global ocean state estimation remains a grand
challenge for Earth system science, hindered by the dual bottlenecks of
computational scalability and degraded data fidelity in traditional data
assimilation (DA) and deep learning (DL) approaches. Here we present an
AI-driven Data Assimilation Framework for Ocean (ADAF-Ocean) that directly
assimilates multi-source and multi-scale observations, ranging from sparse
in-situ measurements to 4 km satellite swaths, without any interpolation or
data thinning. Inspired by Neural Processes, ADAF-Ocean learns a continuous
mapping from heterogeneous inputs to ocean states, preserving native data
fidelity. Through AI-driven super-resolution, it reconstructs 0.25$^\circ$
mesoscale dynamics from coarse 1$^\circ$ fields, which ensures both efficiency
and scalability, with just 3.7\% more parameters than the 1$^\circ$
configuration. When coupled with a DL forecasting system, ADAF-Ocean extends
global forecast skill by up to 20 days compared to baselines without
assimilation. This framework establishes a computationally viable and
scientifically rigorous pathway toward real-time, high-resolution Earth system
monitoring.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06041v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06041v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S2ML: Spatio-Spectral Mutual Learning for Depth Completion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or
structured light often suffer from incomplete depth values due to weak
reflections, boundary shadows, and artifacts, which limit their applications in
downstream vision tasks. Existing methods address this problem through depth
completion in the image domain, but they overlook the physical characteristics
of raw depth images. It has been observed that the presence of invalid depth
areas alters the frequency distribution pattern. In this work, we propose a
Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of
both spatial and frequency domains for depth completion. Specifically, we
consider the distinct properties of amplitude and phase spectra and devise a
dedicated spectral fusion module. Meanwhile, the local and global correlations
between spatial-domain and frequency-domain features are calculated in a
unified embedding space. The gradual mutual representation and refinement
encourage the network to fully explore complementary physical characteristics
and priors for more accurate depth completion. Extensive experiments
demonstrate the effectiveness of our proposed S2ML method, outperforming the
state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2
and SUN RGB-D datasets, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06033v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06033v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ITPP: Learning Disentangled Event Dynamics in Marked Temporal Point Processes
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Marked Temporal Point Processes (MTPPs) provide a principled framework for
modeling asynchronous event sequences by conditioning on the history of past
events. However, most existing MTPP models rely on channel-mixing strategies
that encode information from different event types into a single, fixed-size
latent representation. This entanglement can obscure type-specific dynamics,
leading to performance degradation and increased risk of overfitting. In this
work, we introduce ITPP, a novel channel-independent architecture for MTPP
modeling that decouples event type information using an encoder-decoder
framework with an ODE-based backbone. Central to ITPP is a type-aware inverted
self-attention mechanism, designed to explicitly model inter-channel
correlations among heterogeneous event types. This architecture enhances
effectiveness and robustness while reducing overfitting. Comprehensive
experiments on multiple real-world and synthetic datasets demonstrate that ITPP
consistently outperforms state-of-the-art MTPP models in both predictive
accuracy and generalization.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06032v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06032v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Video Frame Interpolation (VFI) remains a cornerstone in video enhancement,
enabling temporal upscaling for tasks like slow-motion rendering, frame rate
conversion, and video restoration. While classical methods rely on optical flow
and learning-based models assume access to dense ground-truth, both struggle
with occlusions, domain shifts, and ambiguous motion. This article introduces
MiVID, a lightweight, self-supervised, diffusion-based framework for video
interpolation. Our model eliminates the need for explicit motion estimation by
combining a 3D U-Net backbone with transformer-style temporal attention,
trained under a hybrid masking regime that simulates occlusions and motion
uncertainty. The use of cosine-based progressive masking and adaptive loss
scheduling allows our network to learn robust spatiotemporal representations
without any high-frame-rate supervision. Our framework is evaluated on UCF101-7
and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and
9-frame video segments, making it a low-resource yet highly effective pipeline.
Despite these constraints, our model achieves optimal results at just 50
epochs, competitive with several supervised baselines.This work demonstrates
the power of self-supervised diffusion priors for temporally coherent frame
synthesis and provides a scalable path toward accessible and generalizable VFI
systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06019v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06019v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        One-Shot Knowledge Transfer for Scalable Person Re-Identification
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06016v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06016v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MoSKA: Mixture of Shared KV Attention for Efficient Long-Sequence LLM Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The escalating context length in Large Language Models (LLMs) creates a
severe performance bottleneck around the Key-Value (KV) cache, whose
memory-bound nature leads to significant GPU under-utilization. This paper
introduces Mixture of Shared KV Attention (MoSKA), an architecture that
addresses this challenge by exploiting the heterogeneity of context data. It
differentiates between per-request unique and massively reused shared
sequences. The core of MoSKA is a novel Shared KV Attention mechanism that
transforms the attention on shared data from a series of memory-bound GEMV
operations into a single, compute-bound GEMM by batching concurrent requests.
This is supported by an MoE-inspired sparse attention strategy that prunes the
search space and a tailored Disaggregated Infrastructure that specializes
hardware for unique and shared data. This comprehensive approach demonstrates a
throughput increase of up to 538.7x over baselines in workloads with high
context sharing, offering a clear architectural path toward scalable LLM
inference.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06010v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06010v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05996v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05996v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Revisiting Entropy in Reinforcement Learning for Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
predominant approach for enhancing the reasoning capabilities of large language
models (LLMs). However, the entropy of LLMs usually collapses during RLVR
training, causing premature convergence to suboptimal local minima and hinder
further performance improvement. Although various approaches have been proposed
to mitigate entropy collapse, a comprehensive study of entropy in RLVR remains
lacking. To address this gap, we conduct extensive experiments to investigate
the entropy dynamics of LLMs trained with RLVR and analyze how model entropy
correlates with response diversity, calibration, and performance across various
benchmarks. Our findings reveal that the number of off-policy updates, the
diversity of training data, and the clipping thresholds in the optimization
objective are critical factors influencing the entropy of LLMs trained with
RLVR. Moreover, we theoretically and empirically demonstrate that tokens with
positive advantages are the primary contributors to entropy collapse, and that
model entropy can be effectively regulated by adjusting the relative loss
weights of tokens with positive and negative advantages during training.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05993v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05993v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ontology Learning and Knowledge Graph Construction: A Comparison of Approaches and Their Impact on RAG Performance
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieval-Augmented Generation (RAG) systems combine Large Language Models
(LLMs) with external knowledge, and their performance depends heavily on how
that knowledge is represented. This study investigates how different Knowledge
Graph (KG) construction strategies influence RAG performance. We compare a
variety of approaches: standard vector-based RAG, GraphRAG, and retrieval over
KGs built from ontologies derived either from relational databases or textual
corpora. Results show that ontology-guided KGs incorporating chunk information
achieve competitive performance with state-of-the-art frameworks, substantially
outperforming vector retrieval baselines. Moreover, the findings reveal that
ontology-guided KGs built from relational databases perform competitively to
ones built with ontologies extracted from text, with the benefit of offering a
dual advantage: they require a one-time-only ontology learning process,
substantially reducing LLM usage costs; and avoid the complexity of ontology
merging inherent to text-based approaches.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05991v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05991v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep neural networks (DNNs) are widely used in perception systems for
safety-critical applications, such as autonomous driving and robotics. However,
DNNs remain vulnerable to various safety concerns, including generalization
errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can
lead to hazardous failures. This survey provides a comprehensive overview of
runtime safety monitoring approaches, which operate in parallel to DNNs during
inference to detect these safety concerns without modifying the DNN itself. We
categorize existing methods into three main groups: Monitoring inputs, internal
representations, and outputs. We analyze the state-of-the-art for each
category, identify strengths and limitations, and map methods to the safety
concerns they address. In addition, we highlight open challenges and future
research directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05982v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05982v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Kunlun Anomaly Troubleshooter: Enabling Kernel-Level Anomaly Detection and Causal Reasoning for Large Model Distributed Inference
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Anomaly troubleshooting for large model distributed inference (LMDI) remains
a critical challenge. Resolving anomalies such as inference performance
degradation or latency jitter in distributed system demands significant manual
efforts from domain experts, resulting in extremely time-consuming diagnosis
processes with relatively low accuracy. In this paper, we introduce Kunlun
Anomaly Troubleshooter (KAT), the first anomaly troubleshooting framework
tailored for LMDI. KAT addresses this problem through two core innovations.
First, KAT exploits the synchronicity and consistency of GPU workers,
innovatively leverages function trace data to precisely detect kernel-level
anomalies and associated hardware components at nanosecond resolution. Second,
KAT integrates these detection results into a domain-adapted LLM, delivering
systematic causal reasoning and natural language interpretation of complex
anomaly symptoms. Evaluations conducted in Alibaba Cloud Service production
environment indicate that KAT achieves over 0.884 precision and 0.936 recall in
anomaly detection, providing detail anomaly insights that significantly narrow
down the diagnostic scope and improve both the efficiency and success rate of
troubleshooting.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05978v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05978v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        An Epistemic Perspective on Agent Awareness
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The paper proposes to treat agent awareness as a form of knowledge, breaking
the tradition in the existing literature on awareness. It distinguishes the de
re and de dicto forms of such knowledge. The work introduces two modalities
capturing these forms and formally specifies their meaning using a version of
2D-semantics. The main technical result is a sound and complete logical system
describing the interplay between the two proposed modalities and the standard
"knowledge of the fact" modality.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05977v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05977v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Interpretable Recognition of Cognitive Distortions in Natural Language Texts
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose a new approach to multi-factor classification of natural language
texts based on weighted structured patterns such as N-grams, taking into
account the heterarchical relationships between them, applied to solve such a
socially impactful problem as the automation of detection of specific cognitive
distortions in psychological care, relying on an interpretable, robust and
transparent artificial intelligence model. The proposed recognition and
learning algorithms improve the current state of the art in this field. The
improvement is tested on two publicly available datasets, with significant
improvements over literature-known F1 scores for the task, with optimal
hyper-parameters determined, having code and models available for future use by
the community.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05969v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05969v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DiA-gnostic VLVAE: Disentangled Alignment-Constrained Vision Language Variational AutoEncoder for Robust Radiology Reporting with Missing Modalities
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The integration of medical images with clinical context is essential for
generating accurate and clinically interpretable radiology reports. However,
current automated methods often rely on resource-heavy Large Language Models
(LLMs) or static knowledge graphs and struggle with two fundamental challenges
in real-world clinical data: (1) missing modalities, such as incomplete
clinical context , and (2) feature entanglement, where mixed modality-specific
and shared information leads to suboptimal fusion and clinically unfaithful
hallucinated findings. To address these challenges, we propose the DiA-gnostic
VLVAE, which achieves robust radiology reporting through Disentangled
Alignment. Our framework is designed to be resilient to missing modalities by
disentangling shared and modality-specific features using a Mixture-of-Experts
(MoE) based Vision-Language Variational Autoencoder (VLVAE). A constrained
optimization objective enforces orthogonality and alignment between these
latent representations to prevent suboptimal fusion. A compact LLaMA-X decoder
then uses these disentangled representations to generate reports efficiently.
On the IU X-Ray and MIMIC-CXR datasets, DiA has achieved competetive BLEU@4
scores of 0.266 and 0.134, respectively. Experimental results show that the
proposed method significantly outperforms state-of-the-art models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05968v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05968v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05967v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05967v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Typical detection-free methods for image-to-point cloud registration leverage
transformer-based architectures to aggregate cross-modal features and establish
correspondences. However, they often struggle under challenging conditions,
where noise disrupts similarity computation and leads to incorrect
correspondences. Moreover, without dedicated designs, it remains difficult to
effectively select informative and correlated representations across
modalities, thereby limiting the robustness and accuracy of registration. To
address these challenges, we propose a novel cross-modal registration framework
composed of two key modules: the Iterative Agents Selection (IAS) module and
the Reliable Agents Interaction (RAI) module. IAS enhances structural feature
awareness with phase maps and employs reinforcement learning principles to
efficiently select reliable agents. RAI then leverages these selected agents to
guide cross-modal interactions, effectively reducing mismatches and improving
overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes
benchmarks demonstrate that our method consistently achieves state-of-the-art
performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05965v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05965v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Klear-AgentForge: Forging Agentic Intelligence through Posttraining Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the proliferation of powerful agentic models, the lack of critical
post-training details hinders the development of strong counterparts in the
open-source community. In this study, we present a comprehensive and fully
open-source pipeline for training a high-performance agentic model for
interacting with external tools and environments, named Klear-Qwen3-AgentForge,
starting from the Qwen3-8B base model. We design effective supervised
fine-tuning (SFT) with synthetic data followed by multi-turn reinforcement
learning (RL) to unlock the potential for multiple diverse agentic tasks. We
perform exclusive experiments on various agentic benchmarks in both tool use
and coding domains. Klear-Qwen3-AgentForge-8B achieves state-of-the-art
performance among LLMs of similar size and remains competitive with
significantly larger models.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05951v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05951v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A PDE Perspective on Generative Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Score-based diffusion models have emerged as a powerful class of generative
methods, achieving state-of-the-art performance across diverse domains. Despite
their empirical success, the mathematical foundations of those models remain
only partially understood, particularly regarding the stability and consistency
of the underlying stochastic and partial differential equations governing their
dynamics.
  In this work, we develop a rigorous partial differential equation (PDE)
framework for score-based diffusion processes. Building on the Li--Yau
differential inequality for the heat flow, we prove well-posedness and derive
sharp $L^p$-stability estimates for the associated score-based Fokker--Planck
dynamics, providing a mathematically consistent description of their temporal
evolution. Through entropy stability methods, we further show that the
reverse-time dynamics of diffusion models concentrate on the data manifold for
compactly supported data distributions and a broad class of initialization
schemes, with a concentration rate of order $\sqrt{t}$ as $t \to 0$.
  These results yield a theoretical guarantee that, under exact score guidance,
diffusion trajectories return to the data manifold while preserving imitation
fidelity. Our findings also provide practical insights for designing diffusion
models, including principled criteria for score-function construction, loss
formulation, and stopping-time selection. Altogether, this framework provides a
quantitative understanding of the trade-off between generative capacity and
imitation fidelity, bridging rigorous analysis and model design within a
unified mathematical perspective.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05940v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05940v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        10 Open Challenges Steering the Future of Vision-Language-Action Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Due to their ability of follow natural language instructions,
vision-language-action (VLA) models are increasingly prevalent in the embodied
AI arena, following the widespread success of their precursors -- LLMs and
VLMs. In this paper, we discuss 10 principal milestones in the ongoing
development of VLA models -- multimodality, reasoning, data, evaluation,
cross-robot action generalization, efficiency, whole-body coordination, safety,
agents, and coordination with humans. Furthermore, we discuss the emerging
trends of using spatial understanding, modeling world dynamics, post training,
and data synthesis -- all aiming to reach these milestones. Through these
discussions, we hope to bring attention to the research avenues that may
accelerate the development of VLA models into wider acceptability.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05936v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05936v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reinforcement Learning Improves Traversal of Hierarchical Knowledge in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning (RL) is often credited with improving language model
reasoning and generalization at the expense of degrading memorized knowledge.
We challenge this narrative by observing that RL-enhanced models consistently
outperform their base and supervised fine-tuned (SFT) counterparts on pure
knowledge recall tasks, particularly those requiring traversal of hierarchical,
structured knowledge (e.g., medical codes). We hypothesize these gains stem not
from newly acquired data, but from improved procedural skills in navigating and
searching existing knowledge hierarchies within the model parameters. To
support this hypothesis, we show that structured prompting, which explicitly
guides SFTed models through hierarchical traversal, recovers most of the
performance gap (reducing 24pp to 7pp on MedConceptsQA for DeepSeek-V3/R1). We
further find that while prompting improves final-answer accuracy, RL-enhanced
models retain superior ability to recall correct procedural paths on
deep-retrieval tasks. Finally our layer-wise internal activation analysis
reveals that while factual representations (e.g., activations for the statement
"code 57.95 refers to urinary infection") maintain high cosine similarity
between SFT and RL models, query representations (e.g., "what is code 57.95")
diverge noticeably, indicating that RL primarily transforms how models traverse
knowledge rather than the knowledge representation itself.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05933v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05933v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Future of AI in the GCC Post-NPM Landscape: A Comparative Analysis of Kuwait and the UAE
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Comparative evidence on how Gulf Cooperation Council (GCC) states turn
artificial intelligence (AI) ambitions into post--New Public Management
(post-NPM) outcomes is scarce because most studies examine Western democracies.
We analyze constitutional, collective-choice, and operational rules shaping AI
uptake in two contrasting GCC members, the United Arab Emirates (UAE) and
Kuwait, and whether they foster citizen centricity, collaborative governance,
and public value creation. Anchored in Ostrom's Institutional Analysis and
Development framework, the study combines a most similar/most different systems
design with multiple sources: 62 public documents from 2018--2025, embedded UAE
cases (Smart Dubai and MBZUAI), and 39 interviews with officials conducted Aug
2024--May 2025. Dual coding and process tracing connect rule configurations to
AI performance. Cross-case analysis identifies four reinforcing mechanisms
behind divergent trajectories. In the UAE, concentrated authority, credible
sanctions, pro-innovation narratives, and flexible reinvestment rules scale
pilots into hundreds of services and sizable recycled savings. In Kuwait,
dispersed veto points, exhortative sanctions, cautious discourse, and lapsed AI
budgets confine initiatives to pilot mode despite equivalent fiscal resources.
The findings refine institutional theory by showing that vertical rule
coherence, not wealth, determines AI's public-value yield, and temper post-NPM
optimism by revealing that efficiency metrics serve societal goals only when
backed by enforceable safeguards. To curb ethics washing and test
transferability beyond the GCC, future work should track rule diffusion over
time, develop blended legitimacy--efficiency scorecards, and examine how
narrative framing shapes citizen consent for data sharing.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05932v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05932v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Self-Abstraction from Grounded Experience for Plan-Guided Policy Refinement
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language model (LLM) based agents are increasingly used to tackle
software engineering tasks that require multi-step reasoning and code
modification, demonstrating promising yet limited performance. However, most
existing LLM agents typically operate within static execution frameworks,
lacking a principled mechanism to learn and self-improve from their own
experience and past rollouts. As a result, their performance remains bounded by
the initial framework design and the underlying LLM's capabilities. We propose
Self-Abstraction from Grounded Experience (SAGE), a framework that enables
agents to learn from their own task executions and refine their behavior
through self-abstraction. After an initial rollout, the agent induces a concise
plan abstraction from its grounded experience, distilling key steps,
dependencies, and constraints. This learned abstraction is then fed back as
contextual guidance, refining the agent's policy and supporting more
structured, informed subsequent executions. Empirically, SAGE delivers
consistent performance gains across diverse LLM backbones and agent
architectures. Notably, it yields a 7.2% relative performance improvement over
the strong Mini-SWE-Agent baseline when paired with the GPT-5 (high) backbone.
SAGE further achieves strong overall performance on SWE-Bench Verified
benchmark, reaching 73.2% and 74% Pass@1 resolve rates with the Mini-SWE-Agent
and OpenHands CodeAct agent framework, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05931v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05931v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05929v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05929v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Artificial intelligence and the Gulf Cooperation Council workforce adapting to the future of work
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid expansion of artificial intelligence (AI) in the Gulf Cooperation
Council (GCC) raises a central question: are investments in compute
infrastructure matched by an equally robust build-out of skills, incentives,
and governance? Grounded in socio-technical systems (STS) theory, this
mixed-methods study audits workforce preparedness across Kingdom of Saudi
Arabia (KSA), the United Arab Emirates (UAE), Qatar, Kuwait, Bahrain, and Oman.
We combine term frequency--inverse document frequency (TF--IDF) analysis of six
national AI strategies (NASs), an inventory of 47 publicly disclosed AI
initiatives (January 2017--April 2025), paired case studies, the Mohamed bin
Zayed University of Artificial Intelligence (MBZUAI) and the Saudi Data &
Artificial Intelligence Authority (SDAIA) Academy, and a scenario matrix
linking oil-revenue slack (technical capacity) to regulatory coherence (social
alignment). Across the corpus, 34/47 initiatives (0.72; 95% Wilson CI
0.58--0.83) exhibit joint social--technical design; country-level indices span
0.57--0.90 (small n; intervals overlap). Scenario results suggest that, under
our modeled conditions, regulatory convergence plausibly binds outcomes more
than fiscal capacity: fragmented rules can offset high oil revenues, while
harmonized standards help preserve progress under austerity. We also identify
an emerging two-track talent system, research elites versus rapidly trained
practitioners, that risks labor-market bifurcation without bridging mechanisms.
By extending STS inquiry to oil-rich, state-led economies, the study refines
theory and sets a research agenda focused on longitudinal coupling metrics,
ethnographies of coordination, and outcome-based performance indicators.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05927v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05927v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        IDALC: A Semi-Supervised Framework for Intent Detection and Active Learning based Correction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Voice-controlled dialog systems have become immensely popular due to their
ability to perform a wide range of actions in response to diverse user queries.
These agents possess a predefined set of skills or intents to fulfill specific
user tasks. But every system has its own limitations. There are instances
where, even for known intents, if any model exhibits low confidence, it results
in rejection of utterances that necessitate manual annotation. Additionally, as
time progresses, there may be a need to retrain these agents with new intents
from the system-rejected queries to carry out additional tasks. Labeling all
these emerging intents and rejected utterances over time is impractical, thus
calling for an efficient mechanism to reduce annotation costs. In this paper,
we introduce IDALC (Intent Detection and Active Learning based Correction), a
semi-supervised framework designed to detect user intents and rectify
system-rejected utterances while minimizing the need for human annotation.
Empirical findings on various benchmark datasets demonstrate that our system
surpasses baseline methods, achieving a 5-10% higher accuracy and a 4-8%
improvement in macro-F1. Remarkably, we maintain the overall annotation cost at
just 6-10% of the unlabelled data available to the system. The overall
framework of IDALC is shown in Fig. 1
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05921v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05921v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        IoT-based Fresh Produce Supply Chain Under Uncertainty: An Adaptive Optimization Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Fruits and vegetables form a vital component of the global economy; however,
their distribution poses complex logistical challenges due to high
perishability, supply fluctuations, strict quality and safety standards, and
environmental sensitivity. In this paper, we propose an adaptive optimization
model that accounts for delays, travel time, and associated temperature changes
impacting produce shelf life, and compare it against traditional approaches
such as Robust Optimization, Distributionally Robust Optimization, and
Stochastic Programming. Additionally, we conduct a series of computational
experiments using Internet of Things (IoT) sensor data to evaluate the
performance of our proposed model. Our study demonstrates that the proposed
adaptive model achieves a higher shelf life, extending it by over 18\% compared
to traditional optimization models, by dynamically mitigating temperature
deviations through a temperature feedback mechanism. The promising results
demonstrate the potential of this approach to improve both the freshness and
efficiency of logistics systems an aspect often neglected in previous works.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05920v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05920v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Injecting Falsehoods: Adversarial Man-in-the-Middle Attacks Undermining Factual Recall in LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    LLMs are now an integral part of information retrieval. As such, their role
as question answering chatbots raises significant concerns due to their shown
vulnerability to adversarial man-in-the-middle (MitM) attacks. Here, we propose
the first principled attack evaluation on LLM factual memory under prompt
injection via Xmera, our novel, theory-grounded MitM framework. By perturbing
the input given to "victim" LLMs in three closed-book and fact-based QA
settings, we undermine the correctness of the responses and assess the
uncertainty of their generation process. Surprisingly, trivial
instruction-based attacks report the highest success rate (up to ~85.3%) while
simultaneously having a high uncertainty for incorrectly answered questions. To
provide a simple defense mechanism against Xmera, we train Random Forest
classifiers on the response uncertainty levels to distinguish between attacked
and unattacked queries (average AUC of up to ~96%). We believe that signaling
users to be cautious about the answers they receive from black-box and
potentially corrupt LLMs is a first checkpoint toward user cyberspace safety.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05919v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05919v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        NILC: Discovering New Intents with LLM-assisted Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    New intent discovery (NID) seeks to recognize both new and known intents from
unlabeled user utterances, which finds prevalent use in practical dialogue
systems. Existing works towards NID mainly adopt a cascaded architecture,
wherein the first stage focuses on encoding the utterances into informative
text embeddings beforehand, while the latter is to group similar embeddings
into clusters (i.e., intents), typically by K-Means. However, such a cascaded
pipeline fails to leverage the feedback from both steps for mutual refinement,
and, meanwhile, the embedding-only clustering overlooks nuanced textual
semantics, leading to suboptimal performance. To bridge this gap, this paper
proposes NILC, a novel clustering framework specially catered for effective
NID. Particularly, NILC follows an iterative workflow, in which clustering
assignments are judiciously updated by carefully refining cluster centroids and
text embeddings of uncertain utterances with the aid of large language models
(LLMs). Specifically, NILC first taps into LLMs to create additional semantic
centroids for clusters, thereby enriching the contextual semantics of the
Euclidean centroids of embeddings. Moreover, LLMs are then harnessed to augment
hard samples (ambiguous or terse utterances) identified from clusters via
rewriting for subsequent cluster correction. Further, we inject supervision
signals through non-trivial techniques seeding and soft must links for more
accurate NID in the semi-supervised setting. Extensive experiments comparing
NILC against multiple recent baselines under both unsupervised and
semi-supervised settings showcase that NILC can achieve significant performance
improvements over six benchmark datasets of diverse domains consistently.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05913v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05913v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Imperfect Learner: Incorporating Developmental Trajectories in Memory-based Student Simulation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    User simulation is important for developing and evaluating human-centered AI,
yet current student simulation in educational applications has significant
limitations. Existing approaches focus on single learning experiences and do
not account for students' gradual knowledge construction and evolving skill
sets. Moreover, large language models are optimized to produce direct and
accurate responses, making it challenging to represent the incomplete
understanding and developmental constraints that characterize real learners. In
this paper, we introduce a novel framework for memory-based student simulation
that incorporates developmental trajectories through a hierarchical memory
mechanism with structured knowledge representation. The framework also
integrates metacognitive processes and personality traits to enrich the
individual learner profiling, through dynamical consolidation of both cognitive
development and personal learning characteristics. In practice, we implement a
curriculum-aligned simulator grounded on the Next Generation Science Standards.
Experimental results show that our approach can effectively reflect the gradual
nature of knowledge development and the characteristic difficulties students
face, providing a more accurate representation of learning processes.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05903v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05903v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Retrieval-Augmented Generation in Medicine: A Scoping Review of Technical Implementations, Clinical Applications, and Ethical Considerations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid growth of medical knowledge and increasing complexity of clinical
practice pose challenges. In this context, large language models (LLMs) have
demonstrated value; however, inherent limitations remain. Retrieval-augmented
generation (RAG) technologies show potential to enhance their clinical
applicability. This study reviewed RAG applications in medicine. We found that
research primarily relied on publicly available data, with limited application
in private data. For retrieval, approaches commonly relied on English-centric
embedding models, while LLMs were mostly generic, with limited use of
medical-specific LLMs. For evaluation, automated metrics evaluated generation
quality and task performance, whereas human evaluation focused on accuracy,
completeness, relevance, and fluency, with insufficient attention to bias and
safety. RAG applications were concentrated on question answering, report
generation, text summarization, and information extraction. Overall, medical
RAG remains at an early stage, requiring advances in clinical validation,
cross-linguistic adaptation, and support for low-resource settings to enable
trustworthy and responsible global use.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05901v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05901v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the effectiveness of quantization-aware training (QAT) in compressing
deep neural networks, its performance on multi-task architectures often
degrades significantly due to task-specific feature discrepancies and gradient
conflicts. To address these challenges, we propose Gradient-Aware Balanced
Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and
fuses task-specific features in a quantization-friendly manner. We further
introduce Attention Distribution Alignment (ADA), a feature-level distillation
strategy tailored for quantized models. Our method demonstrates strong
generalization across network architectures and QAT algorithms, with
theoretical guarantees on gradient bias reduction. Extensive experiments
demonstrate that our strategy consistently enhances a variety of QAT methods
across different network architectures and bit-widths. On PASCAL VOC and COCO
datasets, the proposed approach achieves average mAP improvements of
approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit
quantization, our method narrows the accuracy gap with the full-precision model
to only 1.7% on VOC, showcasing its effectiveness in preserving performance
under low-bit constraints. Notably, the proposed framework is modular, easy to
integrate, and compatible with any existing QAT technique-enhancing the
performance of quantized models without requiring modifications to the original
network architecture.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05898v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05898v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Remarkably Efficient Paradigm to Multimodal Large Language Models for Sequential Recommendation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this paper, we proposed Speeder, a remarkably efficient paradigm to
multimodal large language models for sequential recommendation. Speeder
introduces 3 key components: (1) Multimodal Representation Compression (MRC),
which efficiently reduces redundancy in item descriptions; (2) Sequential
Position Awareness Enhancement (SPAE), which strengthens the model's ability to
capture complex sequential dependencies; (3) Modality-aware Progressive
Optimization (MPO), which progressively integrates different modalities to
improve the model's understanding and reduce cognitive biases. Through
extensive experiments, Speeder demonstrates superior performance over baselines
in terms of VHR@1 and computational efficiency. Specifically, Speeder achieved
250% of the training speed and 400% of the inference speed compared to the
state-of-the-art MLLM-based SR models. Future work could focus on incorporating
real-time feedback from real-world systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05885v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05885v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Unveiling Modality Bias: Automated Sample-Specific Analysis for Multimodal Misinformation Benchmarks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Numerous multimodal misinformation benchmarks exhibit bias toward specific
modalities, allowing detectors to make predictions based solely on one
modality. While previous research has quantified bias at the dataset level or
manually identified spurious correlations between modalities and labels, these
approaches lack meaningful insights at the sample level and struggle to scale
to the vast amount of online information. In this paper, we investigate the
design for automated recognition of modality bias at the sample level.
Specifically, we propose three bias quantification methods based on
theories/views of different levels of granularity: 1) a coarse-grained
evaluation of modality benefit; 2) a medium-grained quantification of
information flow; and 3) a fine-grained causality analysis. To verify the
effectiveness, we conduct a human evaluation on two popular benchmarks.
Experimental results reveal three interesting findings that provide potential
direction toward future research: 1)~Ensembling multiple views is crucial for
reliable automated analysis; 2)~Automated analysis is prone to detector-induced
fluctuations; and 3)~Different views produce a higher agreement on
modality-balanced samples but diverge on biased ones.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05883v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05883v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Physics-Informed Neural Networks for Real-Time Gas Crossover Prediction in PEM Electrolyzers: First Application with Multi-Membrane Validation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Green hydrogen production via polymer electrolyte membrane (PEM) water
electrolysis is pivotal for energy transition, yet hydrogen crossover through
membranes threatens safety and economic viability-approaching explosive limits
(4 mol% H$_2$ in O$_2$) while reducing Faradaic efficiency by 2.5%. Current
physics-based models require extensive calibration and computational resources
that preclude real-time implementation, while purely data-driven approaches
fail to extrapolate beyond training conditions-critical for dynamic
electrolyzer operation. Here we present the first application of
physics-informed neural networks (PINNs) for hydrogen crossover prediction,
integrating mass conservation, Fick's diffusion law, and Henry's solubility law
within a compact architecture (17,793 parameters). Validated across six
membranes under industrially relevant conditions (0.05-5.0 A/cm$^2$, 1-200 bar,
25-85{\deg}C), our PINN achieves exceptional accuracy (R$^2$ = 99.84%, RMSE =
0.0348%) with sub-millisecond inference times suitable for real-time control.
Remarkably, the model maintains R$^2$ > 86% when predicting crossover at
pressures 2.5x beyond training range-substantially outperforming pure neural
networks (R$^2$ = 43.4%). The hardware-agnostic deployment, from desktop CPUs
to edge devices (Raspberry Pi 4), enables distributed safety monitoring
essential for gigawatt-scale installations. By bridging physical rigor and
computational efficiency, this work establishes a new paradigm for real-time
electrolyzer monitoring, accelerating deployment of safe, efficient green
hydrogen infrastructure crucial for net-zero emissions targets.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05879v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05879v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Social platforms connect billions of people, yet their engagement-first
algorithms often work on users rather than with them, amplifying stress,
misinformation, and a loss of control. We propose Human-Layer AI
(HL-AI)--user-owned, explainable intermediaries that sit in the browser between
platform logic and the interface. HL-AI gives people practical,
moment-to-moment control without requiring platform cooperation. We contribute
a working Chrome/Edge prototype implementing five representative pattern
frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed
Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying
mathematical formulation balancing user utility, autonomy costs, and risk
thresholds. Evaluation spans technical accuracy, usability, and behavioral
outcomes. The result is a suite of humane controls that help users rewrite
before harm, read with integrity cues, tune feeds with intention, pause
compulsive loops, and seek shelter during harassment, all while preserving
agency through explanations and override options. This prototype offers a
practical path to retrofit today's feeds with safety, agency, and well-being,
inviting rigorous cross-cultural user evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05875v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05875v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        An Empirical Study of Reasoning Steps in Thinking Code LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Thinking Large Language Models (LLMs) generate explicit intermediate
reasoning traces before final answers, potentially improving transparency,
interpretability, and solution accuracy for code generation. However, the
quality of these reasoning chains remains underexplored. We present a
comprehensive empirical study examining the reasoning process and quality of
thinking LLMs for code generation. We evaluate six state-of-the-art reasoning
LLMs (DeepSeek-R1, OpenAI-o3-mini, Claude-3.7-Sonnet-Thinking,
Gemini-2.0-Flash-Thinking, Gemini-2.5-Flash, and Qwen-QwQ) across 100 code
generation tasks of varying difficulty from BigCodeBench. We quantify
reasoning-chain structure through step counts and verbosity, conduct controlled
step-budget adjustments, and perform a 21-participant human evaluation across
three dimensions: efficiency, logical correctness, and completeness. Our
step-count interventions reveal that targeted step increases can improve
resolution rates for certain models/tasks, while modest reductions often
preserve success on standard tasks, rarely on hard ones. Through systematic
analysis, we develop a reasoning-problematic taxonomy, identifying completeness
as the dominant failure mode. Task complexity significantly impacts reasoning
quality; hard problems are substantially more prone to incompleteness than
standard tasks. Our stability analysis demonstrates that thinking LLMs maintain
consistent logical structures across computational effort levels and can
self-correct previous errors. This study provides new insights into the
strengths and limitations of current thinking LLMs in software engineering.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05874v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05874v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Endoscopic images often suffer from diverse and co-occurring degradations
such as low lighting, smoke, and bleeding, which obscure critical clinical
details. Existing restoration methods are typically task-specific and often
require prior knowledge of the degradation type, limiting their robustness in
real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic
diffusion-based framework that restores multiple degradation types using a
single model. EndoIR introduces a Dual-Domain Prompter that extracts joint
spatial-frequency features, coupled with an adaptive embedding that encodes
both shared and task-specific cues as conditioning for denoising. To mitigate
feature confusion in conventional concatenation-based conditioning, we design a
Dual-Stream Diffusion architecture that processes clean and degraded inputs
separately, with a Rectified Fusion Block integrating them in a structured,
degradation-aware manner. Furthermore, Noise-Aware Routing Block improves
efficiency by dynamically selecting only noise-relevant features during
denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR
achieves state-of-the-art performance across multiple degradation scenarios
while using fewer parameters than strong baselines, and downstream segmentation
experiments confirm its clinical utility.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05873v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05873v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptation and Fine-tuning with TabPFN for Travelling Salesman Problem
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Tabular Prior-Data Fitted Network (TabPFN) is a foundation model designed for
small to medium-sized tabular data, which has attracted much attention
recently. This paper investigates the application of TabPFN in Combinatorial
Optimization (CO) problems. The aim is to lessen challenges in time and
data-intensive training requirements often observed in using traditional
methods including exact and heuristic algorithms, Machine Learning (ML)-based
models, to solve CO problems. Proposing possibly the first ever application of
TabPFN for such a purpose, we adapt and fine-tune the TabPFN model to solve the
Travelling Salesman Problem (TSP), one of the most well-known CO problems.
Specifically, we adopt the node-based approach and the node-predicting
adaptation strategy to construct the entire TSP route. Our evaluation with
varying instance sizes confirms that TabPFN requires minimal training, adapts
to TSP using a single sample, performs better generalization across varying TSP
instance sizes, and reduces performance degradation. Furthermore, the training
process with adaptation and fine-tuning is completed within minutes. The
methodology leads to strong solution quality even without post-processing and
achieves performance comparable to other models with post-processing
refinement. Our findings suggest that the TabPFN model is a promising approach
to solve structured and CO problems efficiently under training resource
constraints and rapid deployment requirements.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05872v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05872v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CGCE: Classifier-Guided Concept Erasure in Generative Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05865v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05865v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EMOD: A Unified EEG Emotion Representation Framework Leveraging V-A Guided Contrastive Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Emotion recognition from EEG signals is essential for affective computing and
has been widely explored using deep learning. While recent deep learning
approaches have achieved strong performance on single EEG emotion datasets,
their generalization across datasets remains limited due to the heterogeneity
in annotation schemes and data formats. Existing models typically require
dataset-specific architectures tailored to input structure and lack semantic
alignment across diverse emotion labels. To address these challenges, we
propose EMOD: A Unified EEG Emotion Representation Framework Leveraging
Valence-Arousal (V-A) Guided Contrastive Learning. EMOD learns transferable and
emotion-aware representations from heterogeneous datasets by bridging both
semantic and structural gaps. Specifically, we project discrete and continuous
emotion labels into a unified V-A space and formulate a soft-weighted
supervised contrastive loss that encourages emotionally similar samples to
cluster in the latent space. To accommodate variable EEG formats, EMOD employs
a flexible backbone comprising a Triple-Domain Encoder followed by a
Spatial-Temporal Transformer, enabling robust extraction and integration of
temporal, spectral, and spatial features. We pretrain EMOD on eight public EEG
datasets and evaluate its performance on three benchmark datasets. Experimental
results show that EMOD achieves state-of-the-art performance, demonstrating
strong adaptability and generalization across diverse EEG-based emotion
recognition scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05863v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05863v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Predicting the Future by Retrieving the Past
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep learning models such as MLP, Transformer, and TCN have achieved
remarkable success in univariate time series forecasting, typically relying on
sliding window samples from historical data for training. However, while these
models implicitly compress historical information into their parameters during
training, they are unable to explicitly and dynamically access this global
knowledge during inference, relying only on the local context within the
lookback window. This results in an underutilization of rich patterns from the
global history. To bridge this gap, we propose Predicting the Future by
Retrieving the Past (PFRP), a novel approach that explicitly integrates global
historical data to enhance forecasting accuracy. Specifically, we construct a
Global Memory Bank (GMB) to effectively store and manage global historical
patterns. A retrieval mechanism is then employed to extract similar patterns
from the GMB, enabling the generation of global predictions. By adaptively
combining these global predictions with the outputs of any local prediction
model, PFRP produces more accurate and interpretable forecasts. Extensive
experiments conducted on seven real-world datasets demonstrate that PFRP
significantly enhances the average performance of advanced univariate
forecasting models by 8.4\%. Codes can be found in
https://github.com/ddz16/PFRP.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05859v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05859v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Can a Small Model Learn to Look Before It Leaps? Dynamic Learning and Proactive Correction for Hallucination Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Hallucination in large language models (LLMs) remains a critical barrier to
their safe deployment. Existing tool-augmented hallucination detection methods
require pre-defined fixed verification strategies, which are crucial to the
quality and effectiveness of tool calls. Some methods directly employ powerful
closed-source LLMs such as GPT-4 as detectors, which are effective but too
costly. To mitigate the cost issue, some methods adopt the teacher-student
architecture and finetune open-source small models as detectors via agent
tuning. However, these methods are limited by fixed strategies. When faced with
a dynamically changing execution environment, they may lack adaptability and
inappropriately call tools, ultimately leading to detection failure. To address
the problem of insufficient strategy adaptability, we propose the innovative
``Learning to Evaluate and Adaptively Plan''(LEAP) framework, which endows an
efficient student model with the dynamic learning and proactive correction
capabilities of the teacher model. Specifically, our method formulates the
hallucination detection problem as a dynamic strategy learning problem. We
first employ a teacher model to generate trajectories within the dynamic
learning loop and dynamically adjust the strategy based on execution failures.
We then distill this dynamic planning capability into an efficient student
model via agent tuning. Finally, during strategy execution, the student model
adopts a proactive correction mechanism, enabling it to propose, review, and
optimize its own verification strategies before execution. We demonstrate
through experiments on three challenging benchmarks that our LEAP-tuned model
outperforms existing state-of-the-art methods.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05854v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05854v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Quantifying Edits Decay in Fine-tuned LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Knowledge editing has emerged as a lightweight alternative to retraining for
correcting or injecting specific facts in large language models (LLMs).
Meanwhile, fine-tuning remains the default operation for adapting LLMs to new
domains and tasks. Despite their widespread adoption, these two post-training
interventions have been studied in isolation, leaving open a crucial question:
if we fine-tune an edited model, do the edits survive? This question is
motivated by two practical scenarios: removing covert or malicious edits, and
preserving beneficial edits. If fine-tuning impairs edits as shown in Figure 1,
current KE methods become less useful, as every fine-tuned model would require
re-editing, which significantly increases the cost; if edits persist,
fine-tuned models risk propagating hidden malicious edits, raising serious
safety concerns. To this end, we systematically quantify edits decay after
fine-tuning, investigating how fine-tuning affects knowledge editing. We
evaluate two state-of-the-art editing methods (MEMIT, AlphaEdit) and three
fine-tuning approaches (full-parameter, LoRA, DoRA) across five LLMs and three
datasets, yielding 232 experimental configurations. Our results show that edits
decay after fine-tuning, with survival varying across configurations, e.g.,
AlphaEdit edits decay more than MEMIT edits. Further, we propose
selective-layer fine-tuning and find that fine-tuning edited layers only can
effectively remove edits, though at a slight cost to downstream performance.
Surprisingly, fine-tuning non-edited layers impairs more edits than full
fine-tuning. Overall, our study establishes empirical baselines and actionable
strategies for integrating knowledge editing with fine-tuning, and underscores
that evaluating model editing requires considering the full LLM application
pipeline.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05852v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05852v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Retrieval Quality at Context Limit
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The ability of large language models (LLMs) to recall and retrieve
information from long contexts is critical for many real-world applications.
Prior work (Liu et al., 2023) reported that LLMs suffer significant drops in
retrieval accuracy for facts placed in the middle of large contexts, an effect
known as "Lost in the Middle" (LITM). We find the model Gemini 2.5 Flash can
answer needle-in-a-haystack questions with great accuracy regardless of
document position including when the document is nearly at the input context
limit. Our results suggest that the "Lost in the Middle" effect is not present
for simple factoid Q\&A in Gemini 2.5 Flash, indicating substantial
improvements in long-context retrieval.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05850v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05850v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Symbolic regression seeks to uncover physical laws from experimental data by
searching for closed-form expressions, which is an important task in AI-driven
scientific discovery. Yet the exponential growth of the search space of
expression renders the task computationally challenging. A promising yet
underexplored direction for reducing the effective search space and
accelerating training lies in symbolic equivalence: many expressions, although
syntactically different, define the same function -- for example,
$\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$.
Existing algorithms treat such variants as distinct outputs, leading to
redundant exploration and slow learning. We introduce EGG-SR, a unified
framework that integrates equality graphs (e-graphs) into diverse symbolic
regression algorithms, including Monte Carlo Tree Search (MCTS), deep
reinforcement learning (DRL), and large language models (LLMs). EGG-SR
compactly represents equivalent expressions through the proposed EGG module,
enabling more efficient learning by: (1) pruning redundant subtree exploration
in EGG-MCTS, (2) aggregating rewards across equivalence classes in EGG-DRL, and
(3) enriching feedback prompts in EGG-LLM. Under mild assumptions, we show that
embedding e-graphs tightens the regret bound of MCTS and reduces the variance
of the DRL gradient estimator. Empirically, EGG-SR consistently enhances
multiple baselines across challenging benchmarks, discovering equations with
lower normalized mean squared error than state-of-the-art methods. Code
implementation is available at: https://www.github.com/jiangnanhugo/egg-sr.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05849v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05849v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Diffusion Model Guidance through Calibration and Regularization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05844v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05844v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05841v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05841v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hilbert-Guided Block-Sparse Local Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05832v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05832v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Policy Gradient-Based EMT-in-the-Loop Learning to Mitigate Sub-Synchronous Control Interactions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This paper explores the development of learning-based tunable control gains
using EMT-in-the-loop simulation framework (e.g., PSCAD interfaced with
Python-based learning modules) to address critical sub-synchronous
oscillations. Since sub-synchronous control interactions (SSCI) arise from the
mis-tuning of control gains under specific grid configurations, effective
mitigation strategies require adaptive re-tuning of these gains. Such
adaptiveness can be achieved by employing a closed-loop, learning-based
framework that considers the grid conditions responsible for such
sub-synchronous oscillations. This paper addresses this need by adopting
methodologies inspired by Markov decision process (MDP) based reinforcement
learning (RL), with a particular emphasis on simpler deep policy gradient
methods with additional SSCI-specific signal processing modules such as
down-sampling, bandpass filtering, and oscillation energy dependent reward
computations. Our experimentation in a real-world event setting demonstrates
that the deep policy gradient based trained policy can adaptively compute gain
settings in response to varying grid conditions and optimally suppress control
interaction-induced oscillations.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05822v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05822v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        WAR-Re: Web API Recommendation with Semantic Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the development of cloud computing, the number of Web APIs has increased
dramatically, further intensifying the demand for efficient Web API
recommendation. Despite the demonstrated success of previous Web API
recommendation solutions, two critical challenges persist: 1) a fixed top-N
recommendation that cannot accommodate the varying API cardinality requirements
of different mashups, and 2) these methods output only ranked API lists without
accompanying reasons, depriving users of understanding the recommendation. To
address these challenges, we propose WAR-Re, an LLM-based model for Web API
recommendation with semantic reasoning for justification. WAR-Re leverages
special start and stop tokens to handle the first challenge and uses two-stage
training: supervised fine-tuning and reinforcement learning via Group Relative
Policy Optimization (GRPO) to enhance the model's ability in both tasks.
Comprehensive experimental evaluations on the ProgrammableWeb dataset
demonstrate that WAR-Re achieves a gain of up to 21.59\% over the
state-of-the-art baseline model in recommendation accuracy, while consistently
producing high-quality semantic reasons for recommendations.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05820v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05820v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        In-depth Analysis on Caching and Pre-fetching in Mixture of Experts Offloading
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In today's landscape, Mixture of Experts (MoE) is a crucial architecture that
has been used by many of the most advanced models. One of the major challenges
of MoE models is that they usually require much more memory than their dense
counterparts due to their unique architecture, and hence are harder to deploy
in environments with limited GPU memory, such as edge devices. MoE offloading
is a promising technique proposed to overcome this challenge, especially if it
is enhanced with caching and pre-fetching, but prior work stopped at suboptimal
caching algorithm and offered limited insights. In this work, we study MoE
offloading in depth and make the following contributions: 1. We analyze the
expert activation and LRU caching behavior in detail and provide traces. 2. We
propose LFU caching optimization based on our analysis and obtain strong
improvements from LRU. 3. We implement and experiment speculative expert
pre-fetching, providing detailed trace showing its huge potential . 4. In
addition, our study extensively covers the behavior of the MoE architecture
itself, offering information on the characteristic of the gating network and
experts. This can inspire future work on the interpretation of MoE models and
the development of pruning techniques for MoE architecture with minimal
performance loss.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05814v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05814v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MOSS: Efficient and Accurate FP8 LLM Training with Microscaling and Automatic Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Training large language models with FP8 formats offers significant efficiency
gains. However, the reduced numerical precision of FP8 poses challenges for
stable and accurate training. Current frameworks preserve training performance
using mixed-granularity quantization, i.e., applying per-group quantization for
activations and per-tensor/block quantization for weights. While effective,
per-group quantization requires scaling along the inner dimension of matrix
multiplication, introducing additional dequantization overhead. Moreover, these
frameworks often rely on just-in-time scaling to dynamically adjust scaling
factors based on the current data distribution. However, this online
quantization is inefficient for FP8 training, as it involves multiple memory
reads and writes that negate the performance benefits of FP8. To overcome these
limitations, we propose MOSS, a novel FP8 training framework that ensures both
efficiency and numerical stability. MOSS introduces two key innovations: (1) a
two-level microscaling strategy for quantizing sensitive activations, which
balances precision and dequantization cost by combining a high-precision global
scale with compact, power-of-two local scales; and (2) automatic scaling for
weights in linear layers, which eliminates the need for costly max-reduction
operations by predicting and adjusting scaling factors during training.
Leveraging these techniques, MOSS enables efficient FP8 training of a 7B
parameter model, achieving performance comparable to the BF16 baseline while
achieving up to 34% higher training throughput.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05811v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05811v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DiagnoLLM: A Hybrid Bayesian Neural Language Framework for Interpretable Disease Diagnosis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Building trustworthy clinical AI systems requires not only accurate
predictions but also transparent, biologically grounded explanations. We
present \texttt{DiagnoLLM}, a hybrid framework that integrates Bayesian
deconvolution, eQTL-guided deep learning, and LLM-based narrative generation
for interpretable disease diagnosis. DiagnoLLM begins with GP-unmix, a Gaussian
Process-based hierarchical model that infers cell-type-specific gene expression
profiles from bulk and single-cell RNA-seq data while modeling biological
uncertainty. These features, combined with regulatory priors from eQTL
analysis, power a neural classifier that achieves high predictive performance
in Alzheimer's Disease (AD) detection (88.0\% accuracy). To support human
understanding and trust, we introduce an LLM-based reasoning module that
translates model outputs into audience-specific diagnostic reports, grounded in
clinical features, attribution signals, and domain knowledge. Human evaluations
confirm that these reports are accurate, actionable, and appropriately tailored
for both physicians and patients. Our findings show that LLMs, when deployed as
post-hoc reasoners rather than end-to-end predictors, can serve as effective
communicators within hybrid diagnostic pipelines.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05810v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05810v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Measuring Model Performance in the Presence of an Intervention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    AI models are often evaluated based on their ability to predict the outcome
of interest. However, in many AI for social impact applications, the presence
of an intervention that affects the outcome can bias the evaluation. Randomized
controlled trials (RCTs) randomly assign interventions, allowing data from the
control group to be used for unbiased model evaluation. However, this approach
is inefficient because it ignores data from the treatment group. Given the
complexity and cost often associated with RCTs, making the most use of the data
is essential. Thus, we investigate model evaluation strategies that leverage
all data from an RCT. First, we theoretically quantify the estimation bias that
arises from na\"ively aggregating performance estimates from treatment and
control groups, and derive the condition under which this bias leads to
incorrect model selection. Leveraging these theoretical insights, we propose
nuisance parameter weighting (NPW), an unbiased model evaluation approach that
reweights data from the treatment group to mimic the distributions of samples
that would or would not experience the outcome under no intervention. Using
synthetic and real-world datasets, we demonstrate that our proposed evaluation
approach consistently yields better model selection than the standard approach,
which ignores data from the treatment group, across various intervention effect
and sample size settings. Our contribution represents a meaningful step towards
more efficient model evaluation in real-world contexts.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05805v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05805v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond the Lower Bound: Bridging Regret Minimization and Best Arm Identification in Lexicographic Bandits
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In multi-objective decision-making with hierarchical preferences,
lexicographic bandits provide a natural framework for optimizing multiple
objectives in a prioritized order. In this setting, a learner repeatedly
selects arms and observes reward vectors, aiming to maximize the reward for the
highest-priority objective, then the next, and so on. While previous studies
have primarily focused on regret minimization, this work bridges the gap
between \textit{regret minimization} and \textit{best arm identification} under
lexicographic preferences. We propose two elimination-based algorithms to
address this joint objective. The first algorithm eliminates suboptimal arms
sequentially, layer by layer, in accordance with the objective priorities, and
achieves sample complexity and regret bounds comparable to those of the best
single-objective algorithms. The second algorithm simultaneously leverages
reward information from all objectives in each round, effectively exploiting
cross-objective dependencies. Remarkably, it outperforms the known lower bound
for the single-objective bandit problem, highlighting the benefit of
cross-objective information sharing in the multi-objective setting. Empirical
results further validate their superior performance over baselines.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05802v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05802v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        When AI Meets the Web: Prompt Injection Risks in Third-Party AI Chatbot Plugins
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Prompt injection attacks pose a critical threat to large language models
(LLMs), with prior work focusing on cutting-edge LLM applications like personal
copilots. In contrast, simpler LLM applications, such as customer service
chatbots, are widespread on the web, yet their security posture and exposure to
such attacks remain poorly understood. These applications often rely on
third-party chatbot plugins that act as intermediaries to commercial LLM APIs,
offering non-expert website builders intuitive ways to customize chatbot
behaviors. To bridge this gap, we present the first large-scale study of 17
third-party chatbot plugins used by over 10,000 public websites, uncovering
previously unknown prompt injection risks in practice. First, 8 of these
plugins (used by 8,000 websites) fail to enforce the integrity of the
conversation history transmitted in network requests between the website
visitor and the chatbot. This oversight amplifies the impact of direct prompt
injection attacks by allowing adversaries to forge conversation histories
(including fake system messages), boosting their ability to elicit unintended
behavior (e.g., code generation) by 3 to 8x. Second, 15 plugins offer tools,
such as web-scraping, to enrich the chatbot's context with website-specific
content. However, these tools do not distinguish the website's trusted content
(e.g., product descriptions) from untrusted, third-party content (e.g.,
customer reviews), introducing a risk of indirect prompt injection. Notably, we
found that ~13% of e-commerce websites have already exposed their chatbots to
third-party content. We systematically evaluate both vulnerabilities through
controlled experiments grounded in real-world observations, focusing on factors
such as system prompt design and the underlying LLM. Our findings show that
many plugins adopt insecure practices that undermine the built-in LLM
safeguards.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05797v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05797v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Robotic grasping is a fundamental capability for autonomous manipulation;
however, most existing methods rely on large-scale expert annotations and
necessitate retraining to handle new objects. We present VLAD-Grasp, a
Vision-Language model Assisted zero-shot approach for Detecting grasps. From a
single RGB-D image, our method (1) prompts a large vision-language model to
generate a goal image where a straight rod "impales" the object, representing
an antipodal grasp, (2) predicts depth and segmentation to lift this generated
image into 3D, and (3) aligns generated and observed object point clouds via
principal component analysis and correspondence-free optimization to recover an
executable grasp pose. Unlike prior work, our approach is training-free and
does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves
performance that is competitive with or superior to that of state-of-the-art
supervised models on the Cornell and Jacquard datasets. We further demonstrate
zero-shot generalization to novel real-world objects on a Franka Research 3
robot, highlighting vision-language foundation models as powerful priors for
robotic manipulation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05791v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05791v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SymLight: Exploring Interpretable and Deployable Symbolic Policies for Traffic Signal Control
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep Reinforcement Learning have achieved significant success in
automatically devising effective traffic signal control (TSC) policies. Neural
policies, however, tend to be over-parameterized and non-transparent, hindering
their interpretability and deployability on resource-limited edge devices. This
work presents SymLight, a priority function search framework based on Monte
Carlo Tree Search (MCTS) for discovering inherently interpretable and
deployable symbolic priority functions to serve as the TSC policies. The
priority function, in particular, accepts traffic features as input and then
outputs a priority for each traffic signal phase, which subsequently directs
the phase transition. For effective search, we propose a concise yet expressive
priority function representation. This helps mitigate the combinatorial
explosion of the action space in MCTS. Additionally, a probabilistic structural
rollout strategy is introduced to leverage structural patterns from previously
discovered high-quality priority functions, guiding the rollout process. Our
experiments on real-world datasets demonstrate SymLight's superior performance
across a range of baselines. A key advantage is SymLight's ability to produce
interpretable and deployable TSC policies while maintaining excellent
performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05790v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05790v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        DRAGON: Guard LLM Unlearning in Context via Negative Detection and Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unlearning in Large Language Models (LLMs) is crucial for protecting private
data and removing harmful knowledge. Most existing approaches rely on
fine-tuning to balance unlearning efficiency with general language
capabilities. However, these methods typically require training or access to
retain data, which is often unavailable in real world scenarios. Although these
methods can perform well when both forget and retain data are available, few
works have demonstrated equivalent capability in more practical, data-limited
scenarios. To overcome these limitations, we propose Detect-Reasoning Augmented
GeneratiON (DRAGON), a systematic, reasoning-based framework that utilizes
in-context chain-of-thought (CoT) instructions to guard deployed LLMs before
inference. Instead of modifying the base model, DRAGON leverages the inherent
instruction-following ability of LLMs and introduces a lightweight detection
module to identify forget-worthy prompts without any retain data. These are
then routed through a dedicated CoT guard model to enforce safe and accurate
in-context intervention. To robustly evaluate unlearning performance, we
introduce novel metrics for unlearning performance and the continual unlearning
setting. Extensive experiments across three representative unlearning tasks
validate the effectiveness of DRAGON, demonstrating its strong unlearning
capability, scalability, and applicability in practical scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05784v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05784v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sign language recognition from skeletal data using graph and recurrent neural networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This work presents an approach for recognizing isolated sign language
gestures using skeleton-based pose data extracted from video sequences. A
Graph-GRU temporal network is proposed to model both spatial and temporal
dependencies between frames, enabling accurate classification. The model is
trained and evaluated on the AUTSL (Ankara university Turkish sign language)
dataset, achieving high accuracy. Experimental results demonstrate the
effectiveness of integrating graph-based spatial representations with temporal
modeling, providing a scalable framework for sign language recognition. The
results of this approach highlight the potential of pose-driven methods for
sign language understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05772v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05772v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning Gaussian DAG Models without Condition Number Bounds
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the problem of learning the topology of a directed Gaussian
Graphical Model under the equal-variance assumption, where the graph has $n$
nodes and maximum in-degree $d$. Prior work has established that $O(d \log n)$
samples are sufficient for this task. However, an important factor that is
often overlooked in these analyses is the dependence on the condition number of
the covariance matrix of the model. Indeed, all algorithms from prior work
require a number of samples that grows polynomially with this condition number.
In many cases this is unsatisfactory, since the condition number could grow
polynomially with $n$, rendering these prior approaches impractical in
high-dimensional settings. In this work, we provide an algorithm that recovers
the underlying graph and prove that the number of samples required is
independent of the condition number. Furthermore, we establish lower bounds
that nearly match the upper bound up to a $d$-factor, thus providing an almost
tight characterization of the true sample complexity of the problem. Moreover,
under a further assumption that all the variances of the variables are bounded,
we design a polynomial-time algorithm that recovers the underlying graph, at
the cost of an additional polynomial dependence of the sample complexity on
$d$. We complement our theoretical findings with simulations on synthetic
datasets that confirm our predictions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06164v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06164v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in
children plays a crucial role in improving outcomes in education and mental
health. Diagnosing ADHD using neuroimaging data, however, remains challenging
due to heterogeneous presentations and overlapping symptoms with other
conditions. To address this, we propose a novel parameter-efficient transfer
learning approach that adapts a large-scale 3D convolutional foundation model,
pre-trained on CT images, to an MRI-based ADHD classification task. Our method
introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional
kernels into 2D low-rank updates, dramatically reducing trainable parameters
while achieving superior performance. In a five-fold cross-validated evaluation
on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved
state-of-the-art results, with one model variant reaching 71.9% accuracy and
another attaining an AUC of 0.716. Both variants use only 1.64 million
trainable parameters (over 113x fewer than a fully fine-tuned foundation
model). Our results represent one of the first successful cross-modal
(CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a
new benchmark for ADHD classification while greatly improving efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06163v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06163v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Robustness of Graph Neural Networks through p-Laplacian
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    With the increase of data in day-to-day life, businesses and different
stakeholders need to analyze the data for better pre- dictions. Traditionally,
relational data has been a source of various insights, but with the increase in
computational power and the need to understand deeper relationships between en-
tities, the need to design new techniques has arisen. For this graph data
analysis has become an extraordinary tool for un- derstanding the data, which
reveals more realistic and flexible modelling of complex relationships.
Recently, Graph Neural Networks (GNNs) have shown great promise in various ap-
plications, such as social network analysis, recommendation systems, drug
discovery, and more. However, many adversar- ial attacks can happen over the
data, whether during training (poisoning attack) or during testing (evasion
attack), which can adversely manipulate the desired outcome from the GNN model.
Therefore, it is crucial to make the GNNs robust to such attacks. The existing
robustness methods are computa- tionally demanding and perform poorly when the
intensity of attack increases. This paper presents a computationally ef-
ficient framework, namely, pLAPGNN, based on weighted p-Laplacian for making
GNNs robust. Empirical evaluation on real datasets establishes the efficacy and
efficiency of the proposed method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06143v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06143v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        On the Convergence and Stability of Distributed Sub-model Training
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As learning models continue to grow in size, enabling on-device local
training of these models has emerged as a critical challenge in federated
learning. A popular solution is sub-model training, where the server only
distributes randomly sampled sub-models to the edge clients, and clients only
update these small models. However, those random sampling of sub-models may not
give satisfying convergence performance. In this paper, observing the success
of SGD with shuffling, we propose a distributed shuffled sub-model training,
where the full model is partitioned into several sub-models in advance, and the
server shuffles those sub-models, sends each of them to clients at each round,
and by the end of local updating period, clients send back the updated
sub-models, and server averages them. We establish the convergence rate of this
algorithm. We also study the generalization of distributed sub-model training
via stability analysis, and find that the sub-model training can improve the
generalization via amplifying the stability of training process. The extensive
experiments also validate our theoretical findings.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06132v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06132v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        A Deep Learning Model for Predicting Transformation Legality
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Compilers must check the legality of code transformations to guarantee the
correctness of applying a sequence of code transformations to a given code.
While such a legality check needs to be precisely computed in general, we can
use an approximate legality prediction model in certain cases, such as training
a reinforcement learning (RL) agent for schedule prediction. In this paper, we
propose an approximate method for legality checks. We propose a novel DL model
for predicting the legality of transformations. The model takes the code
representation and a list of transformations as input and predicts whether
applying those transformations to the code is legal. We implement and evaluate
the proposed model, demonstrating its effectiveness. Our evaluation shows an F1
score of 0.91 on a test set of randomly generated programs. To further evaluate
the model in a practical scenario, we used the model to replace the legality
check used during the training of an RL agent designed for automatic code
optimization. We demonstrate that such a replacement enables the agent to train
on twice as many steps, resulting in faster training and reducing resource
usage by approximately 80\% for CPU and 35\% for RAM. The agent trained using
this approach maintains comparable performance, with only a 4\% reduction on
benchmarks from the Polybench suite compared to the traditional method.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06120v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06120v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Guardian-regularized Safe Offline Reinforcement Learning for Smart Weaning of Mechanical Circulatory Devices
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the sequential decision-making problem for automated weaning of
mechanical circulatory support (MCS) devices in cardiogenic shock patients. MCS
devices are percutaneous micro-axial flow pumps that provide left ventricular
unloading and forward blood flow, but current weaning strategies vary
significantly across care teams and lack data-driven approaches. Offline
reinforcement learning (RL) has proven to be successful in sequential
decision-making tasks, but our setting presents challenges for training and
evaluating traditional offline RL methods: prohibition of online patient
interaction, highly uncertain circulatory dynamics due to concurrent
treatments, and limited data availability. We developed an end-to-end machine
learning framework with two key contributions (1) Clinically-aware
OOD-regularized Model-based Policy Optimization (CORMPO), a density-regularized
offline RL algorithm for out-of-distribution suppression that also incorporates
clinically-informed reward shaping and (2) a Transformer-based probabilistic
digital twin that models MCS circulatory dynamics for policy evaluation with
rich physiological and clinical metrics. We prove that \textsf{CORMPO} achieves
theoretical performance guarantees under mild assumptions. CORMPO attains a
higher reward than the offline RL baselines by 28% and higher scores in
clinical metrics by 82.6% on real and synthetic datasets. Our approach offers a
principled framework for safe offline policy learning in high-stakes medical
applications where domain expertise and safety constraints are essential.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06111v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06111v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Forecasting Thermospheric Density with Transformers for Multi-Satellite Orbit Management
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Accurate thermospheric density prediction is crucial for reliable satellite
operations in Low Earth Orbits, especially at high solar and geomagnetic
activity. Physics-based models such as TIE-GCM offer high fidelity but are
computationally expensive, while empirical models like NRLMSIS are efficient
yet lack predictive power. This work presents a transformer-based model that
forecasts densities up to three days ahead and is intended as a drop-in
replacement for an empirical baseline. Unlike recent approaches, it avoids
spatial reduction and complex input pipelines, operating directly on a compact
input set. Validated on real-world data, the model improves key prediction
metrics and shows potential to support mission planning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06105v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06105v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Approximating Shapley Explanations in Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning has achieved remarkable success in complex
decision-making environments, yet its lack of transparency limits its
deployment in practice, especially in safety-critical settings. Shapley values
from cooperative game theory provide a principled framework for explaining
reinforcement learning; however, the computational cost of Shapley explanations
is an obstacle to their use. We introduce FastSVERL, a scalable method for
explaining reinforcement learning by approximating Shapley values. FastSVERL is
designed to handle the unique challenges of reinforcement learning, including
temporal dependencies across multi-step trajectories, learning from off-policy
data, and adapting to evolving agent behaviours in real time. FastSVERL
introduces a practical, scalable approach for principled and rigorous
interpretability in reinforcement learning.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06094v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06094v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MuonAll: Muon Variant for Efficient Finetuning of Large Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Muon optimizer has demonstrated robust results in pretraining of language
models but its performance in finetuning of existing public pretrained models
is not yet explored. Currently, Muon is used along with AdamW introducing a
scope of improvement for adopting all parameters inside Muon. We introduce
MuonAll, which incorporates all the parameters inside Muon by transforming into
2D matrices. We conduct extensive finetuning experiments across publicly
available language models with model sizes upto half billion parameters. Muon
and MuonAll perform at par with AdamW across major benchmarks, highlighting
their effectiveness as alternative optimizers. We open-source the distributed
implementations of Muon and MuonAll, available at
https://github.com/Saurabh750/optimizer
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06086v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06086v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Event-driven physics-informed operator learning for reliability analysis
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reliability analysis of engineering systems under uncertainty poses
significant computational challenges, particularly for problems involving
high-dimensional stochastic inputs, nonlinear system responses, and
multiphysics couplings. Traditional surrogate modeling approaches often incur
high energy consumption, which severely limits their scalability and
deployability in resource-constrained environments. We introduce NeuroPOL,
\textit{the first neuroscience-inspired physics-informed operator learning
framework} for reliability analysis. NeuroPOL incorporates Variable Spiking
Neurons into a physics-informed operator architecture, replacing continuous
activations with event-driven spiking dynamics. This innovation promotes sparse
communication, significantly reduces computational load, and enables an
energy-efficient surrogate model. The proposed framework lowers both
computational and power demands, supporting real-time reliability assessment
and deployment on edge devices and digital twins. By embedding governing
physical laws into operator learning, NeuroPOL builds physics-consistent
surrogates capable of accurate uncertainty propagation and efficient failure
probability estimation, even for high-dimensional problems. We evaluate
NeuroPOL on five canonical benchmarks, the Burgers equation, Nagumo equation,
two-dimensional Poisson equation, two-dimensional Darcy equation, and
incompressible Navier-Stokes equation with energy coupling. Results show that
NeuroPOL achieves reliability measures comparable to standard physics-informed
operators, while introducing significant communication sparsity, enabling
scalable, distributed, and energy-efficient deployment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06083v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06083v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Make It Long, Keep It Fast: End-to-End 10k-Sequence Modeling at Billion Scale on Douyin
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Short-video recommenders such as Douyin must exploit extremely long user
histories without breaking latency or cost budgets. We present an end-to-end
system that scales long-sequence modeling to 10k-length histories in
production. First, we introduce Stacked Target-to-History Cross Attention
(STCA), which replaces history self-attention with stacked cross-attention from
the target to the history, reducing complexity from quadratic to linear in
sequence length and enabling efficient end-to-end training. Second, we propose
Request Level Batching (RLB), a user-centric batching scheme that aggregates
multiple targets for the same user/request to share the user-side encoding,
substantially lowering sequence-related storage, communication, and compute
without changing the learning objective. Third, we design a
length-extrapolative training strategy -- train on shorter windows, infer on
much longer ones -- so the model generalizes to 10k histories without
additional training cost. Across offline and online experiments, we observe
predictable, monotonic gains as we scale history length and model capacity,
mirroring the scaling law behavior observed in large language models. Deployed
at full traffic on Douyin, our system delivers significant improvements on key
engagement metrics while meeting production latency, demonstrating a practical
path to scaling end-to-end long-sequence recommendation to the 10k regime.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06077v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06077v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CatBack: Universal Backdoor Attacks on Tabular Data via Categorical Encoding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Backdoor attacks in machine learning have drawn significant attention for
their potential to compromise models stealthily, yet most research has focused
on homogeneous data such as images. In this work, we propose a novel backdoor
attack on tabular data, which is particularly challenging due to the presence
of both numerical and categorical features. Our key idea is a novel technique
to convert categorical values into floating-point representations. This
approach preserves enough information to maintain clean-model accuracy compared
to traditional methods like one-hot or ordinal encoding. By doing this, we
create a gradient-based universal perturbation that applies to all features,
including categorical ones.
  We evaluate our method on five datasets and four popular models. Our results
show up to a 100% attack success rate in both white-box and black-box settings
(including real-world applications like Vertex AI), revealing a severe
vulnerability for tabular data. Our method is shown to surpass the previous
works like Tabdoor in terms of performance, while remaining stealthy against
state-of-the-art defense mechanisms. We evaluate our attack against Spectral
Signatures, Neural Cleanse, Beatrix, and Fine-Pruning, all of which fail to
defend successfully against it. We also verify that our attack successfully
bypasses popular outlier detection mechanisms.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06072v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06072v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Function Based Isolation Forest (FuBIF): A Unifying Framework for Interpretable Isolation-Based Anomaly Detection
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Anomaly Detection (AD) is evolving through algorithms capable of identifying
outliers in complex datasets. The Isolation Forest (IF), a pivotal AD
technique, exhibits adaptability limitations and biases. This paper introduces
the Function-based Isolation Forest (FuBIF), a generalization of IF that
enables the use of real-valued functions for dataset branching, significantly
enhancing the flexibility of evaluation tree construction. Complementing this,
the FuBIF Feature Importance (FuBIFFI) algorithm extends the interpretability
in IF-based approaches by providing feature importance scores across possible
FuBIF models. This paper details the operational framework of FuBIF, evaluates
its performance against established methods, and explores its theoretical
contributions. An open-source implementation is provided to encourage further
research and ensure reproducibility.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06054v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06054v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Visual Exploration of Feature Relationships in Sparse Autoencoders with Curated Concepts
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Sparse autoencoders (SAEs) have emerged as a powerful tool for uncovering
interpretable features in large language models (LLMs) through the sparse
directions they learn. However, the sheer number of extracted directions makes
comprehensive exploration intractable. While conventional embedding techniques
such as UMAP can reveal global structure, they suffer from limitations
including high-dimensional compression artifacts, overplotting, and misleading
neighborhood distortions. In this work, we propose a focused exploration
framework that prioritizes curated concepts and their corresponding SAE
features over attempts to visualize all available features simultaneously. We
present an interactive visualization system that combines topology-based visual
encoding with dimensionality reduction to faithfully represent both local and
global relationships among selected features. This hybrid approach enables
users to investigate SAE behavior through targeted, interpretable subsets,
facilitating deeper and more nuanced analysis of concept representation in
latent space.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06048v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06048v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Physics-Informed Design of Input Convex Neural Networks for Consistency Optimal Transport Flow Matching
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose a consistency model based on the optimal-transport flow. A
physics-informed design of partially input-convex neural networks (PICNN) plays
a central role in constructing the flow field that emulates the displacement
interpolation. During the training stage, we couple the Hamilton-Jacobi (HJ)
residual in the OT formulation with the original flow matching loss function.
Our approach avoids inner optimization subproblems that are present in previous
one-step OFM approaches. During the prediction stage, our approach supports
both one-step (Brenier-map) and multi-step ODE sampling from the same learned
potential, leveraging the straightness of the OT flow. We validate scalability
and performance on standard OT benchmarks.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06042v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06042v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Algorithmic Phase Transition in Symmetric Correlated Spiked Wigner Model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We study the computational task of detecting and estimating correlated
signals in a pair of spiked Wigner matrices. Our model consists of observations
  $$
  X = \tfrac{\lambda}{\sqrt{n}} xx^{\top} + W \,, \quad Y =
\tfrac{\mu}{\sqrt{n}} yy^{\top} + Z \,.
  $$
  where $x,y \in \mathbb R^n$ are signal vectors with norm $\|x\|,\|y\|
\approx\sqrt{n}$ and correlation $\langle x,y \rangle \approx \rho\|x\|\|y\|$,
while $W,Z$ are independent Gaussian noise matrices. We propose an efficient
algorithm that succeeds whenever $F(\lambda,\mu,\rho)>1$, where
  $$
  F(\lambda,\mu,\rho)=\max\Big\{ \lambda,\mu, \frac{ \lambda^2 \rho^2 }{
1-\lambda^2+\lambda^2 \rho^2 } + \frac{ \mu^2 \rho^2 }{ 1-\mu^2+\mu^2 \rho^2 }
\Big\} \,.
  $$
  Our result shows that an algorithm can leverage the correlation between the
spikes to detect and estimate the signals even in regimes where efficiently
recovering either $x$ from $X$ alone or $y$ from $Y$ alone is believed to be
computationally infeasible.
  We complement our algorithmic result with evidence for a matching
computational lower bound. In particular, we prove that when
$F(\lambda,\mu,\rho)<1$, all algorithms based on {\em low-degree polynomials}
fails to distinguish $(X,Y)$ with two independent Wigner matrices. This
low-degree analysis strongly suggests that $F(\lambda,\mu,\rho)=1$ is the
precise computation threshold for this problem.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06040v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06040v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Lethe: Layer- and Time-Adaptive KV Cache Pruning for Reasoning-Intensive LLM Serving
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Generative reasoning with large language models (LLMs) often involves long
decoding sequences, leading to substantial memory and latency overheads from
accumulating key-value (KV) caches. While existing KV compression methods
primarily focus on reducing prefill memory from long input sequences, they fall
short in addressing the dynamic and layer-sensitive nature of long-form
generation, which is central to reasoning tasks. We propose Lethe, a dynamic KV
cache management framework that introduces adaptivity along both the spatial
and temporal dimensions of decoding. Along the spatial dimension, Lethe
performs layerwise sparsity-aware allocation, assigning token pruning budgets
to each transformer layer based on estimated attention redundancy. Along the
temporal dimension, Lethe conducts multi-round token pruning during generation,
driven by a Recency-Aware Selective Retention} (RASR) mechanism. RASR extends
traditional recency-based heuristics by also considering token relevance
derived from evolving attention patterns, enabling informed decisions about
which tokens to retain or evict. Empirical results demonstrate that Lethe
achieves a favorable balance between efficiency and generation quality across
diverse models and tasks, increases throughput by up to 2.56x.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06029v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06029v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Learning solutions of parameterized stiff ODEs using Gaussian processes
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Stiff ordinary differential equations (ODEs) play an important role in many
scientific and engineering applications. Often, the dependence of the solution
of the ODE on additional parameters is of interest, e.g.\ when dealing with
uncertainty quantification or design optimization. Directly studying this
dependence can quickly become too computationally expensive, such that cheaper
surrogate models approximating the solution are of interest. One popular class
of surrogate models are Gaussian processes (GPs). They perform well when
approximating stationary functions, functions which have a similar level of
variation along any given parameter direction, however solutions to stiff ODEs
are often characterized by a mixture of regions of rapid and slow variation
along the time axis and when dealing with such nonstationary functions, GP
performance frequently degrades drastically. We therefore aim to reparameterize
stiff ODE solutions based on the available data, to make them appear more
stationary and hence recover good GP performance. This approach comes with
minimal computational overhead and requires no internal changes to the GP
implementation, as it can be seen as a separate preprocessing step. We
illustrate the achieved benefits using multiple examples.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05990v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05990v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Bespoke Co-processor for Energy-Efficient Health Monitoring on RISC-V-based Flexible Wearables
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Flexible electronics offer unique advantages for conformable, lightweight,
and disposable healthcare wearables. However, their limited gate count, large
feature sizes, and high static power consumption make on-body machine learning
classification highly challenging. While existing bendable RISC-V systems
provide compact solutions, they lack the energy efficiency required. We present
a mechanically flexible RISC-V that integrates a bespoke multiply-accumulate
co-processor with fixed coefficients to maximize energy efficiency and minimize
latency. Our approach formulates a constrained programming problem to jointly
determine co-processor constants and optimally map Multi-Layer Perceptron (MLP)
inference operations, enabling compact, model-specific hardware by leveraging
the low fabrication and non-recurring engineering costs of flexible
technologies. Post-layout results demonstrate near-real-time performance across
several healthcare datasets, with our circuits operating within the power
budget of existing flexible batteries and occupying only 2.42 mm^2, offering a
promising path toward accessible, sustainable, and conformable healthcare
wearables. Our microprocessors achieve an average 2.35x speedup and 2.15x lower
energy consumption compared to the state of the art.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05985v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05985v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Benchmarking of Clustering Validity Measures Revisited
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Validation plays a crucial role in the clustering process. Many different
internal validity indexes exist for the purpose of determining the best
clustering solution(s) from a given collection of candidates, e.g., as produced
by different algorithms or different algorithm hyper-parameters. In this study,
we present a comprehensive benchmark study of 26 internal validity indexes,
which includes highly popular classic indexes as well as more recently
developed ones. We adopted an enhanced revision of the methodology presented in
Vendramin et al. (2010), developed here to address several shortcomings of this
previous work. This overall new approach consists of three complementary
custom-tailored evaluation sub-methodologies, each of which has been designed
to assess specific aspects of an index's behaviour while preventing potential
biases of the other sub-methodologies. Each sub-methodology features two
complementary measures of performance, alongside mechanisms that allow for an
in-depth investigation of more complex behaviours of the internal validity
indexes under study. Additionally, a new collection of 16177 datasets has been
produced, paired with eight widely-used clustering algorithms, for a wider
applicability scope and representation of more diverse clustering scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05983v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05983v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Are Time-Indexed Foundation Models the Future of Time Series Imputation?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Foundation models for time series imputation remain largely unexplored.
Recently, two such models, TabPFN-TS and MoTM, have emerged. These models share
a common philosophy that places them within the family of time-indexed
foundation models. This paper presents the first large-scale empirical study of
these models for zero-shot imputation, which enables missing value recovery
without retraining across a wide range of scenarios. We conduct extensive
univariate experiments across 33 out-of-domain datasets (approximately 1.3M
imputation windows) and evaluate their ability to integrate covariates at
inference time to improve accuracy without fine-tuning. Our results demonstrate
that time-indexed foundation models are a powerful and practical step toward
achieving general-purpose, zero-shot imputation for real-world time series.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05980v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05980v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Explainable Deep Learning-based Classification of Wolff-Parkinson-White Electrocardiographic Signals
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Wolff-Parkinson-White (WPW) syndrome is a cardiac electrophysiology (EP)
disorder caused by the presence of an accessory pathway (AP) that bypasses the
atrioventricular node, faster ventricular activation rate, and provides a
substrate for atrio-ventricular reentrant tachycardia (AVRT). Accurate
localization of the AP is critical for planning and guiding catheter ablation
procedures. While traditional diagnostic tree (DT) methods and more recent
machine learning (ML) approaches have been proposed to predict AP location from
surface electrocardiogram (ECG), they are often constrained by limited
anatomical localization resolution, poor interpretability, and the use of small
clinical datasets. In this study, we present a Deep Learning (DL) model for the
localization of single manifest APs across 24 cardiac regions, trained on a
large, physiologically realistic database of synthetic ECGs generated using a
personalized virtual heart model. We also integrate eXplainable Artificial
Intelligence (XAI) methods, Guided Backpropagation, Grad-CAM, and Guided
Grad-CAM, into the pipeline. This enables interpretation of DL decision-making
and addresses one of the main barriers to clinical adoption: lack of
transparency in ML predictions. Our model achieves localization accuracy above
95%, with a sensitivity of 94.32% and specificity of 99.78%. XAI outputs are
physiologically validated against known depolarization patterns, and a novel
index is introduced to identify the most informative ECG leads for AP
localization. Results highlight lead V2 as the most critical, followed by aVF,
V1, and aVL. This work demonstrates the potential of combining cardiac digital
twins with explainable DL to enable accurate, transparent, and non-invasive AP
localization.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05973v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05973v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Next-Latent Prediction Transformers Learn Compact World Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Transformers replace recurrence with a memory that grows with sequence length
and self-attention that enables ad-hoc look ups over past tokens. Consequently,
they lack an inherent incentive to compress history into compact latent states
with consistent transition rules. This often leads to learning solutions that
generalize poorly. We introduce Next-Latent Prediction (NextLat), which extends
standard next-token training with self-supervised predictions in the latent
space. Specifically, NextLat trains a transformer to learn latent
representations that are predictive of its next latent state given the next
output token. Theoretically, we show that these latents provably converge to
belief states, compressed information of the history necessary to predict the
future. This simple auxiliary objective also injects a recurrent inductive bias
into transformers, while leaving their architecture, parallel training, and
inference unchanged. NextLat effectively encourages the transformer to form
compact internal world models with its own belief states and transition
dynamics -- a crucial property absent in standard next-token prediction
transformers. Empirically, across benchmarks targeting core sequence modeling
competencies -- world modeling, reasoning, planning, and language modeling --
NextLat demonstrates significant gains over standard next-token training in
downstream accuracy, representation compression, and lookahead planning.
NextLat stands as a simple and efficient paradigm for shaping transformer
representations toward stronger generalization.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05963v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05963v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Deep Survival Analysis of Longitudinal EHR Data for Joint Prediction of Hospitalization and Death in COPD Patients
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Patients with chronic obstructive pulmonary disease (COPD) have an increased
risk of hospitalizations, strongly associated with decreased survival, yet
predicting the timing of these events remains challenging and has received
limited attention in the literature. In this study, we performed survival
analysis to predict hospitalization and death in COPD patients using
longitudinal electronic health records (EHRs), comparing statistical models,
machine learning (ML), and deep learning (DL) approaches. We analyzed data from
more than 150k patients from the SIDIAP database in Catalonia, Spain, from 2013
to 2017, modeling hospitalization as a first event and death as a
semi-competing terminal event. Multiple models were evaluated, including Cox
proportional hazards, SurvivalBoost, DeepPseudo, SurvTRACE, Dynamic Deep-Hit,
and Deep Recurrent Survival Machine. Results showed that DL models utilizing
recurrent architectures outperformed both ML and linear approaches in
concordance and time-dependent AUC, especially for hospitalization, which
proved to be the harder event to predict. This study is, to our knowledge, the
first to apply deep survival analysis on longitudinal EHR data to jointly
predict multiple time-to-event outcomes in COPD patients, highlighting the
potential of DL approaches to capture temporal patterns and improve risk
stratification.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05960v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05960v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CSGaze: Context-aware Social Gaze Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05955v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05955v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        From Kernels to Attention: A Transformer Framework for Density and Score Estimation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We introduce a unified attention-based framework for joint score and density
estimation. Framing the problem as a sequence-to-sequence task, we develop a
permutation- and affine-equivariant transformer that estimates both the
probability density $f(x)$ and its score $\nabla_x \log f(x)$ directly from
i.i.d. samples. Unlike traditional score-matching methods that require training
a separate model for each distribution, our approach learns a single
distribution-agnostic operator that generalizes across densities and sample
sizes. The architecture employs cross-attention to connect observed samples
with arbitrary query points, enabling generalization beyond the training data,
while built-in symmetry constraints ensure equivariance to permutation and
affine transformations. Analytically, we show that the attention weights can
recover classical kernel density estimation (KDE), and verify it empirically,
establishing a principled link between classical KDE and the transformer
architecture. Empirically, the model achieves substantially lower error and
better scaling than KDE and score-debiased KDE (SD-KDE), while exhibiting
better runtime scaling. Together, these results establish transformers as
general-purpose, data-adaptive operators for nonparametric density and score
estimation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05924v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05924v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FusionLog: Cross-System Log-based Anomaly Detection via Fusion of General and Proprietary Knowledge
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Log-based anomaly detection is critical for ensuring the stability and
reliability of web systems. One of the key problems in this task is the lack of
sufficient labeled logs, which limits the rapid deployment in new systems.
Existing works usually leverage large-scale labeled logs from a mature web
system and a small amount of labeled logs from a new system, using transfer
learning to extract and generalize general knowledge across both domains.
However, these methods focus solely on the transfer of general knowledge and
neglect the disparity and potential mismatch between such knowledge and the
proprietary knowledge of target system, thus constraining performance. To
address this limitation, we propose FusionLog, a novel zero-label cross-system
log-based anomaly detection method that effectively achieves the fusion of
general and proprietary knowledge, enabling cross-system generalization without
any labeled target logs. Specifically, we first design a training-free router
based on semantic similarity that dynamically partitions unlabeled target logs
into 'general logs' and 'proprietary logs.' For general logs, FusionLog employs
a small model based on system-agnostic representation meta-learning for direct
training and inference, inheriting the general anomaly patterns shared between
the source and target systems. For proprietary logs, we iteratively generate
pseudo-labels and fine-tune the small model using multi-round collaborative
knowledge distillation and fusion based on large language model (LLM) and small
model (SM) to enhance its capability to recognize anomaly patterns specific to
the target system. Experimental results on three public log datasets from
different systems show that FusionLog achieves over 90% F1-score under a fully
zero-label setting, significantly outperforming state-of-the-art cross-system
log-based anomaly detection methods.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05878v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05878v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In recent years, the advancement of Graph Neural Networks (GNNs) has
significantly propelled progress in Multi-View Clustering (MVC). However,
existing methods face the problem of coarse-grained graph fusion. Specifically,
current approaches typically generate a separate graph structure for each view
and then perform weighted fusion of graph structures at the view level, which
is a relatively rough strategy. To address this limitation, we present a novel
Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly
consists of two modules. In particular, we propose an innovative Mixture of
Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a
Mixture-of-Experts network to implement fine-grained fusion of ego graphs at
the sample level, rather than the conventional view-level fusion. Additionally,
we present the Ego Graph Contrastive Learning (EGCL) module to align the fused
representation with the view-specific representation. The EGCL module enhances
the representation similarity of samples from the same cluster, not merely from
the same sample, further boosting fine-grained graph representation. Extensive
experiments demonstrate that MoEGCL achieves state-of-the-art results in deep
multi-view clustering tasks. The source code is publicly available at
https://github.com/HackerHyper/MoEGCL.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05876v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05876v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CADM: Cluster-customized Adaptive Distance Metric for Categorical Data Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    An appropriate distance metric is crucial for categorical data clustering, as
the distance between categorical data cannot be directly calculated. However,
the distances between attribute values usually vary in different clusters
induced by their different distributions, which has not been taken into
account, thus leading to unreasonable distance measurement. Therefore, we
propose a cluster-customized distance metric for categorical data clustering,
which can competitively update distances based on different distributions of
attributes in each cluster. In addition, we extend the proposed distance metric
to the mixed data that contains both numerical and categorical attributes.
Experiments demonstrate the efficacy of the proposed method, i.e., achieving an
average ranking of around first in fourteen datasets. The source code is
available at https://anonymous.4open.science/r/CADM-47D8
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05826v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05826v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        AiEDA: An Open-Source AI-Aided Design Library for Design-to-Vector
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent research has demonstrated that artificial intelligence (AI) can assist
electronic design automation (EDA) in improving both the quality and efficiency
of chip design. But current AI for EDA (AI-EDA) infrastructures remain
fragmented, lacking comprehensive solutions for the entire data pipeline from
design execution to AI integration. Key challenges include fragmented flow
engines that generate raw data, heterogeneous file formats for data exchange,
non-standardized data extraction methods, and poorly organized data storage.
This work introduces a unified open-source library for EDA (AiEDA) that
addresses these issues. AiEDA integrates multiple design-to-vector data
representation techniques that transform diverse chip design data into
universal multi-level vector representations, establishing an AI-aided design
(AAD) paradigm optimized for AI-EDA workflows. AiEDA provides complete physical
design flows with programmatic data extraction and standardized Python
interfaces bridging EDA datasets and AI frameworks. Leveraging the AiEDA
library, we generate iDATA, a 600GB dataset of structured data derived from 50
real chip designs (28nm), and validate its effectiveness through seven
representative AAD tasks spanning prediction, generation, optimization and
analysis. The code is publicly available at
https://github.com/OSCC-Project/AiEDA, while the full iDATA dataset is being
prepared for public release, providing a foundation for future AI-EDA research.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05823v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05823v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Catching Contamination Before Generation: Spectral Kill Switches for Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Agentic language models compose multi step reasoning chains, yet intermediate
steps can be corrupted by inconsistent context, retrieval errors, or
adversarial inputs, which makes post hoc evaluation too late because errors
propagate before detection. We introduce a diagnostic that requires no
additional training and uses only the forward pass to emit a binary accept or
reject signal during agent execution. The method analyzes token graphs induced
by attention and computes two spectral statistics in early layers, namely the
high frequency energy ratio and spectral entropy. We formalize these signals,
establish invariances, and provide finite sample estimators with uncertainty
quantification. Under a two regime mixture assumption with a monotone
likelihood ratio property, we show that a single threshold on the high
frequency energy ratio is optimal in the Bayes sense for detecting context
inconsistency. Empirically, the high frequency energy ratio exhibits robust
bimodality during context verification across multiple model families, which
enables gating decisions with overhead below one millisecond on our hardware
and configurations. We demonstrate integration into retrieval augmented agent
pipelines and discuss deployment as an inline safety monitor. The approach
detects contamination while the model is still processing the text, before
errors commit to the reasoning chain.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05804v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05804v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ambient sensor-based human activity recognition (HAR) in smart homes remains
challenging due to the need for real-time inference, spatially grounded
reasoning, and context-aware temporal modeling. Existing approaches often rely
on pre-segmented, within-activity data and overlook the physical layout of the
environment, limiting their robustness in continuous, real-world deployments.
In this paper, we propose MARAuder's Map, a novel framework for real-time
activity recognition from raw, unsegmented sensor streams. Our method projects
sensor activations onto the physical floorplan to generate trajectory-aware,
image-like sequences that capture the spatial flow of human movement. These
representations are processed by a hybrid deep learning model that jointly
captures spatial structure and temporal dependencies. To enhance temporal
awareness, we introduce a learnable time embedding module that encodes
contextual cues such as hour-of-day and day-of-week. Additionally, an
attention-based encoder selectively focuses on informative segments within each
observation window, enabling accurate recognition even under cross-activity
transitions and temporal ambiguity. Extensive experiments on multiple
real-world smart home datasets demonstrate that our method outperforms strong
baselines, offering a practical solution for real-time HAR in ambient sensor
environments.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05773v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05773v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Multi-Reward GRPO Fine-Tuning for De-biasing Large Language Models: A Study Based on Chinese-Context Discrimination Data
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) often exhibit implicit biases and discriminatory
tendencies that reflect underlying social stereotypes. While recent alignment
techniques such as RLHF and DPO have mitigated some of these issues, they
remain limited in addressing culturally specific and multi-dimensional forms of
discrimination. This paper proposes a Multi-Reward Group Relative Policy
Optimization (GRPO) framework to fine-tune LLMs toward ethical and bias-free
behavior. Our approach constructs a synthetic English-language dataset derived
from Chinese-context discrimination categories, including regional, ethnic, and
occupational biases. Each instance is paired with both neutral and biased
responses to train a reward model based on DeBERTa-v3, which provides
multi-dimensional reward signals capturing fairness, neutrality, and linguistic
quality. The trained reward model then guides GRPO fine-tuning to optimize
model outputs along these ethical dimensions. Experimental results demonstrate
significant reductions in bias intensity and improved alignment with
non-discriminatory standards without compromising fluency or informativeness.
This study highlights the effectiveness of GRPO-based multi-reward optimization
for de-biasing LLMs and offers a replicable framework for cultural-contextual
ethical alignment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06023v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06023v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MCP-RiskCue: Can LLM infer risk information from MCP server System Logs?
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) demonstrate strong capabilities in solving
complex tasks when integrated with external tools. The Model Context Protocol
(MCP) has become a standard interface for enabling such tool-based
interactions. However, these interactions introduce substantial security
concerns, particularly when the MCP server is compromised or untrustworthy.
While prior benchmarks primarily focus on prompt injection attacks or analyze
the vulnerabilities of LLM MCP interaction trajectories, limited attention has
been given to the underlying system logs associated with malicious MCP servers.
To address this gap, we present the first synthetic benchmark for evaluating
LLMs ability to identify security risks from system logs. We define nine
categories of MCP server risks and generate 1,800 synthetic system logs using
ten state-of-the-art LLMs. These logs are embedded in the return values of 243
curated MCP servers, yielding a dataset of 2,421 chat histories for training
and 471 queries for evaluation. Our pilot experiments reveal that smaller
models often fail to detect risky system logs, leading to high false negatives.
While models trained with supervised fine-tuning (SFT) tend to over-flag benign
logs, resulting in elevated false positives, Reinforcement Learning from
Verifiable Reward (RLVR) offers a better precision-recall balance. In
particular, after training with Group Relative Policy Optimization (GRPO),
Llama3.1-8B-Instruct achieves 83% accuracy, surpassing the best-performing
large remote model by 9 percentage points. Fine-grained, per-category analysis
further underscores the effectiveness of reinforcement learning in enhancing
LLM safety within the MCP framework. Code and data are available at:
https://github.com/PorUna-byte/MCP-Guard/tree/master
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05867v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05867v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-8" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Vision
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 22 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前视觉技术研究主要集中在提升图像和视频处理的准确性与效率，尤其是在运动模糊、深度补全、视频帧插值等领域。研究者们逐渐采用混合模型和自监督学习方法，以克服传统技术在处理复杂场景和物理特性时的局限性。此外，随着边缘计算和安全监控的关注度上升，如何在保证用户隐私和系统安全的前提下优化模型性能成为新的研究热点。这些趋势表明，视觉技术在实际应用中正朝着更智能、更安全的方向发展。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hybrid CNN-ViT Framework for Motion-Blurred Scene Text Restoration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Motion blur in scene text images severely impairs readability and hinders the
reliability of computer vision tasks, including autonomous driving, document
digitization, and visual information retrieval. Conventional deblurring
approaches are often inadequate in handling spatially varying blur and
typically fall short in modeling the long-range dependencies necessary for
restoring textual clarity. To overcome these limitations, we introduce a hybrid
deep learning framework that combines convolutional neural networks (CNNs) with
vision transformers (ViTs), thereby leveraging both local feature extraction
and global contextual reasoning. The architecture employs a CNN-based
encoder-decoder to preserve structural details, while a transformer module
enhances global awareness through self-attention. Training is conducted on a
curated dataset derived from TextOCR, where sharp scene-text samples are paired
with synthetically blurred versions generated using realistic motion-blur
kernels of multiple sizes and orientations. Model optimization is guided by a
composite loss that incorporates mean absolute error (MAE), squared error
(MSE), perceptual similarity, and structural similarity (SSIM). Quantitative
eval- uations show that the proposed method attains 32.20 dB in PSNR and 0.934
in SSIM, while remaining lightweight with 2.83 million parameters and an
average inference time of 61 ms. These results highlight the effectiveness and
computational efficiency of the CNN-ViT hybrid design, establishing its
practicality for real-world motion-blurred scene-text restoration.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06087v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06087v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S2ML: Spatio-Spectral Mutual Learning for Depth Completion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The raw depth images captured by RGB-D cameras using Time-of-Flight (TOF) or
structured light often suffer from incomplete depth values due to weak
reflections, boundary shadows, and artifacts, which limit their applications in
downstream vision tasks. Existing methods address this problem through depth
completion in the image domain, but they overlook the physical characteristics
of raw depth images. It has been observed that the presence of invalid depth
areas alters the frequency distribution pattern. In this work, we propose a
Spatio-Spectral Mutual Learning framework (S2ML) to harmonize the advantages of
both spatial and frequency domains for depth completion. Specifically, we
consider the distinct properties of amplitude and phase spectra and devise a
dedicated spectral fusion module. Meanwhile, the local and global correlations
between spatial-domain and frequency-domain features are calculated in a
unified embedding space. The gradual mutual representation and refinement
encourage the network to fully explore complementary physical characteristics
and priors for more accurate depth completion. Extensive experiments
demonstrate the effectiveness of our proposed S2ML method, outperforming the
state-of-the-art method CFormer by 0.828 dB and 0.834 dB on the NYU-Depth V2
and SUN RGB-D datasets, respectively.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06033v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06033v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MiVID: Multi-Strategic Self-Supervision for Video Frame Interpolation using Diffusion Model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Video Frame Interpolation (VFI) remains a cornerstone in video enhancement,
enabling temporal upscaling for tasks like slow-motion rendering, frame rate
conversion, and video restoration. While classical methods rely on optical flow
and learning-based models assume access to dense ground-truth, both struggle
with occlusions, domain shifts, and ambiguous motion. This article introduces
MiVID, a lightweight, self-supervised, diffusion-based framework for video
interpolation. Our model eliminates the need for explicit motion estimation by
combining a 3D U-Net backbone with transformer-style temporal attention,
trained under a hybrid masking regime that simulates occlusions and motion
uncertainty. The use of cosine-based progressive masking and adaptive loss
scheduling allows our network to learn robust spatiotemporal representations
without any high-frame-rate supervision. Our framework is evaluated on UCF101-7
and DAVIS-7 datasets. MiVID is trained entirely on CPU using the datasets and
9-frame video segments, making it a low-resource yet highly effective pipeline.
Despite these constraints, our model achieves optimal results at just 50
epochs, competitive with several supervised baselines.This work demonstrates
the power of self-supervised diffusion priors for temporally coherent frame
synthesis and provides a scalable path toward accessible and generalizable VFI
systems.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06019v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06019v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        One-Shot Knowledge Transfer for Scalable Person Re-Identification
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Edge computing in person re-identification (ReID) is crucial for reducing the
load on central cloud servers and ensuring user privacy. Conventional
compression methods for obtaining compact models require computations for each
individual student model. When multiple models of varying sizes are needed to
accommodate different resource conditions, this leads to repetitive and
cumbersome computations. To address this challenge, we propose a novel
knowledge inheritance approach named OSKT (One-Shot Knowledge Transfer), which
consolidates the knowledge of the teacher model into an intermediate carrier
called a weight chain. When a downstream scenario demands a model that meets
specific resource constraints, this weight chain can be expanded to the target
model size without additional computation. OSKT significantly outperforms
state-of-the-art compression methods, with the added advantage of one-time
knowledge transfer that eliminates the need for frequent computations for each
target model.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06016v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06016v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Exploring Category-level Articulated Object Pose Tracking on SE(3) Manifolds
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Articulated objects are prevalent in daily life and robotic manipulation
tasks. However, compared to rigid objects, pose tracking for articulated
objects remains an underexplored problem due to their inherent kinematic
constraints. To address these challenges, this work proposes a novel
point-pair-based pose tracking framework, termed \textbf{PPF-Tracker}. The
proposed framework first performs quasi-canonicalization of point clouds in the
SE(3) Lie group space, and then models articulated objects using Point Pair
Features (PPF) to predict pose voting parameters by leveraging the invariance
properties of SE(3). Finally, semantic information of joint axes is
incorporated to impose unified kinematic constraints across all parts of the
articulated object. PPF-Tracker is systematically evaluated on both synthetic
datasets and real-world scenarios, demonstrating strong generalization across
diverse and challenging environments. Experimental results highlight the
effectiveness and robustness of PPF-Tracker in multi-frame pose tracking of
articulated objects. We believe this work can foster advances in robotics,
embodied intelligence, and augmented reality. Codes are available at
https://github.com/mengxh20/PPFTracker.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05996v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05996v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Runtime Safety Monitoring of Deep Neural Networks for Perception: A Survey
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Deep neural networks (DNNs) are widely used in perception systems for
safety-critical applications, such as autonomous driving and robotics. However,
DNNs remain vulnerable to various safety concerns, including generalization
errors, out-of-distribution (OOD) inputs, and adversarial attacks, which can
lead to hazardous failures. This survey provides a comprehensive overview of
runtime safety monitoring approaches, which operate in parallel to DNNs during
inference to detect these safety concerns without modifying the DNN itself. We
categorize existing methods into three main groups: Monitoring inputs, internal
representations, and outputs. We analyze the state-of-the-art for each
category, identify strengths and limitations, and map methods to the safety
concerns they address. In addition, we highlight open challenges and future
research directions.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05982v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05982v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adapted Foundation Models for Breast MRI Triaging in Contrast-Enhanced and Non-Contrast Enhanced Protocols
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Background: Magnetic resonance imaging (MRI) has high sensitivity for breast
cancer detection, but interpretation is time-consuming. Artificial intelligence
may aid in pre-screening. Purpose: To evaluate the DINOv2-based Medical Slice
Transformer (MST) for ruling out significant findings (Breast Imaging Reporting
and Data System [BI-RADS] >=4) in contrast-enhanced and non-contrast-enhanced
abbreviated breast MRI. Materials and Methods: This institutional review board
approved retrospective study included 1,847 single-breast MRI examinations (377
BI-RADS >=4) from an in-house dataset and 924 from an external validation
dataset (Duke). Four abbreviated protocols were tested: T1-weighted early
subtraction (T1sub), diffusion-weighted imaging with b=1500 s/mm2 (DWI1500),
DWI1500+T2-weighted (T2w), and T1sub+T2w. Performance was assessed at 90%, 95%,
and 97.5% sensitivity using five-fold cross-validation and area under the
receiver operating characteristic curve (AUC) analysis. AUC differences were
compared with the DeLong test. False negatives were characterized, and
attention maps of true positives were rated in the external dataset. Results: A
total of 1,448 female patients (mean age, 49 +/- 12 years) were included.
T1sub+T2w achieved an AUC of 0.77 +/- 0.04; DWI1500+T2w, 0.74 +/- 0.04
(p=0.15). At 97.5% sensitivity, T1sub+T2w had the highest specificity (19% +/-
7%), followed by DWI1500+T2w (17% +/- 11%). Missed lesions had a mean diameter
<10 mm at 95% and 97.5% thresholds for both T1sub and DWI1500, predominantly
non-mass enhancements. External validation yielded an AUC of 0.77, with 88% of
attention maps rated good or moderate. Conclusion: At 97.5% sensitivity, the
MST framework correctly triaged cases without BI-RADS >=4, achieving 19%
specificity for contrast-enhanced and 17% for non-contrast-enhanced MRI.
Further research is warranted before clinical implementation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05967v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05967v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Adaptive Agent Selection and Interaction Network for Image-to-point cloud Registration
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Typical detection-free methods for image-to-point cloud registration leverage
transformer-based architectures to aggregate cross-modal features and establish
correspondences. However, they often struggle under challenging conditions,
where noise disrupts similarity computation and leads to incorrect
correspondences. Moreover, without dedicated designs, it remains difficult to
effectively select informative and correlated representations across
modalities, thereby limiting the robustness and accuracy of registration. To
address these challenges, we propose a novel cross-modal registration framework
composed of two key modules: the Iterative Agents Selection (IAS) module and
the Reliable Agents Interaction (RAI) module. IAS enhances structural feature
awareness with phase maps and employs reinforcement learning principles to
efficiently select reliable agents. RAI then leverages these selected agents to
guide cross-modal interactions, effectively reducing mismatches and improving
overall robustness. Extensive experiments on the RGB-D Scenes v2 and 7-Scenes
benchmarks demonstrate that our method consistently achieves state-of-the-art
performance.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05965v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05965v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CoMA: Complementary Masking and Hierarchical Dynamic Multi-Window Self-Attention in a Unified Pre-training Framework
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Masked Autoencoders (MAE) achieve self-supervised learning of image
representations by randomly removing a portion of visual tokens and
reconstructing the original image as a pretext task, thereby significantly
enhancing pretraining efficiency and yielding excellent adaptability across
downstream tasks. However, MAE and other MAE-style paradigms that adopt random
masking generally require more pre-training epochs to maintain adaptability.
Meanwhile, ViT in MAE suffers from inefficient parameter use due to fixed
spatial resolution across layers. To overcome these limitations, we propose the
Complementary Masked Autoencoders (CoMA), which employ a complementary masking
strategy to ensure uniform sampling across all pixels, thereby improving
effective learning of all features and enhancing the model's adaptability.
Furthermore, we introduce DyViT, a hierarchical vision transformer that employs
a Dynamic Multi-Window Self-Attention (DM-MSA), significantly reducing the
parameters and FLOPs while improving fine-grained feature learning. Pre-trained
on ImageNet-1K with CoMA, DyViT matches the downstream performance of MAE using
only 12% of the pre-training epochs, demonstrating more effective learning. It
also attains a 10% reduction in pre-training time per epoch, further
underscoring its superior pre-training efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05929v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05929v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GABFusion: Rethinking Feature Fusion for Low-Bit Quantization of Multi-Task Networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite the effectiveness of quantization-aware training (QAT) in compressing
deep neural networks, its performance on multi-task architectures often
degrades significantly due to task-specific feature discrepancies and gradient
conflicts. To address these challenges, we propose Gradient-Aware Balanced
Feature Fusion (GABFusion), which dynamically balances gradient magnitudes and
fuses task-specific features in a quantization-friendly manner. We further
introduce Attention Distribution Alignment (ADA), a feature-level distillation
strategy tailored for quantized models. Our method demonstrates strong
generalization across network architectures and QAT algorithms, with
theoretical guarantees on gradient bias reduction. Extensive experiments
demonstrate that our strategy consistently enhances a variety of QAT methods
across different network architectures and bit-widths. On PASCAL VOC and COCO
datasets, the proposed approach achieves average mAP improvements of
approximately 3.3% and 1.6%, respectively. When applied to YOLOv5 under 4-bit
quantization, our method narrows the accuracy gap with the full-precision model
to only 1.7% on VOC, showcasing its effectiveness in preserving performance
under low-bit constraints. Notably, the proposed framework is modular, easy to
integrate, and compatible with any existing QAT technique-enhancing the
performance of quantized models without requiring modifications to the original
network architecture.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05898v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05898v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Towards a Humanized Social-Media Ecosystem: AI-Augmented HCI Design Patterns for Safety, Agency & Well-Being
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Social platforms connect billions of people, yet their engagement-first
algorithms often work on users rather than with them, amplifying stress,
misinformation, and a loss of control. We propose Human-Layer AI
(HL-AI)--user-owned, explainable intermediaries that sit in the browser between
platform logic and the interface. HL-AI gives people practical,
moment-to-moment control without requiring platform cooperation. We contribute
a working Chrome/Edge prototype implementing five representative pattern
frameworks--Context-Aware Post Rewriter, Post Integrity Meter, Granular Feed
Curator, Micro-Withdrawal Agent, and Recovery Mode--alongside a unifying
mathematical formulation balancing user utility, autonomy costs, and risk
thresholds. Evaluation spans technical accuracy, usability, and behavioral
outcomes. The result is a suite of humane controls that help users rewrite
before harm, read with integrity cues, tune feeds with intention, pause
compulsive loops, and seek shelter during harassment, all while preserving
agency through explanations and override options. This prototype offers a
practical path to retrofit today's feeds with safety, agency, and well-being,
inviting rigorous cross-cultural user evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05875v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05875v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        EndoIR: Degradation-Agnostic All-in-One Endoscopic Image Restoration via Noise-Aware Routing Diffusion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Endoscopic images often suffer from diverse and co-occurring degradations
such as low lighting, smoke, and bleeding, which obscure critical clinical
details. Existing restoration methods are typically task-specific and often
require prior knowledge of the degradation type, limiting their robustness in
real-world clinical use. We propose EndoIR, an all-in-one, degradation-agnostic
diffusion-based framework that restores multiple degradation types using a
single model. EndoIR introduces a Dual-Domain Prompter that extracts joint
spatial-frequency features, coupled with an adaptive embedding that encodes
both shared and task-specific cues as conditioning for denoising. To mitigate
feature confusion in conventional concatenation-based conditioning, we design a
Dual-Stream Diffusion architecture that processes clean and degraded inputs
separately, with a Rectified Fusion Block integrating them in a structured,
degradation-aware manner. Furthermore, Noise-Aware Routing Block improves
efficiency by dynamically selecting only noise-relevant features during
denoising. Experiments on SegSTRONG-C and CEC datasets demonstrate that EndoIR
achieves state-of-the-art performance across multiple degradation scenarios
while using fewer parameters than strong baselines, and downstream segmentation
experiments confirm its clinical utility.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05873v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05873v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CGCE: Classifier-Guided Concept Erasure in Generative Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in large-scale generative models have enabled the
creation of high-quality images and videos, but have also raised significant
safety concerns regarding the generation of unsafe content. To mitigate this,
concept erasure methods have been developed to remove undesirable concepts from
pre-trained models. However, existing methods remain vulnerable to adversarial
attacks that can regenerate the erased content. Moreover, achieving robust
erasure often degrades the model's generative quality for safe, unrelated
concepts, creating a difficult trade-off between safety and performance. To
address this challenge, we introduce Classifier-Guided Concept Erasure (CGCE),
an efficient plug-and-play framework that provides robust concept erasure for
diverse generative models without altering their original weights. CGCE uses a
lightweight classifier operating on text embeddings to first detect and then
refine prompts containing undesired concepts. This approach is highly scalable,
allowing for multi-concept erasure by aggregating guidance from several
classifiers. By modifying only unsafe embeddings at inference time, our method
prevents harmful content generation while preserving the model's original
quality on benign prompts. Extensive experiments show that CGCE achieves
state-of-the-art robustness against a wide range of red-teaming attacks. Our
approach also maintains high generative utility, demonstrating a superior
balance between safety and performance. We showcase the versatility of CGCE
through its successful application to various modern T2I and T2V models,
establishing it as a practical and effective solution for safe generative AI.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05865v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05865v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Enhancing Diffusion Model Guidance through Calibration and Regularization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Classifier-guided diffusion models have emerged as a powerful approach for
conditional image generation, but they suffer from overconfident predictions
during early denoising steps, causing the guidance gradient to vanish. This
paper introduces two complementary contributions to address this issue. First,
we propose a differentiable calibration objective based on the Smooth Expected
Calibration Error (Smooth ECE), which improves classifier calibration with
minimal fine-tuning and yields measurable improvements in Frechet Inception
Distance (FID). Second, we develop enhanced sampling guidance methods that
operate on off-the-shelf classifiers without requiring retraining. These
include tilted sampling with batch-level reweighting, adaptive
entropy-regularized sampling to preserve diversity, and a novel
f-divergence-based sampling strategy that strengthens class-consistent guidance
while maintaining mode coverage. Experiments on ImageNet 128x128 demonstrate
that our divergence-regularized guidance achieves an FID of 2.13 using a
ResNet-101 classifier, improving upon existing classifier-guided diffusion
methods while requiring no diffusion model retraining. The results show that
principled calibration and divergence-aware sampling provide practical and
effective improvements for classifier-guided diffusion.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05844v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05844v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Understanding Cross Task Generalization in Handwriting-Based Alzheimer's Screening via Vision Language Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Alzheimer's disease is a prevalent neurodegenerative disorder for which early
detection is critical. Handwriting-often disrupted in prodromal AD-provides a
non-invasive and cost-effective window into subtle motor and cognitive decline.
Existing handwriting-based AD studies, mostly relying on online trajectories
and hand-crafted features, have not systematically examined how task type
influences diagnostic performance and cross-task generalization. Meanwhile,
large-scale vision language models have demonstrated remarkable zero or
few-shot anomaly detection in natural images and strong adaptability across
medical modalities such as chest X-ray and brain MRI. However,
handwriting-based disease detection remains largely unexplored within this
paradigm. To close this gap, we introduce a lightweight Cross-Layer Fusion
Adapter framework that repurposes CLIP for handwriting-based AD screening. CLFA
implants multi-level fusion adapters within the visual encoder to progressively
align representations toward handwriting-specific medical cues, enabling
prompt-free and efficient zero-shot inference. Using this framework, we
systematically investigate cross-task generalization-training on a specific
handwriting task and evaluating on unseen ones-to reveal which task types and
writing patterns most effectively discriminate AD. Extensive analyses further
highlight characteristic stroke patterns and task-level factors that contribute
to early AD identification, offering both diagnostic insights and a benchmark
for handwriting-based cognitive assessment.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05841v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05841v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Hilbert-Guided Block-Sparse Local Attention
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The quadratic compute and memory costs of global self-attention severely
limit its use in high-resolution images. Local attention reduces complexity by
restricting attention to neighborhoods. Block-sparse kernels can further
improve the efficiency of local attention, but conventional local attention
patterns often fail to deliver significant speedups because tokens within a
window are not contiguous in the 1D sequence. This work proposes a novel method
for constructing windows and neighborhoods based on the Hilbert curve. Image
tokens are first reordered along a Hilbert curve, and windows and neighborhoods
are then formed on the reordered 1D sequence. From a block-sparse perspective,
this strategy significantly increases block sparsity and can be combined with
existing block-sparse kernels to improve the efficiency of 2D local attention.
Experiments show that the proposed Hilbert Window Attention and Hilbert Slide
Attention can accelerate window attention and slide attention by about
$4\times$ and $18\times$, respectively. To assess practicality, the strategy is
instantiated as the Hilbert Window Transformer and the Hilbert Neighborhood
Transformer, both of which achieve end-to-end speedups with minimal accuracy
loss. Overall, combining Hilbert-guided local attention with block-sparse
kernels offers a general and practical approach to enhancing the efficiency of
2D local attention for images. The code is available at
https://github.com/Yunge6666/Hilbert-Local-Attention.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05832v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05832v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VLAD-Grasp: Zero-shot Grasp Detection via Vision-Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Robotic grasping is a fundamental capability for autonomous manipulation;
however, most existing methods rely on large-scale expert annotations and
necessitate retraining to handle new objects. We present VLAD-Grasp, a
Vision-Language model Assisted zero-shot approach for Detecting grasps. From a
single RGB-D image, our method (1) prompts a large vision-language model to
generate a goal image where a straight rod "impales" the object, representing
an antipodal grasp, (2) predicts depth and segmentation to lift this generated
image into 3D, and (3) aligns generated and observed object point clouds via
principal component analysis and correspondence-free optimization to recover an
executable grasp pose. Unlike prior work, our approach is training-free and
does not rely on curated grasp datasets. Despite this, VLAD-Grasp achieves
performance that is competitive with or superior to that of state-of-the-art
supervised models on the Cornell and Jacquard datasets. We further demonstrate
zero-shot generalization to novel real-world objects on a Franka Research 3
robot, highlighting vision-language foundation models as powerful priors for
robotic manipulation.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05791v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05791v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Sign language recognition from skeletal data using graph and recurrent neural networks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This work presents an approach for recognizing isolated sign language
gestures using skeleton-based pose data extracted from video sequences. A
Graph-GRU temporal network is proposed to model both spatial and temporal
dependencies between frames, enabling accurate classification. The model is
trained and evaluated on the AUTSL (Ankara university Turkish sign language)
dataset, achieving high accuracy. Experimental results demonstrate the
effectiveness of integrating graph-based spatial representations with temporal
modeling, providing a scalable framework for sign language recognition. The
results of this approach highlight the potential of pose-driven methods for
sign language understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05772v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05772v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Cross-Modal Fine-Tuning of 3D Convolutional Foundation Models for ADHD Classification with Low-Rank Adaptation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Early diagnosis of attention-deficit/hyperactivity disorder (ADHD) in
children plays a crucial role in improving outcomes in education and mental
health. Diagnosing ADHD using neuroimaging data, however, remains challenging
due to heterogeneous presentations and overlapping symptoms with other
conditions. To address this, we propose a novel parameter-efficient transfer
learning approach that adapts a large-scale 3D convolutional foundation model,
pre-trained on CT images, to an MRI-based ADHD classification task. Our method
introduces Low-Rank Adaptation (LoRA) in 3D by factorizing 3D convolutional
kernels into 2D low-rank updates, dramatically reducing trainable parameters
while achieving superior performance. In a five-fold cross-validated evaluation
on a public diffusion MRI database, our 3D LoRA fine-tuning strategy achieved
state-of-the-art results, with one model variant reaching 71.9% accuracy and
another attaining an AUC of 0.716. Both variants use only 1.64 million
trainable parameters (over 113x fewer than a fully fine-tuned foundation
model). Our results represent one of the first successful cross-modal
(CT-to-MRI) adaptations of a foundation model in neuroimaging, establishing a
new benchmark for ADHD classification while greatly improving efficiency.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_06163v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.06163v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        CSGaze: Context-aware Social Gaze Prediction
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    A person's gaze offers valuable insights into their focus of attention, level
of social engagement, and confidence. In this work, we investigate how
contextual cues combined with visual scene and facial information can be
effectively utilized to predict and interpret social gaze patterns during
conversational interactions. We introduce CSGaze, a context aware multimodal
approach that leverages facial, scene information as complementary inputs to
enhance social gaze pattern prediction from multi-person images. The model also
incorporates a fine-grained attention mechanism centered on the principal
speaker, which helps in better modeling social gaze dynamics. Experimental
results show that CSGaze performs competitively with state-of-the-art methods
on GP-Static, UCO-LAEO and AVA-LAEO. Our findings highlight the role of
contextual cues in improving social gaze prediction. Additionally, we provide
initial explainability through generated attention scores, offering insights
into the model's decision-making process. We also demonstrate our model's
generalizability by testing our model on open set datasets that demonstrating
its robustness across diverse scenarios.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05955v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05955v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MoEGCL: Mixture of Ego-Graphs Contrastive Representation Learning for Multi-View Clustering
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In recent years, the advancement of Graph Neural Networks (GNNs) has
significantly propelled progress in Multi-View Clustering (MVC). However,
existing methods face the problem of coarse-grained graph fusion. Specifically,
current approaches typically generate a separate graph structure for each view
and then perform weighted fusion of graph structures at the view level, which
is a relatively rough strategy. To address this limitation, we present a novel
Mixture of Ego-Graphs Contrastive Representation Learning (MoEGCL). It mainly
consists of two modules. In particular, we propose an innovative Mixture of
Ego-Graphs Fusion (MoEGF), which constructs ego graphs and utilizes a
Mixture-of-Experts network to implement fine-grained fusion of ego graphs at
the sample level, rather than the conventional view-level fusion. Additionally,
we present the Ego Graph Contrastive Learning (EGCL) module to align the fused
representation with the view-specific representation. The EGCL module enhances
the representation similarity of samples from the same cluster, not merely from
the same sample, further boosting fine-grained graph representation. Extensive
experiments demonstrate that MoEGCL achieves state-of-the-art results in deep
multi-view clustering tasks. The source code is publicly available at
https://github.com/HackerHyper/MoEGCL.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05876v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05876v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MARAuder's Map: Motion-Aware Real-time Activity Recognition with Layout-Based Trajectories
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ambient sensor-based human activity recognition (HAR) in smart homes remains
challenging due to the need for real-time inference, spatially grounded
reasoning, and context-aware temporal modeling. Existing approaches often rely
on pre-segmented, within-activity data and overlook the physical layout of the
environment, limiting their robustness in continuous, real-world deployments.
In this paper, we propose MARAuder's Map, a novel framework for real-time
activity recognition from raw, unsegmented sensor streams. Our method projects
sensor activations onto the physical floorplan to generate trajectory-aware,
image-like sequences that capture the spatial flow of human movement. These
representations are processed by a hybrid deep learning model that jointly
captures spatial structure and temporal dependencies. To enhance temporal
awareness, we introduce a learnable time embedding module that encodes
contextual cues such as hour-of-day and day-of-week. Additionally, an
attention-based encoder selectively focuses on informative segments within each
observation window, enabling accurate recognition even under cross-activity
transitions and temporal ambiguity. Extensive experiments on multiple
real-world smart home datasets demonstrate that our method outperforms strong
baselines, offering a practical solution for real-time HAR in ambient sensor
environments.
                                    
                                    <div class="paper-links">
                                        <a href="https:///reports/2025-11-09/2511_05773v1.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2511.05773v1" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
        </div>
        
        

        <div class="footer">
            <p>生成时间: 2025-11-21 19:11:32</p>
            <p>访问地址: <a href="https://">https://</a></p>
        </div>
    </div>
</body>
</html>