<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>主流技术方向 - 2025-10-30</title>
    <style>
        :root {
            /* 系统配色 */
            --primary-color: #4f46e5;   /* Indigo */
            --direction-color: #f97316; /* Orange 500 */
            --direction-dark: #c2410c;  /* Orange 700 */
            --direction-light: #fff7ed; /* Orange 50 */
            --direction-border: #fdba74;/* Orange 300 */
            
            --bg-body: #f8fafc;
            --bg-card: #ffffff;
            --text-main: #0f172a;
            --text-secondary: #64748b;
            --text-light: #94a3b8;
            --border-color: #e2e8f0;
            
            --shadow-sm: 0 1px 2px 0 rgba(0, 0, 0, 0.05);
            --shadow-md: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -1px rgba(0, 0, 0, 0.06);
            
            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-main);
            line-height: 1.6;
            padding: 40px 20px;
            min-height: 100vh;
        }

        .container {
            max-width: 1000px;
            margin: 0 auto;
        }

        /* SVG 图标 */
        .icon {
            width: 1.1em;
            height: 1.1em;
            display: inline-block;
            vertical-align: middle;
            stroke-width: 2;
            stroke: currentColor;
            fill: none;
            stroke-linecap: round;
            stroke-linejoin: round;
        }

        /* 导航栏 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            font-size: 14px;
        }

        .back-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: flex;
            align-items: center;
            transition: color 0.2s;
        }

        .back-link:hover { color: var(--primary-color); }
        .back-link .icon { margin-right: 6px; }

        .date-badge {
            background-color: #e0e7ff;
            color: var(--primary-color);
            padding: 4px 12px;
            border-radius: 99px;
            font-weight: 600;
            font-size: 13px;
        }

        /* 头部 */
        .header {
            text-align: center;
            margin-bottom: 40px;
        }

        .header h1 {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            display: flex;
            align-items: center;
            justify-content: center;
            gap: 12px;
            margin-bottom: 12px;
        }
        
        .header h1 .icon { color: var(--direction-color); width: 36px; height: 36px; }
        
        .header .subtitle { 
            color: var(--text-secondary);
            font-size: 16px;
        }

        /* --- Tab 导航栏 --- */
        .tabs-container {
            position: sticky;
            top: 20px;
            z-index: 100;
            margin-bottom: 30px;
            /* 磨砂玻璃效果 */
            background: rgba(248, 250, 252, 0.9);
            backdrop-filter: blur(8px);
            padding: 10px 0;
            border-radius: 16px;
        }

        .tabs-scroll {
            display: flex;
            gap: 12px;
            overflow-x: auto;
            padding: 4px 4px 12px 4px; /* 底部留空间给滚动条或阴影 */
            
            /* 隐藏滚动条但保持可滚动 */
            scrollbar-width: none; /* Firefox */
            -ms-overflow-style: none;  /* IE 10+ */
            
            /* 鼠标交互优化 */
            cursor: grab; /* 提示可拖拽 */
            user-select: none; /* 防止拖拽时选中文字 */
        }
        
        .tabs-scroll::-webkit-scrollbar { 
            display: none; /* Chrome/Safari */
        }

        /* 拖拽时的光标状态 */
        .tabs-scroll.active {
            cursor: grabbing;
        }

        .tab-btn {
            flex-shrink: 0;
            background-color: var(--bg-card);
            color: var(--text-secondary);
            border: 1px solid var(--border-color);
            padding: 10px 20px;
            border-radius: 99px;
            font-size: 14px;
            font-weight: 600;
            cursor: pointer;
            transition: all 0.3s cubic-bezier(0.4, 0, 0.2, 1);
            box-shadow: var(--shadow-sm);
            display: flex;
            align-items: center;
            gap: 8px;
            /* 防止图片/文字干扰拖拽 */
            pointer-events: auto; 
        }

        .tab-btn:hover {
            border-color: var(--direction-border);
            color: var(--direction-color);
            transform: translateY(-2px);
        }

        .tab-btn.active {
            background-color: var(--direction-color);
            color: white;
            border-color: var(--direction-color);
            box-shadow: 0 4px 12px rgba(249, 115, 22, 0.3); /* Orange glow */
        }

        .tab-count {
            background-color: rgba(0,0,0,0.1);
            padding: 2px 8px;
            border-radius: 12px;
            font-size: 11px;
        }
        
        .tab-btn.active .tab-count {
            background-color: rgba(255,255,255,0.2);
        }

        /* --- 内容区域 --- */
        .tab-pane {
            display: none; /* 默认隐藏 */
            animation: fadeIn 0.3s ease-out;
        }
        
        .tab-pane.active {
            display: block;
        }
        
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(10px); }
            to { opacity: 1; transform: translateY(0); }
        }

        .direction-block {
            background-color: var(--bg-card);
            border-radius: 20px;
            border: 1px solid var(--border-color);
            box-shadow: var(--shadow-md);
            overflow: hidden;
        }

        .direction-header-info {
            background-color: var(--direction-light);
            padding: 24px 30px;
            border-bottom: 1px solid var(--direction-border);
        }
        
        .direction-title-lg {
            font-size: 22px;
            font-weight: 800;
            color: var(--direction-dark);
            margin-bottom: 8px;
            display: flex;
            align-items: center;
            gap: 10px;
        }
        
        .direction-desc-lg {
            font-size: 15px;
            color: #9a3412;
            opacity: 0.9;
        }

        /* --- 手风琴论文列表 --- */
        .paper-list {
            padding: 10px 30px 30px;
        }

        .paper-item {
            border-bottom: 1px solid var(--border-color);
        }
        
        .paper-item:last-child { border-bottom: none; }

        /* 折叠头部 (可点击) */
        .paper-header {
            padding: 20px 0;
            cursor: pointer;
            display: flex;
            align-items: flex-start;
            justify-content: space-between;
            gap: 16px;
            group: paper-header;
        }

        .paper-title-row {
            flex-grow: 1;
        }

        .paper-title {
            font-size: 17px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            transition: color 0.2s;
            margin-bottom: 6px;
        }
        
        .paper-header:hover .paper-title {
            color: var(--direction-color);
        }

        .paper-meta-preview {
            font-size: 13px;
            color: var(--text-light);
            display: flex;
            align-items: center;
            gap: 12px;
        }
        
        .score-badge {
            background-color: #f3f4f6;
            color: var(--text-secondary);
            padding: 2px 8px;
            border-radius: 4px;
            font-weight: 600;
            font-size: 12px;
        }
        
        .toggle-icon {
            color: var(--text-light);
            transition: transform 0.3s ease, color 0.2s;
            flex-shrink: 0;
            margin-top: 4px;
        }
        
        /* 激活状态样式 */
        .paper-item.expanded .toggle-icon {
            transform: rotate(180deg);
            color: var(--direction-color);
        }
        
        .paper-item.expanded .paper-title {
            color: var(--direction-color);
        }

        /* 折叠内容区 */
        .paper-body {
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease-out;
            padding-left: 4px; /* 微调对齐 */
        }
        
        .paper-content-inner {
            padding-bottom: 24px;
            color: var(--text-secondary);
            font-size: 15px;
            line-height: 1.7;
            text-align: justify;
        }
        
        .paper-links {
            margin-top: 12px;
            padding-top: 12px;
            border-top: 1px dashed var(--border-color);
            display: flex;
            gap: 16px;
            font-size: 13px;
        }
        
        .action-link {
            color: var(--primary-color);
            text-decoration: none;
            font-weight: 600;
            display: inline-flex;
            align-items: center;
            gap: 4px;
        }
        
        .action-link:hover { text-decoration: underline; }

        /* 空状态 */
        .empty-state {
            text-align: center;
            padding: 60px 20px;
            background-color: var(--bg-card);
            border-radius: 16px;
            border: 1px solid var(--border-color);
            color: var(--text-secondary);
        }

        .footer {
            margin-top: 60px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }
        
        .footer a { color: var(--text-secondary); text-decoration: none; }

        @media (max-width: 640px) {
            .container { padding: 20px 15px; }
            .direction-header-info { padding: 20px; }
            .paper-list { padding: 0 20px 20px; }
            .paper-title { font-size: 16px; }
        }
    </style>
    <script>
        document.addEventListener('DOMContentLoaded', function() {
            // 1. Tab 栏鼠标拖拽滚动 (Draggable Scroll)
            const slider = document.querySelector('.tabs-scroll');
            let isDown = false;
            let startX;
            let scrollLeft;
            let isDragging = false; // 区分是“点击”还是“拖拽”

            slider.addEventListener('mousedown', (e) => {
                isDown = true;
                isDragging = false;
                slider.classList.add('active');
                startX = e.pageX - slider.offsetLeft;
                scrollLeft = slider.scrollLeft;
            });

            slider.addEventListener('mouseleave', () => {
                isDown = false;
                slider.classList.remove('active');
            });

            slider.addEventListener('mouseup', () => {
                isDown = false;
                slider.classList.remove('active');
                // 如果是拖拽结束，为了防止触发 click，我们在 click 事件中做判断
                setTimeout(() => { isDragging = false; }, 0);
            });

            slider.addEventListener('mousemove', (e) => {
                if (!isDown) return;
                e.preventDefault(); // 防止选中文字
                const x = e.pageX - slider.offsetLeft;
                const walk = (x - startX) * 2; // 滚动速度系数
                slider.scrollLeft = scrollLeft - walk;
                
                // 如果移动距离超过 5px，则视为拖拽，不是点击
                if (Math.abs(walk) > 5) {
                    isDragging = true;
                }
            });

            // 2. Tab 切换逻辑
            const tabs = document.querySelectorAll('.tab-btn');
            const panes = document.querySelectorAll('.tab-pane');

            tabs.forEach(tab => {
                tab.addEventListener('click', (e) => {
                    // 如果刚才是在拖拽，则拦截点击，不切换 Tab
                    if (isDragging) {
                        e.preventDefault();
                        e.stopPropagation();
                        return;
                    }

                    // 移除所有激活状态
                    tabs.forEach(t => t.classList.remove('active'));
                    panes.forEach(p => p.classList.remove('active'));

                    // 激活当前
                    tab.classList.add('active');
                    const targetId = tab.getAttribute('data-target');
                    document.getElementById(targetId).classList.add('active');
                    
                    // 滚动 Tab 到可见区域 (移动端/拖拽后体验优化)
                    tab.scrollIntoView({ behavior: 'smooth', block: 'nearest', inline: 'center' });
                });
            });

            // 3. 论文折叠/展开逻辑
            const paperHeaders = document.querySelectorAll('.paper-header');

            paperHeaders.forEach(header => {
                header.addEventListener('click', function() {
                    const item = this.parentElement;
                    const body = item.querySelector('.paper-body');
                    
                    // 切换状态
                    item.classList.toggle('expanded');
                    
                    if (item.classList.contains('expanded')) {
                        body.style.maxHeight = body.scrollHeight + "px";
                    } else {
                        body.style.maxHeight = null;
                    }
                });
            });
        });
    </script>
</head>
<body>
    <div class="container">
        <!-- 导航 -->
        <div class="nav-bar">
            <a href="index.html" class="back-link">
                <svg class="icon" viewBox="0 0 24 24"><line x1="19" y1="12" x2="5" y2="12"></line><polyline points="12 19 5 12 12 5"></polyline></svg>
                返回每日简报
            </a>
            <div class="date-badge">2025-10-30</div>
        </div>

        <!-- 头部 -->
        <div class="header">
            <h1>
                <svg class="icon" viewBox="0 0 24 24"><circle cx="12" cy="12" r="10"></circle><polygon points="16.24 7.76 14.12 14.12 7.76 16.24 9.88 9.88 16.24 7.76"></polygon></svg>
                每日主流技术方向
            </h1>
            <div class="subtitle">聚合 RAG / LLM / Agent 等核心赛道，点击下方标签切换</div>
        </div>

        
        
        
        
        <!-- Tab 导航栏 (可拖拽) -->
        <div class="tabs-container">
            <div class="tabs-scroll">
                
                <button class="tab-btn active" data-target="tab-1">
                    Agent
                    <span class="tab-count">5</span>
                </button>
                
                <button class="tab-btn " data-target="tab-2">
                    Alignment
                    <span class="tab-count">6</span>
                </button>
                
                <button class="tab-btn " data-target="tab-3">
                    LLM
                    <span class="tab-count">24</span>
                </button>
                
                <button class="tab-btn " data-target="tab-4">
                    Multimodal
                    <span class="tab-count">21</span>
                </button>
                
                <button class="tab-btn " data-target="tab-5">
                    Optimization
                    <span class="tab-count">9</span>
                </button>
                
                <button class="tab-btn " data-target="tab-6">
                    Other
                    <span class="tab-count">8</span>
                </button>
                
                <button class="tab-btn " data-target="tab-7">
                    RAG
                    <span class="tab-count">2</span>
                </button>
                
                <button class="tab-btn " data-target="tab-8">
                    RL
                    <span class="tab-count">7</span>
                </button>
                
                <button class="tab-btn " data-target="tab-9">
                    Vision
                    <span class="tab-count">6</span>
                </button>
                
            </div>
        </div>

        <!-- 内容区域 -->
        <div class="content-wrapper">
            
            <div id="tab-1" class="tab-pane active">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Agent
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 5 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前在智能代理（Agent）领域的研究动态主要集中在多模态推理、代理系统的性能优化、复杂任务执行的评估、数据统一和交互搜索能力的提升等方面。研究者们正致力于提高语言代理在多样化和长时间任务中的表现，同时通过新的数据协议和评估基准来解决训练数据碎片化和用户查询模糊性的问题。这些趋势表明，未来的智能代理将更加注重实用性和适应性，具有更广泛的应用潜力。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in text-only large language models (LLMs), such as
DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models
remain fragile or entirely incapable when extended to multi-modal tasks.
Existing approaches largely rely on single-form captions, which lack diversity
and often fail to adapt across different types of Visual Question Answering
(VQA) benchmarks. As a result, they provide no principled or efficient channel
for transmitting fine-grained visual information. We introduce Seeing Eye, a
modular framework that unlocks multimodal reasoning in text-only LLMs through
an agent-based small VLM translator. This translator acts as a perception
agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively
distill multimodal inputs into structured intermediate representations (SIRs)
tailored to the question. These SIRs are then passed to the text-only LLM,
which serves as a reasoning agent. Crucially, the translator and reasoner
engage in multi-round feedback and interaction, enabling the extraction of
targeted visual details and yielding more confident answers. Experiments on
knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate
that Seeing Eye not only reduces inference cost but also surpasses much larger
end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision
translator with an 8B-parameter language reasoner outperforms a monolithic 32B
VLM on challenging knowledge-based questions. Our results highlight that
decoupling perception from reasoning via agent information flow offers a
scalable and plug-and-play pathway to multimodal reasoning, allowing strong
text-only LLMs to fully leverage their reasoning capabilities. Code is
available at: https://github.com/ulab-uiuc/SeeingEye
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25092.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25092" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MASPRM: Multi-Agent System Process Reward Model
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Practical deployment of Multi-Agent Systems (MAS) demands strong test-time
performance, motivating methods that guide inference-time search and
selectively spend compute to improve quality. We present the Multi-Agent System
Process Reward Model (MASPRM). It assigns per-action, per-agent values to
partial inter-agent transcripts and acts as an inference-time controller.
MASPRM is trained from multi-agent Monte Carlo Tree Search (MCTS) rollouts
without requiring step-level human annotations, by propagating returns to local
targets. At inference, MASPRM guides step-level beam search and MCTS, focusing
computation on promising branches and pruning early. On GSM8K and MATH,
MASPRM-guided decoding with an outcome reward model (ORM) applied to the final
answer, improves exact match (EM) over a single straight-through MAS pass by
+30.7 and +22.9 points, respectively. A MASPRM trained on GSM8K transfers
zero-shot to MATH without retraining, adding 8.4 EM points at the same
budget. MASPRM is a plug-in value model that estimates per-agent progress and
complements verifier-style decoders, enabling more reliable, compute-aware
multi-agent reasoning. Code: https://github.com/milad1378yz/MASPRM
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24803.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24803" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25726.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25726" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24702.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24702" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        InteractComp: Evaluating Search Agents With Ambiguous Queries
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24668.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24668" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-2" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Alignment
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 6 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前对齐（Alignment）技术方向的研究动态主要集中在提升模型的透明性、可解释性和多模态交互能力。研究者们正在开发更高效的生成模型和视觉语言模型，以确保模型在复杂任务中的推理能力和决策透明度。此外，针对模型在多模态环境中的说服力和优化策略的研究也逐渐增多，显示出对模型在实际应用中表现的关注。这些趋势表明，提升模型的对齐性不仅有助于增强其性能，还能降低潜在的误导性影响，具有重要的应用价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Gaperon: A Peppered English-French Generative Language Model Suite
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance. We
find that filtering for linguistic quality enhances text fluency and coherence
but yields subpar benchmark results, and that late deliberate contamination --
continuing training on data mixes that include test sets -- recovers
competitive scores while only reasonably harming generation quality. We discuss
how usual neural filtering can unintentionally amplify benchmark leakage. To
support further research, we also introduce harmless data poisoning during
pretraining, providing a realistic testbed for safety studies. By openly
releasing all models, datasets, code, and checkpoints, Gaperon establishes a
reproducible foundation for exploring the trade-offs between data curation,
evaluation, safety, and openness in multilingual language model development.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25771.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25771" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S-Chain: Structured Visual Chain-of-Thought For Medicine
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22728.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22728" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Batch Speculative Decoding Done Right
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3times throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22876.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22876" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22768.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22768" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22319.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22319" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_17439.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.17439" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-3" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            LLM
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 24 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前大语言模型（LLM）领域的研究动态集中在增强模型的推理能力和多模态理解上。研究者们通过强化学习和交互式探索，提升模型在诊断、空间推理及视频理解等复杂任务中的表现。同时，针对现有模型在多模态任务中的局限性，提出了新方法以优化信息流和策略，从而提高推理的效率和可靠性。这些研究不仅推动了LLM在实际应用中的潜力，也为未来的智能系统发展奠定了基础。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evolving Diagnostic Agents in a Virtual Clinical Environment
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24654.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24654" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25760.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25760" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22543.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22543" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in text-only large language models (LLMs), such as
DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models
remain fragile or entirely incapable when extended to multi-modal tasks.
Existing approaches largely rely on single-form captions, which lack diversity
and often fail to adapt across different types of Visual Question Answering
(VQA) benchmarks. As a result, they provide no principled or efficient channel
for transmitting fine-grained visual information. We introduce Seeing Eye, a
modular framework that unlocks multimodal reasoning in text-only LLMs through
an agent-based small VLM translator. This translator acts as a perception
agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively
distill multimodal inputs into structured intermediate representations (SIRs)
tailored to the question. These SIRs are then passed to the text-only LLM,
which serves as a reasoning agent. Crucially, the translator and reasoner
engage in multi-round feedback and interaction, enabling the extraction of
targeted visual details and yielding more confident answers. Experiments on
knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate
that Seeing Eye not only reduces inference cost but also surpasses much larger
end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision
translator with an 8B-parameter language reasoner outperforms a monolithic 32B
VLM on challenging knowledge-based questions. Our results highlight that
decoupling perception from reasoning via agent information flow offers a
scalable and plug-and-play pathway to multimodal reasoning, allowing strong
text-only LLMs to fully leverage their reasoning capabilities. Code is
available at: https://github.com/ulab-uiuc/SeeingEye
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25092.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25092" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in image reasoning methods, particularly "Thinking with
Images", have demonstrated remarkable success in Multimodal Large Language
Models (MLLMs); however, this dynamic reasoning paradigm has not yet been
extended to video reasoning tasks. In this paper, we propose Video-Thinker,
which empowers MLLMs to think with videos by autonomously leveraging their
intrinsic "grounding" and "captioning" capabilities to generate reasoning clues
throughout the inference process. To spark this capability, we construct
Video-Thinker-10K, a curated dataset featuring autonomous tool usage within
chain-of-thought reasoning sequences. Our training strategy begins with
Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group
Relative Policy Optimization (GRPO) to strengthen this reasoning capability.
Through this approach, Video-Thinker enables MLLMs to autonomously navigate
grounding and captioning tasks for video reasoning, eliminating the need for
constructing and calling external tools. Extensive experiments demonstrate that
Video-Thinker achieves significant performance gains on both in-domain tasks
and challenging out-of-domain video reasoning benchmarks, including
Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B
substantially outperforms existing baselines such as Video-R1 and establishes
state-of-the-art performance among 7B-sized MLLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23473.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23473" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24592.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24592" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        TheraMind: A Strategic and Adaptive Agent for Longitudinal Psychological Counseling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) in psychological counseling have attracted
increasing attention. However, existing approaches often lack emotional
understanding, adaptive strategies, and the use of therapeutic methods across
multiple sessions with long-term memory, leaving them far from real clinical
practice. To address these critical gaps, we introduce TheraMind, a strategic
and adaptive agent for longitudinal psychological counseling. The cornerstone
of TheraMind is a novel dual-loop architecture that decouples the complex
counseling process into an Intra-Session Loop for tactical dialogue management
and a Cross-Session Loop for strategic therapeutic planning. The Intra-Session
Loop perceives the patient's emotional state to dynamically select response
strategies while leveraging cross-session memory to ensure continuity.
Crucially, the Cross-Session Loop empowers the agent with long-term
adaptability by evaluating the efficacy of the applied therapy after each
session and adjusting the method for subsequent interactions. We validate our
approach in a high-fidelity simulation environment grounded in real clinical
cases. Extensive evaluations show that TheraMind outperforms other methods,
especially on multi-session metrics like Coherence, Flexibility, and
Therapeutic Attunement, validating the effectiveness of its dual-loop design in
emulating strategic, adaptive, and longitudinal therapeutic behavior. The code
is publicly available at https://0mwwm0.github.io/TheraMind/.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25758.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25758" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Parallel Loop Transformer for Efficient Test-Time Computation Scaling
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that
delivers the performance benefits of a deep, looped model but with the low
latency of a standard, non-looped model. PLT works using two key techniques.
First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by
computing different loops for different tokens at the same time, all within a
single pass. Second, to prevent memory costs from growing, we use an Efficient
Representation Enhancement strategy. This method shares the memory (KV cache)
from the first loop with all other loops. It then uses a Gated Sliding-Window
Attention (G-SWA) to combine this shared global information with local
information, maintaining high accuracy. Our experiments show that PLT achieves
the high accuracy of a traditional looped model but with almost no extra
latency or memory cost compared to a standard transformer.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24824.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24824" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Tool Decathlon: Benchmarking Language Agents for Diverse, Realistic, and Long-Horizon Task Execution
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Real-world language agents must handle complex, multi-step workflows across
diverse Apps. For instance, an agent may manage emails by coordinating with
calendars and file systems, or monitor a production database to detect
anomalies and generate reports following an operating manual. However, existing
language agent benchmarks often focus on narrow domains or simplified tasks
that lack the diversity, realism, and long-horizon complexity required to
evaluate agents' real-world performance. To address this gap, we introduce the
Tool Decathlon (dubbed as Toolathlon), a benchmark for language agents offering
diverse Apps and tools, realistic environment setup, and reliable
execution-based evaluation. Toolathlon spans 32 software applications and 604
tools, ranging from everyday platforms such as Google Calendar and Notion to
professional ones like WooCommerce, Kubernetes, and BigQuery. Most of the tools
are based on a high-quality set of Model Context Protocol (MCP) servers that we
may have revised or implemented ourselves. Unlike prior works, which primarily
ensure functional realism but offer limited environment state diversity, we
provide realistic initial environment states from real software, such as Canvas
courses with dozens of students or real financial spreadsheets. This benchmark
includes 108 manually sourced or crafted tasks in total, requiring interacting
with multiple Apps over around 20 turns on average to complete. Each task is
strictly verifiable through dedicated evaluation scripts. Comprehensive
evaluation of SOTA models highlights their significant shortcomings: the
best-performing model, Claude-4.5-Sonnet, achieves only a 38.6% success rate
with 20.2 tool calling turns on average, while the top open-weights model
DeepSeek-V3.2-Exp reaches 20.1%. We expect Toolathlon to drive the development
of more capable language agents for real-world, long-horizon task execution.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25726.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25726" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VFXMaster: Unlocking Dynamic Visual Effect Generation via In-Context Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Visual effects (VFX) are crucial to the expressive power of digital media,
yet their creation remains a major challenge for generative AI. Prevailing
methods often rely on the one-LoRA-per-effect paradigm, which is
resource-intensive and fundamentally incapable of generalizing to unseen
effects, thus limiting scalability and creation. To address this challenge, we
introduce VFXMaster, the first unified, reference-based framework for VFX video
generation. It recasts effect generation as an in-context learning task,
enabling it to reproduce diverse dynamic effects from a reference video onto
target content. In addition, it demonstrates remarkable generalization to
unseen effect categories. Specifically, we design an in-context conditioning
strategy that prompts the model with a reference example. An in-context
attention mask is designed to precisely decouple and inject the essential
effect attributes, allowing a single unified model to master the effect
imitation without information leakage. In addition, we propose an efficient
one-shot effect adaptation mechanism to boost generalization capability on
tough unseen effects from a single user-provided video rapidly. Extensive
experiments demonstrate that our method effectively imitates various categories
of effect information and exhibits outstanding generalization to out-of-domain
effects. To foster future research, we will release our code, models, and a
comprehensive dataset to the community.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25772.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25772" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Scaling Latent Reasoning via Looped Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Modern LLMs are trained to "think" primarily via explicit text generation,
such as chain-of-thought (CoT), which defers reasoning to post-training and
under-leverages pre-training data. We present and open-source Ouro, named after
the recursive Ouroboros, a family of pre-trained Looped Language Models
(LoopLM) that instead build reasoning into the pre-training phase through (i)
iterative computation in latent space, (ii) an entropy-regularized objective
for learned depth allocation, and (iii) scaling to 7.7T tokens. Ouro 1.4B and
2.6B models enjoy superior performance that match the results of up to 12B SOTA
LLMs across a wide range of benchmarks. Through controlled experiments, we show
this advantage stems not from increased knowledge capacity, but from superior
knowledge manipulation capabilities. We also show that LoopLM yields reasoning
traces more aligned with final outputs than explicit CoT. We hope our results
show the potential of LoopLM as a novel scaling direction in the reasoning era.
Our model could be found in: http://ouro-llm.github.io.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25741.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25741" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic Domains
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems. BhashaBench V1 contains 74,166 meticulously curated
question-answer pairs, with 52,494 in English and 21,672 in Hindi, sourced from
authentic government and domain-specific exams. It spans four major domains:
Agriculture, Legal, Finance, and Ayurveda, comprising 90+ subdomains and
covering 500+ topics, enabling fine-grained evaluation. Evaluation of 29+ LLMs
reveals significant domain and language specific performance gaps, with
especially large disparities in low-resource domains. For instance, GPT-4o
achieves 76.49% overall accuracy in Legal but only 59.74% in Ayurveda. Models
consistently perform better on English content compared to Hindi across all
domains. Subdomain-level analysis shows that areas such as Cyber Law,
International Finance perform relatively well, while Panchakarma, Seed Science,
and Human Rights remain notably weak. BhashaBench V1 provides a comprehensive
dataset for evaluating large language models across India's diverse knowledge
domains. It enables assessment of models' ability to integrate domain-specific
knowledge with bilingual understanding. All code, benchmarks, and resources are
publicly available to support open research.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25409.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25409" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ATOM: AdapTive and OptiMized dynamic temporal knowledge graph construction using LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In today's rapidly expanding data landscape, knowledge extraction from
unstructured text is vital for real-time analytics, temporal inference, and
dynamic memory frameworks. However, traditional static knowledge graph (KG)
construction often overlooks the dynamic and time-sensitive nature of
real-world data, limiting adaptability to continuous changes. Moreover, recent
zero- or few-shot approaches that avoid domain-specific fine-tuning or reliance
on prebuilt ontologies often suffer from instability across multiple runs, as
well as incomplete coverage of key facts. To address these challenges, we
introduce ATOM (AdapTive and OptiMized), a few-shot and scalable approach that
builds and continuously updates Temporal Knowledge Graphs (TKGs) from
unstructured texts. ATOM splits input documents into minimal, self-contained
"atomic" facts, improving extraction exhaustivity and stability. Then, it
constructs atomic TKGs from these facts while employing a dual-time modeling
that distinguishes when information is observed from when it is valid. The
resulting atomic TKGs are subsequently merged in parallel. Empirical
evaluations demonstrate that ATOM achieves ~18% higher exhaustivity, ~17%
better stability, and over 90% latency reduction compared to baseline methods,
demonstrating a strong scalability potential for dynamic TKG construction.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22590.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22590" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond Understanding: Evaluating the Pragmatic Gap in LLMs' Cultural Processing of Figurative Language
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We present a comprehensive evaluation of the ability of large language models
(LLMs) to process culturally grounded language, specifically to understand and
pragmatically use figurative expressions that encode local knowledge and
cultural nuance. Using figurative language as a proxy for cultural nuance and
local knowledge, we design evaluation tasks for contextual understanding,
pragmatic use, and connotation interpretation in Arabic and English. We
evaluate 22 open- and closed-source LLMs on Egyptian Arabic idioms,
multidialectal Arabic proverbs, and English proverbs. Our results show a
consistent hierarchy: the average accuracy for Arabic proverbs is 4.29% lower
than for English proverbs, and performance for Egyptian idioms is 10.28% lower
than for Arabic proverbs. For the pragmatic use task, accuracy drops by 14.07%
relative to understanding, though providing contextual idiomatic sentences
improves accuracy by 10.66%. Models also struggle with connotative meaning,
reaching at most 85.58% agreement with human annotators on idioms with 100%
inter-annotator agreement. These findings demonstrate that figurative language
serves as an effective diagnostic for cultural reasoning: while LLMs can often
interpret figurative meaning, they face challenges in using it appropriately.
To support future research, we release Kinayat, the first dataset of Egyptian
Arabic idioms designed for both figurative understanding and pragmatic use
evaluation.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23828.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23828" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Batch Speculative Decoding Done Right
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Speculative decoding speeds up LLM inference by using a small draft model to
propose multiple tokens that a target model verifies in parallel. Extending
this idea to batches is essential for production serving, but it introduces the
ragged tensor problem: sequences in the same batch accept different numbers of
draft tokens, breaking right-alignment and corrupting position IDs, attention
masks, and KV-cache state. We show that several existing batch implementations
violate output equivalence-the fundamental requirement that speculative
decoding must produce identical token sequences to standard autoregressive
generation. These violations occur precisely due to improper handling of the
ragged tensor problem. In response, we (1) characterize the synchronization
requirements that guarantee correctness, (2) present a correctness-first batch
speculative decoding EQSPEC that exposes realignment as consuming 40% of
overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences
and dynamically forms same-length groups, to reduce the realignment overhead
while preserving per-sequence speculative speedups. On the SpecBench dataset,
across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our
approach achieves up to 3times throughput improvement at batch size 8
compared to batch size 1, with efficient scaling through batch size 8, while
maintaining 95% output equivalence. Our method requires no custom kernels and
integrates cleanly with existing inference stacks. Our code is available at
https://github.com/eBay/spec_dec.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22876.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22876" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+ Languages and Cultures
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    To date, there exist almost no culturally-specific evaluation benchmarks for
large language models (LLMs) that cover a large number of languages and
cultures. In this paper, we present Global PIQA, a participatory commonsense
reasoning benchmark for over 100 languages, constructed by hand by 335
researchers from 65 countries around the world. The 116 language varieties in
Global PIQA cover five continents, 14 language families, and 23 writing
systems. In the non-parallel split of Global PIQA, over 50% of examples
reference local foods, customs, traditions, or other culturally-specific
elements. We find that state-of-the-art LLMs perform well on Global PIQA in
aggregate, but they exhibit weaker performance in lower-resource languages (up
to a 37% accuracy gap, despite random chance at 50%). Open models generally
perform worse than proprietary models. Global PIQA highlights that in many
languages and cultures, everyday knowledge remains an area for improvement,
alongside more widely-discussed capabilities such as complex reasoning and
expert knowledge. Beyond its uses for LLM evaluation, we hope that Global PIQA
provides a glimpse into the wide diversity of cultures in which human language
is embedded.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24081.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24081" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Agent Data Protocol: Unifying Datasets for Diverse, Effective Fine-tuning of LLM Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Public research results on large-scale supervised finetuning of AI agents
remain relatively rare, since the collection of agent training data presents
unique challenges. In this work, we argue that the bottleneck is not a lack of
underlying data sources, but that a large variety of data is fragmented across
heterogeneous formats, tools, and interfaces. To this end, we introduce the
agent data protocol (ADP), a light-weight representation language that serves
as an "interlingua" between agent datasets in diverse formats and unified agent
training pipelines downstream. The design of ADP is expressive enough to
capture a large variety of tasks, including API/tool use, browsing, coding,
software engineering, and general agentic workflows, while remaining simple to
parse and train on without engineering at a per-dataset level. In experiments,
we unified a broad collection of 13 existing agent training datasets into ADP
format, and converted the standardized ADP data into training-ready formats for
multiple agent frameworks. We performed SFT on these data, and demonstrated an
average performance gain of ~20% over corresponding base models, and delivers
state-of-the-art or near-SOTA performance on standard coding, browsing, tool
use, and research benchmarks, without domain-specific tuning. All code and data
are released publicly, in the hope that ADP could help lower the barrier to
standardized, scalable, and reproducible agent training.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24702.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24702" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22373.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22373" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Understanding objects at the level of their constituent parts is fundamental
to advancing computer vision, graphics, and robotics. While datasets like
PartNet have driven progress in 3D part understanding, their reliance on
untextured geometries and expert-dependent annotation limits scalability and
usability. We introduce PartNeXt, a next-generation dataset addressing these
gaps with over 23,000 high-quality, textured 3D models annotated with
fine-grained, hierarchical part labels across 50 categories. We benchmark
PartNeXt on two tasks: (1) class-agnostic part segmentation, where
state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with
fine-grained and leaf-level parts, and (2) 3D part-centric question answering,
a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary
part grounding. Additionally, training Point-SAM on PartNeXt yields substantial
gains over PartNet, underscoring the dataset's superior quality and diversity.
By combining scalable annotation, texture-aware labels, and multi-task
evaluation, PartNeXt opens new avenues for research in structured 3D
understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_20155.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.20155" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        STAR-Bench: Probing Deep Spatio-Temporal Reasoning as Audio 4D Intelligence
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Despite rapid progress in Multi-modal Large Language Models and Large
Audio-Language Models, existing audio benchmarks largely test semantics that
can be recovered from text captions, masking deficits in fine-grained
perceptual reasoning. We formalize audio 4D intelligence that is defined as
reasoning over sound dynamics in time and 3D space, and introduce STAR-Bench to
measure it. STAR-Bench combines a Foundational Acoustic Perception setting (six
attributes under absolute and relative regimes) with a Holistic Spatio-Temporal
Reasoning setting that includes segment reordering for continuous and discrete
processes and spatial tasks spanning static localization, multi-source
relations, and dynamic trajectories. Our data curation pipeline uses two
methods to ensure high-quality samples. For foundational tasks, we use
procedurally synthesized and physics-simulated audio. For holistic data, we
follow a four-stage process that includes human annotation and final selection
based on human performance. Unlike prior benchmarks where caption-only
answering reduces accuracy slightly, STAR-Bench induces far larger drops
(-31.5\% temporal, -35.2\% spatial), evidencing its focus on linguistically
hard-to-describe cues. Evaluating 19 models reveals substantial gaps compared
with humans and a capability hierarchy: closed-source models are bottlenecked
by fine-grained perception, while open-source models lag across perception,
knowledge, and reasoning. Our STAR-Bench provides critical insights and a clear
path forward for developing future models with a more robust understanding of
the physical world.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24693.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24693" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22037.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22037" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        InteractComp: Evaluating Search Agents With Ambiguous Queries
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24668.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24668" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VisCoder2: Building Multi-Language Visualization Coding Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Large language models (LLMs) have recently enabled coding agents capable of
generating, executing, and revising visualization code. However, existing
models often fail in practical workflows due to limited language coverage,
unreliable execution, and lack of iterative correction mechanisms. Progress has
been constrained by narrow datasets and benchmarks that emphasize single-round
generation and single-language tasks. To address these challenges, we introduce
three complementary resources for advancing visualization coding agents.
VisCode-Multi-679K is a large-scale, supervised dataset containing 679K
validated and executable visualization samples with multi-turn correction
dialogues across 12 programming languages. VisPlotBench is a benchmark for
systematic evaluation, featuring executable tasks, rendered outputs, and
protocols for both initial generation and multi-round self-debug. Finally, we
present VisCoder2, a family of multi-language visualization models trained on
VisCode-Multi-679K. Experiments show that VisCoder2 significantly outperforms
strong open-source baselines and approaches the performance of proprietary
models like GPT-4.1, with further gains from iterative self-debug, reaching
82.4% overall execution pass rate at the 32B scale, particularly in symbolic or
compiler-dependent languages.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23642.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23642" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Routing Matters in MoE: Scaling Diffusion Transformers with Explicit Routing Guidance
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Mixture-of-Experts (MoE) has emerged as a powerful paradigm for scaling model
capacity while preserving computational efficiency. Despite its notable success
in large language models (LLMs), existing attempts to apply MoE to Diffusion
Transformers (DiTs) have yielded limited gains. We attribute this gap to
fundamental differences between language and visual tokens. Language tokens are
semantically dense with pronounced inter-token variation, while visual tokens
exhibit spatial redundancy and functional heterogeneity, hindering expert
specialization in vision MoE. To this end, we present ProMoE, an MoE framework
featuring a two-step router with explicit routing guidance that promotes expert
specialization. Specifically, this guidance encourages the router to partition
image tokens into conditional and unconditional sets via conditional routing
according to their functional roles, and refine the assignments of conditional
image tokens through prototypical routing with learnable prototypes based on
semantic content. Moreover, the similarity-based expert allocation in latent
space enabled by prototypical routing offers a natural mechanism for
incorporating explicit semantic guidance, and we validate that such guidance is
crucial for vision MoE. Building on this, we propose a routing contrastive loss
that explicitly enhances the prototypical routing process, promoting
intra-expert coherence and inter-expert diversity. Extensive experiments on
ImageNet benchmark demonstrate that ProMoE surpasses state-of-the-art methods
under both Rectified Flow and DDPM training objectives. Code and models will be
made publicly available.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24711.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24711" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-4" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Multimodal
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 21 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前多模态技术的研究动态主要集中在提升模型对复杂生物分子互动、空间推理、语言理解和生成等任务的能力。研究者们通过构建统一的训练框架和多模态推理机制，旨在实现更高效的数据利用和更准确的任务执行。此外，随着视频推理和代码智能等领域的探索，未来多模态模型在理解和生成方面的潜在应用价值将不断扩大，推动跨领域的创新发展。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ODesign: A World Model for Biomolecular Interaction Design
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Biomolecular interactions underpin almost all biological processes, and their
rational design is central to programming new biological functions. Generative
AI models have emerged as powerful tools for molecular design, yet most remain
specialized for individual molecular types and lack fine-grained control over
interaction details. Here we present ODesign, an all-atom generative world
model for all-to-all biomolecular interaction design. ODesign allows scientists
to specify epitopes on arbitrary targets and generate diverse classes of
binding partners with fine-grained control. Across entity-, token-, and
atom-level benchmarks in the protein modality, ODesign demonstrates superior
controllability and performance to modality-specific baselines. Extending
beyond proteins, it generalizes to nucleic acid and small-molecule design,
enabling interaction types such as protein-binding RNA/DNA and RNA/DNA-binding
ligands that were previously inaccessible. By unifying multimodal biomolecular
interactions within a single generative framework, ODesign moves toward a
general-purpose molecular world model capable of programmable design. ODesign
is available at https://odesign.lglab.ac.cn ,
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22304.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22304" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Multimodal Spatial Reasoning in the Large Model Era: A Survey and Benchmarks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Humans possess spatial reasoning abilities that enable them to understand
spaces through multimodal observations, such as vision and sound. Large
multimodal reasoning models extend these abilities by learning to perceive and
reason, showing promising performance across diverse spatial tasks. However,
systematic reviews and publicly available benchmarks for these models remain
limited. In this survey, we provide a comprehensive review of multimodal
spatial reasoning tasks with large models, categorizing recent progress in
multimodal large language models (MLLMs) and introducing open benchmarks for
evaluation. We begin by outlining general spatial reasoning, focusing on
post-training techniques, explainability, and architecture. Beyond classical 2D
tasks, we examine spatial relationship reasoning, scene and layout
understanding, as well as visual question answering and grounding in 3D space.
We also review advances in embodied AI, including vision-language navigation
and action models. Additionally, we consider emerging modalities such as audio
and egocentric video, which contribute to novel spatial understanding through
new sensors. We believe this survey establishes a solid foundation and offers
insights into the growing field of multimodal spatial reasoning. Updated
information about this survey, codes and implementation of the open benchmarks
can be found at https://github.com/zhengxuJosh/Awesome-Spatial-Reasoning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25760.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25760" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PairUni: Pairwise Training for Unified Multimodal Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25682.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25682" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SeeingEye: Agentic Information Flow Unlocks Multimodal Reasoning In Text-only LLMs
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in text-only large language models (LLMs), such as
DeepSeek-R1, demonstrate remarkable reasoning ability. However, these models
remain fragile or entirely incapable when extended to multi-modal tasks.
Existing approaches largely rely on single-form captions, which lack diversity
and often fail to adapt across different types of Visual Question Answering
(VQA) benchmarks. As a result, they provide no principled or efficient channel
for transmitting fine-grained visual information. We introduce Seeing Eye, a
modular framework that unlocks multimodal reasoning in text-only LLMs through
an agent-based small VLM translator. This translator acts as a perception
agent: it can invoke specialized tools (e.g., OCR and crop) and iteratively
distill multimodal inputs into structured intermediate representations (SIRs)
tailored to the question. These SIRs are then passed to the text-only LLM,
which serves as a reasoning agent. Crucially, the translator and reasoner
engage in multi-round feedback and interaction, enabling the extraction of
targeted visual details and yielding more confident answers. Experiments on
knowledge-intensive VQA benchmarks, including MMMU and MIA-Bench, demonstrate
that Seeing Eye not only reduces inference cost but also surpasses much larger
end-to-end VLMs. For example, an instantiation combining a 3B-parameter vision
translator with an 8B-parameter language reasoner outperforms a monolithic 32B
VLM on challenging knowledge-based questions. Our results highlight that
decoupling perception from reasoning via agent information flow offers a
scalable and plug-and-play pathway to multimodal reasoning, allowing strong
text-only LLMs to fully leverage their reasoning capabilities. Code is
available at: https://github.com/ulab-uiuc/SeeingEye
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25092.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25092" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        JanusCoder: Towards a Foundational Visual-Programmatic Interface for Code Intelligence
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    The scope of neural code intelligence is rapidly expanding beyond text-based
source code to encompass the rich visual outputs that programs generate. This
visual dimension is critical for advanced applications like flexible content
generation and precise, program-driven editing of visualizations. However,
progress has been impeded by the scarcity of high-quality multimodal code data,
a bottleneck stemming from challenges in synthesis and quality assessment. To
address these challenges, we make contributions from both a data and modeling
perspective. We first introduce a complete synthesis toolkit that leverages
reciprocal synergies between data modalities to efficiently produce a
large-scale, high-quality corpus spanning from standard charts to complex
interactive web UIs and code-driven animations. Leveraging this toolkit, we
construct JanusCode-800K, the largest multimodal code corpus to date. This
powers the training of our models, JanusCoder and JanusCoderV, which establish
a visual-programmatic interface for generating code from textual instructions,
visual inputs, or a combination of both. Our unified model is a departure from
existing approaches that build specialized models for isolated tasks. Extensive
experiments on both text-centric and vision-centric coding tasks demonstrate
the superior performance of the JanusCoder series, with our 7B to 14B scale
models approaching or even exceeding the performance of commercial models.
Furthermore, extensive analysis provides key insights into harmonizing
programmatic logic with its visual expression. Our code and checkpoints will
are available at https://github.com/InternLM/JanusCoder.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23538.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23538" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in image reasoning methods, particularly "Thinking with
Images", have demonstrated remarkable success in Multimodal Large Language
Models (MLLMs); however, this dynamic reasoning paradigm has not yet been
extended to video reasoning tasks. In this paper, we propose Video-Thinker,
which empowers MLLMs to think with videos by autonomously leveraging their
intrinsic "grounding" and "captioning" capabilities to generate reasoning clues
throughout the inference process. To spark this capability, we construct
Video-Thinker-10K, a curated dataset featuring autonomous tool usage within
chain-of-thought reasoning sequences. Our training strategy begins with
Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group
Relative Policy Optimization (GRPO) to strengthen this reasoning capability.
Through this approach, Video-Thinker enables MLLMs to autonomously navigate
grounding and captioning tasks for video reasoning, eliminating the need for
constructing and calling external tools. Extensive experiments demonstrate that
Video-Thinker achieves significant performance gains on both in-domain tasks
and challenging out-of-domain video reasoning benchmarks, including
Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B
substantially outperforms existing baselines such as Video-R1 and establishes
state-of-the-art performance among 7B-sized MLLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23473.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23473" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Rethinking Driving World Model as Synthetic Data Generator for Perception Tasks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advancements in driving world models enable controllable generation of
high-quality RGB videos or multimodal videos. Existing methods primarily focus
on metrics related to generation quality and controllability. However, they
often overlook the evaluation of downstream perception tasks, which are
really crucial for the performance of autonomous driving. Existing
methods usually leverage a training strategy that first pretrains on synthetic
data and finetunes on real data, resulting in twice the epochs compared to the
baseline (real data only). When we double the epochs in the baseline, the
benefit of synthetic data becomes negligible. To thoroughly demonstrate the
benefit of synthetic data, we introduce Dream4Drive, a novel synthetic data
generation framework designed for enhancing the downstream perception tasks.
Dream4Drive first decomposes the input video into several 3D-aware guidance
maps and subsequently renders the 3D assets onto these guidance maps. Finally,
the driving world model is fine-tuned to produce the edited, multi-view
photorealistic videos, which can be used to train the downstream perception
models. Dream4Drive enables unprecedented flexibility in generating multi-view
corner cases at scale, significantly boosting corner case perception in
autonomous driving. To facilitate future research, we also contribute a
large-scale 3D asset dataset named DriveObj3D, covering the typical categories
in driving scenarios and enabling diverse 3D-aware video editing. We conduct
comprehensive experiments to show that Dream4Drive can effectively boost the
performance of downstream perception models under various training epochs.
Page: https://wm-research.github.io/Dream4Drive/ GitHub Link:
https://github.com/wm-research/Dream4Drive
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_19195.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.19195" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25590.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25590" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24821.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24821" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S-Chain: Structured Visual Chain-of-Thought For Medicine
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22728.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22728" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has delivered
impressive gains in mathematical and multimodal reasoning and has become a
standard post-training paradigm for contemporary language and vision-language
models. However, the RLVR recipe introduces a significant risk of capability
regression, where models forget foundational skills after prolonged training
without employing regularization strategies. We empirically confirm this
concern, observing that open-source reasoning models suffer performance
degradation on core capabilities such as perception and faithfulness. While
imposing regularization terms like KL divergence can help prevent deviation
from the base model, these terms are calculated on the current task, thus they
do not guarantee broader knowledge. Meanwhile, commonly used experience replay
across heterogeneous domains makes it nontrivial to decide how much training
focus each objective should receive. To address this, we propose RECAP-a replay
strategy with dynamic objective reweighting for general knowledge preservation.
Our reweighting mechanism adapts in an online manner using short-horizon
signals of convergence and instability, shifting the post-training focus away
from saturated objectives and toward underperforming or volatile ones. Our
method is end-to-end and readily applicable to existing RLVR pipelines without
training additional models or heavy tuning. Extensive experiments on benchmarks
based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our
method, which not only preserves general capabilities but also improves
reasoning by enabling more flexible trade-offs among in-task rewards.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_21978.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.21978" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Latent Chain-of-Thought for Visual Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Chain-of-thought (CoT) reasoning is critical for improving the
interpretability and reliability of Large Vision-Language Models (LVLMs).
However, existing training algorithms such as SFT, PPO, and GRPO may not
generalize well across unseen reasoning tasks and heavily rely on a biased
reward model. To address this challenge, we reformulate reasoning in LVLMs as
posterior inference and propose a scalable training algorithm based on
amortized variational inference. By leveraging diversity-seeking reinforcement
learning algorithms, we introduce a novel sparse reward function for
token-level learning signals that encourage diverse, high-likelihood latent
CoT, overcoming deterministic sampling limitations and avoiding reward hacking.
Additionally, we implement a Bayesian inference-scaling strategy that replaces
costly Best-of-N and Beam Search with a marginal likelihood to efficiently rank
optimal rationales and answers. We empirically demonstrate that the proposed
method enhances the state-of-the-art LVLMs on seven reasoning benchmarks, in
terms of effectiveness, generalization, and interpretability.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23925.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23925" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        MMPersuade: A Dataset and Evaluation Framework for Multimodal Persuasion
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As Large Vision-Language Models (LVLMs) are increasingly deployed in domains
such as shopping, health, and news, they are exposed to pervasive persuasive
content. A critical question is how these models function as persuadees-how and
why they can be influenced by persuasive multimodal inputs. Understanding both
their susceptibility to persuasion and the effectiveness of different
persuasive strategies is crucial, as overly persuadable models may adopt
misleading beliefs, override user preferences, or generate unethical or unsafe
outputs when exposed to manipulative messages. We introduce MMPersuade, a
unified framework for systematically studying multimodal persuasion dynamics in
LVLMs. MMPersuade contributes (i) a comprehensive multimodal dataset that pairs
images and videos with established persuasion principles across commercial,
subjective and behavioral, and adversarial contexts, and (ii) an evaluation
framework that quantifies both persuasion effectiveness and model
susceptibility via third-party agreement scoring and self-estimated token
probabilities on conversation histories. Our study of six leading LVLMs as
persuadees yields three key insights: (i) multimodal inputs substantially
increase persuasion effectiveness-and model susceptibility-compared to text
alone, especially in misinformation scenarios; (ii) stated prior preferences
decrease susceptibility, yet multimodal information maintains its persuasive
advantage; and (iii) different strategies vary in effectiveness across
contexts, with reciprocity being most potent in commercial and subjective
contexts, and credibility and logic prevailing in adversarial contexts. By
jointly analyzing persuasion effectiveness and susceptibility, MMPersuade
provides a principled foundation for developing models that are robust,
preference-consistent, and ethically aligned when engaging with persuasive
multimodal content.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22768.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22768" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SAO-Instruct: Free-form Audio Editing using Natural Language Instructions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Generative models have made significant progress in synthesizing
high-fidelity audio from short textual descriptions. However, editing existing
audio using natural language has remained largely underexplored. Current
approaches either require the complete description of the edited audio or are
constrained to predefined edit instructions that lack flexibility. In this
work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of
editing audio clips using any free-form natural language instruction. To train
our model, we create a dataset of audio editing triplets (input audio, edit
instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual
editing pipeline. Although partially trained on synthetic data, our model
generalizes well to real in-the-wild audio clips and unseen edit instructions.
We demonstrate that SAO-Instruct achieves competitive performance on objective
metrics and outperforms other audio editing approaches in a subjective
listening study. To encourage future research, we release our code and model
weights.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22795.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22795" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce UltraHR-100K, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning
on detail-critical denoising steps, and (ii) Soft-Weighting Frequency
Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
https://github.com/NJU-PCALab/UltraHR-100k{here}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_20661.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.20661" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Existing vision-language-action (VLA) models act in 3D real-world but are
typically built on 2D encoders, leaving a spatial reasoning gap that limits
generalization and adaptability. Recent 3D integration techniques for VLAs
either require specialized sensors and transfer poorly across modalities, or
inject weak cues that lack geometry and degrade vision-language alignment. In
this work, we introduce FALCON (From Spatial to Action), a novel paradigm that
injects rich 3D spatial tokens into the action head. FALCON leverages spatial
foundation models to deliver strong geometric priors from RGB alone, and
includes an Embodied Spatial Model that can optionally fuse depth, or pose for
higher fidelity when available, without retraining or architectural changes. To
preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced
Action Head rather than being concatenated into the vision-language backbone.
These designs enable FALCON to address limitations in spatial representation,
modality transferability, and alignment. In comprehensive evaluations across
three simulation benchmarks and eleven real-world tasks, our proposed FALCON
achieves state-of-the-art performance, consistently surpasses competitive
baselines, and remains robust under clutter, spatial-prompt conditioning, and
variations in object scale and height.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_17439.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.17439" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        VisJudge-Bench: Aesthetics and Quality Assessment of Visualizations
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Visualization, a domain-specific yet widely used form of imagery, is an
effective way to turn complex datasets into intuitive insights, and its value
depends on whether data are faithfully represented, clearly communicated, and
aesthetically designed. However, evaluating visualization quality is
challenging: unlike natural images, it requires simultaneous judgment across
data encoding accuracy, information expressiveness, and visual aesthetics.
Although multimodal large language models (MLLMs) have shown promising
performance in aesthetic assessment of natural images, no systematic benchmark
exists for measuring their capabilities in evaluating visualizations. To
address this, we propose VisJudge-Bench, the first comprehensive benchmark for
evaluating MLLMs' performance in assessing visualization aesthetics and
quality. It contains 3,090 expert-annotated samples from real-world scenarios,
covering single visualizations, multiple visualizations, and dashboards across
32 chart types. Systematic testing on this benchmark reveals that even the most
advanced MLLMs (such as GPT-5) still exhibit significant gaps compared to human
experts in judgment, with a Mean Absolute Error (MAE) of 0.551 and a
correlation with human ratings of only 0.429. To address this issue, we propose
VisJudge, a model specifically designed for visualization aesthetics and
quality assessment. Experimental results demonstrate that VisJudge
significantly narrows the gap with human judgment, reducing the MAE to 0.442 (a
19.8% reduction) and increasing the consistency with human experts to 0.681 (a
58.7% improvement) compared to GPT-5. The benchmark is available at
https://github.com/HKUSTDial/VisJudgeBench.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22373.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22373" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Uniform Discrete Diffusion with Metric Path for Video Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24717.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24717" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22037.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22037" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Group Relative Attention Guidance for Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24657.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24657" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Game-TARS: Pretrained Foundation Models for Scalable Generalist Multimodal Game Agents
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We present Game-TARS, a generalist game agent trained with a unified,
scalable action space anchored to human-aligned native keyboard-mouse inputs.
Unlike API- or GUI-based approaches, this paradigm enables large-scale
continual pre-training across heterogeneous domains, including OS, web, and
simulation games. Game-TARS is pre-trained on over 500B tokens with diverse
trajectories and multimodal data. Key techniques include a decaying continual
loss to reduce causal confusion and an efficient Sparse-Thinking strategy that
balances reasoning depth and inference cost. Experiments show that Game-TARS
achieves about 2 times the success rate over the previous sota model on
open-world Minecraft tasks, is close to the generality of fresh humans in
unseen web 3d games, and outperforms GPT-5, Gemini-2.5-Pro, and Claude-4-Sonnet
in FPS benchmarks. Scaling results on training-time and test-time confirm that
the unified action space sustains improvements when scaled to cross-game and
multimodal data. Our results demonstrate that simple, scalable action
representations combined with large-scale pre-training provide a promising path
toward generalist agents with broad computer-use abilities.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23691.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23691" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-5" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Optimization
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 9 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前优化技术方向的研究动态集中在强化学习与多模态模型的结合，强调在复杂任务中提升推理能力和生成效率。研究者们逐渐认识到传统奖励机制的局限性，开始探索更为细致的奖励设计，如过程挖掘与反思性自我形式化，以更好地捕捉推理过程中的细微差异。此外，针对特定区域的自适应生成方法也在图像编辑中展现出潜在价值，表明未来的研究可能会更加注重任务特异性与数据异质性之间的平衡。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22543.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22543" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PairUni: Pairwise Training for Unified Multimodal Language Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Unified vision-language models (UVLMs) must perform both understanding and
generation within a single architecture, but these tasks rely on heterogeneous
data and supervision, making it difficult to balance them during reinforcement
learning (RL). We propose PairUni, a unified framework that reorganizes data
into understanding-generation (UG) pairs and aligns optimization accordingly.
We first use GPT-o3 to augment single-task data, generating captions for
understanding samples and question-answer (QA) pairs for generation samples,
forming aligned pairs from the same instance. Additionally, for each generation
sample, we retrieve a semantically related understanding example to form a
retrieved pair, linking different but related data points. These paired
structures expose cross-task semantic correspondences and support consistent
policy learning. To leverage this structure, we present Pair-GPRO, a pair-aware
variant based on Group Relative Policy Optimization. It assigns a similarity
score to each pair to modulate the advantage, strengthening learning from
well-aligned examples and reducing task interference. We curate a high-quality
dataset of 16K UG pairs named PairUG for RL fine-tuning and evaluate PairUni on
the powerful Janus-Pro UVLMs. Our approach achieves balanced improvements on
various UVLMs, outperforming strong UVLM RL baselines. Code:
https://github.com/Haochen-Wang409/PairUni{github.com/Haochen-Wang409/PairUni}
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25682.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25682" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reasoning-Aware GRPO using Process Mining
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25065.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25065" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ReForm: Reflective Autoformalization with Prospective Bounded Sequence Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Autoformalization, which translates natural language mathematics into
machine-verifiable formal statements, is critical for using formal mathematical
reasoning to solve math problems stated in natural language. While Large
Language Models can generate syntactically correct formal statements, they
often fail to preserve the original problem's semantic intent. This limitation
arises from the LLM approaches' treating autoformalization as a simplistic
translation task which lacks mechanisms for self-reflection and iterative
refinement that human experts naturally employ. To address these issues, we
propose ReForm, a Reflective Autoformalization method that tightly integrates
semantic consistency evaluation into the autoformalization process. This
enables the model to iteratively generate formal statements, assess its
semantic fidelity, and self-correct identified errors through progressive
refinement. To effectively train this reflective model, we introduce
Prospective Bounded Sequence Optimization (PBSO), which employs different
rewards at different sequence positions to ensure that the model develops both
accurate autoformalization and correct semantic validations, preventing
superficial critiques that would undermine the purpose of reflection. Extensive
experiments across four autoformalization benchmarks demonstrate that ReForm
achieves an average improvement of 17.2 percentage points over the strongest
baselines. To further ensure evaluation reliability, we introduce
ConsistencyCheck, a benchmark of 859 expert-annotated items that not only
validates LLMs as judges but also reveals that autoformalization is inherently
difficult: even human experts produce semantic errors in up to 38.5% of cases.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24592.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24592" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25590.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25590" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22319.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22319" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Optimize Any Topology: A Foundation Model for Shape- and Resolution-Free Structural Topology Optimization
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Structural topology optimization (TO) is central to engineering design but
remains computationally intensive due to complex physics and hard constraints.
Existing deep-learning methods are limited to fixed square grids, a few
hand-coded boundary conditions, and post-hoc optimization, preventing general
deployment. We introduce Optimize Any Topology (OAT), a foundation-model
framework that directly predicts minimum-compliance layouts for arbitrary
aspect ratios, resolutions, volume fractions, loads, and fixtures. OAT combines
a resolution- and shape-agnostic autoencoder with an implicit neural-field
decoder and a conditional latent-diffusion model trained on OpenTO, a new
corpus of 2.2 million optimized structures covering 2 million unique
boundary-condition configurations. On four public benchmarks and two
challenging unseen tests, OAT lowers mean compliance up to 90% relative to the
best prior models and delivers sub-1 second inference on a single GPU across
resolutions from 64 x 64 to 256 x 256 and aspect ratios as high as 10:1. These
results establish OAT as a general, fast, and resolution-free framework for
physics-aware topology optimization and provide a large-scale dataset to spur
further research in generative modeling for inverse design. Code & data can be
found at https://github.com/ahnobari/OptimizeAnyTopology.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23667.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23667" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce UltraHR-100K, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning
on detail-critical denoising steps, and (ii) Soft-Weighting Frequency
Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
https://github.com/NJU-PCALab/UltraHR-100k{here}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_20661.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.20661" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22264.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22264" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-6" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Other
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 8 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前技术方向的研究动态主要集中在模型的创新与优化上。扩散模型的理论基础得到进一步探讨，强调了数据与噪声之间的转化过程；同时，分布式智能和共识机制的应用为AI推理提供了新的解决方案。此外，自然语言处理在音频编辑和专利文本嵌入方面的进展，展示了生成模型在多样化任务中的潜力。整体趋势是向着更高效、更灵活的模型设计与应用迈进，具有广泛的实用价值。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        The Principles of Diffusion Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    This monograph presents the core principles that have guided the development
of diffusion models, tracing their origins and showing how diverse formulations
arise from shared mathematical ideas. Diffusion modeling starts by defining a
forward process that gradually corrupts data into noise, linking the data
distribution to a simple prior through a continuum of intermediate
distributions. The goal is to learn a reverse process that transforms noise
back into data while recovering the same intermediates. We describe three
complementary views. The variational view, inspired by variational
autoencoders, sees diffusion as learning to remove noise step by step. The
score-based view, rooted in energy-based modeling, learns the gradient of the
evolving data distribution, indicating how to nudge samples toward more likely
regions. The flow-based view, related to normalizing flows, treats generation
as following a smooth path that moves samples from noise to data under a
learned velocity field. These perspectives share a common backbone: a
time-dependent velocity field whose flow transports a simple prior to the data.
Sampling then amounts to solving a differential equation that evolves noise
into data along a continuous trajectory. On this foundation, the monograph
discusses guidance for controllable generation, efficient numerical solvers,
and diffusion-motivated flow-map models that learn direct mappings between
arbitrary times. It provides a conceptual and mathematically grounded
understanding of diffusion models for readers with basic deep-learning
knowledge.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_21890.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.21890" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Fortytwo: Swarm Inference with Peer-Ranked Consensus
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    As centralized AI hits compute ceilings and diminishing returns from
ever-larger training runs, meeting demand requires an inference layer that
scales horizontally in both capacity and capability. We present Fortytwo, a
novel protocol that leverages swarm intelligence principles and distributed
pairwise ranking consensus to achieve superior performance in AI inference. Our
approach reimagines collaboration among AI nodes using swarm inference: a
peer-ranked, reputation-weighted consensus across heterogeneous models that
surfaces the highest-quality responses. Using pairwise ranking with a custom
Bradley-Terry-style aggregation model, we demonstrate that swarm inference
substantially outperforms majority voting, achieving 85.90% on GPQA Diamond
versus 68.69% for majority voting with the same model set - an improvement of
+17.21 percentage points (approximately +25.1% relative). The protocol
incorporates on-chain reputation so node influence adapts to demonstrated
accuracy over time, yielding a meritocratic consensus that filters low-quality
or malicious participants. To resist Sybil attacks, Fortytwo employs
proof-of-capability in its consensus: nodes must successfully complete
calibration/test requests and stake reputation to enter ranking rounds, making
multi-identity attacks economically unattractive while preserving openness.
Across six challenging benchmarks, including GPQA Diamond, LiveCodeBench, and
AIME, our evaluation indicates higher accuracy and strong resilience to
adversarial and noisy free-form prompting (e.g., prompt-injection degradation
of only 0.12% versus 6.20% for a monolithic single-model baseline), while
retaining practical deployability. Together, these results establish a
foundation for decentralized AI systems - democratizing access to high-quality
inference through collective intelligence without sacrificing reliability or
security.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24801.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24801" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SAO-Instruct: Free-form Audio Editing using Natural Language Instructions
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Generative models have made significant progress in synthesizing
high-fidelity audio from short textual descriptions. However, editing existing
audio using natural language has remained largely underexplored. Current
approaches either require the complete description of the edited audio or are
constrained to predefined edit instructions that lack flexibility. In this
work, we introduce SAO-Instruct, a model based on Stable Audio Open capable of
editing audio clips using any free-form natural language instruction. To train
our model, we create a dataset of audio editing triplets (input audio, edit
instruction, output audio) using Prompt-to-Prompt, DDPM inversion, and a manual
editing pipeline. Although partially trained on synthetic data, our model
generalizes well to real in-the-wild audio clips and unseen edit instructions.
We demonstrate that SAO-Instruct achieves competitive performance on objective
metrics and outperforms other audio editing approaches in a subjective
listening study. To encourage future research, we release our code and model
weights.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22795.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22795" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PatenTEB: A Comprehensive Benchmark and Model Family for Patent Text Embedding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Patent text embeddings enable prior art search, technology landscaping, and
patent analysis, yet existing benchmarks inadequately capture patent-specific
challenges. We introduce PatenTEB, a comprehensive benchmark comprising 15
tasks across retrieval, classification, paraphrase, and clustering, with 2.06
million examples. PatenTEB employs domain-stratified splits, domain specific
hard negative mining, and systematic coverage of asymmetric
fragment-to-document matching scenarios absent from general embedding
benchmarks. We develop the patembed model family through multi-task training,
spanning 67M to 344M parameters with context lengths up to 4096 tokens.
External validation shows strong generalization: patembed-base achieves
state-of-the-art on MTEB BigPatentClustering.v2 (0.494 V-measure vs. 0.445
previous best), while patembed-large achieves 0.377 NDCG@100 on DAPFAM.
Systematic ablations reveal that multi-task training improves external
generalization despite minor benchmark costs, and that domain-pretrained
initialization provides consistent advantages across task families. All
resources will be made available at https://github.com/iliass-y/patenteb.
Keywords: patent retrieval, sentence embeddings, multi-task learning,
asymmetric retrieval, benchmark evaluation, contrastive learning.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22264.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22264" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Uniform Discrete Diffusion with Metric Path for Video Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24717.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24717" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining, Finetuning, and Decoding the Curse of Multilinguality
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2. Our analyses of the experiments shed
light on multilingual learning dynamics, transfer properties between languages,
and the curse of multilinguality. First, we derive a cross-lingual transfer
matrix, empirically measuring mutual benefit scores between 38 x 38=1444
language pairs. Second, we derive a language-agnostic scaling law that reveals
how to optimally scale model size and data when adding languages without
sacrificing performance. Third, we identify the computational crossover points
for when to pretrain from scratch versus finetune from multilingual
checkpoints. We hope these findings provide the scientific foundation for
democratizing scaling laws across languages, and enable practitioners to
efficiently scale models -- beyond English-first AI.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22037.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22037" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Group Relative Attention Guidance for Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24657.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24657" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        InteractComp: Evaluating Search Agents With Ambiguous Queries
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Language agents have demonstrated remarkable potential in web search and
information retrieval. However, these search agents assume user queries are
complete and unambiguous, an assumption that diverges from reality where users
begin with incomplete queries requiring clarification through interaction. Yet
most agents lack interactive mechanisms during the search process, and existing
benchmarks cannot assess this capability. To address this gap, we introduce
InteractComp, a benchmark designed to evaluate whether search agents can
recognize query ambiguity and actively interact to resolve it during search.
Following the principle of easy to verify, interact to disambiguate, we
construct 210 expert-curated questions across 9 domains through a
target-distractor methodology that creates genuine ambiguity resolvable only
through interaction. Evaluation of 17 models reveals striking failure: the best
model achieves only 13.73% accuracy despite 71.50% with complete context,
exposing systematic overconfidence rather than reasoning deficits. Forced
interaction produces dramatic gains, demonstrating latent capability current
strategies fail to engage. Longitudinal analysis shows interaction capabilities
stagnated over 15 months while search performance improved seven-fold,
revealing a critical blind spot. This stagnation, coupled with the immediate
feedback inherent to search tasks, makes InteractComp a valuable resource for
both evaluating and training interaction capabilities in search agents. The
code is available at https://github.com/FoundationAgents/InteractComp.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24668.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24668" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-7" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RAG
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 2 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前RAG技术方向的研究动态主要集中在提升生成模型的真实性和透明性。研究者们关注如何在动态领域（如在线游戏）和医疗视觉语言模型中建立有效的评估基准和数据集，以确保生成结果的可靠性和可解释性。这一趋势显示出RAG系统在多样化应用场景中的潜在价值，尤其是在需要实时更新和高准确度的领域。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        ChronoPlay: A Framework for Modeling Dual Dynamics and Authenticity in Game RAG Benchmarks
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Retrieval Augmented Generation (RAG) systems are increasingly vital in
dynamic domains like online gaming, yet the lack of a dedicated benchmark has
impeded standardized evaluation in this area. The core difficulty lies in Dual
Dynamics: the constant interplay between game content updates and the shifting
focus of the player community. Furthermore, the necessity of automating such a
benchmark introduces a critical requirement for player-centric authenticity to
ensure generated questions are realistic. To address this integrated challenge,
we introduce ChronoPlay, a novel framework for the automated and continuous
generation of game RAG benchmarks. ChronoPlay utilizes a dual-dynamic update
mechanism to track both forms of change, and a dual-source synthesis engine
that draws from official sources and player community to ensure both factual
correctness and authentic query patterns. We instantiate our framework on three
distinct games to create the first dynamic RAG benchmark for the gaming domain,
offering new insights into model performance under these complex and realistic
conditions. Code is avaliable at: https://github.com/hly1998/ChronoPlay.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_18455.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.18455" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        S-Chain: Structured Visual Chain-of-Thought For Medicine
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Faithful reasoning in medical vision-language models (VLMs) requires not only
accurate predictions but also transparent alignment between textual rationales
and visual evidence. While Chain-of-Thought (CoT) prompting has shown promise
in medical visual question answering (VQA), no large-scale expert-level dataset
has captured stepwise reasoning with precise visual grounding. We introduce
S-Chain, the first large-scale dataset of 12,000 expert-annotated medical
images with bounding boxes and structured visual CoT (SV-CoT), explicitly
linking visual regions to reasoning steps. The dataset further supports 16
languages, totaling over 700k VQA pairs for broad multilingual applicability.
Using S-Chain, we benchmark state-of-the-art medical VLMs (ExGra-Med,
LLaVA-Med) and general-purpose VLMs (Qwen2.5-VL, InternVL2.5), showing that
SV-CoT supervision significantly improves interpretability, grounding fidelity,
and robustness. Beyond benchmarking, we study its synergy with
retrieval-augmented generation, revealing how domain knowledge and visual
grounding interact during autoregressive reasoning. Finally, we propose a new
mechanism that strengthens the alignment between visual evidence and reasoning,
improving both reliability and efficiency. S-Chain establishes a new benchmark
for grounded medical reasoning and paves the way toward more trustworthy and
explainable medical VLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22728.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22728" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-8" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            RL
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 7 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前强化学习（RL）领域的研究动态主要集中在提升大语言模型（LLMs）和多模态模型的推理能力与适应性。研究者们探索了通过交互式学习和反馈机制来优化诊断过程、增强推理策略的有效性，以及利用过程挖掘技术来改进奖励机制。此外，针对模型在长期训练中可能出现的能力遗忘问题，提出了多种正则化策略以维持模型的基础技能。这些研究不仅推动了智能体在复杂任务中的表现，也为未来的应用提供了重要的理论基础和实践指导。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Evolving Diagnostic Agents in a Virtual Clinical Environment
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    In this paper, we present a framework for training large language models
(LLMs) as diagnostic agents with reinforcement learning, enabling them to
manage multi-turn diagnostic processes, adaptively select examinations, and
commit to final diagnoses. Unlike instruction-tuned models trained on static
case summaries, our method acquires diagnostic strategies through interactive
exploration and outcome-based feedback. Our contributions are fourfold: (i) We
present DiagGym, a diagnostics world model trained with electronic health
records that emits examination outcomes conditioned on patient history and
recommended examination, serving as a virtual clinical environment for
realistic diagnosis training and evaluation; (ii) We train DiagAgent via
end-to-end, multi-turn reinforcement learning to learn diagnostic policies that
optimize both information yield and diagnostic accuracy; (iii) We introduce
DiagBench, a diagnostic benchmark comprising 750 cases with physician-validated
examination recommendations and 99 cases annotated with 973 physician-written
rubrics on diagnosis process; (iv) we demonstrate superior performance across
diverse diagnostic settings. DiagAgent significantly outperforms 10
state-of-the-art LLMs, including DeepSeek-v3 and GPT-4o, as well as two
prompt-engineered agents. In single-turn settings, DiagAgent achieves 9.34%
higher diagnostic accuracy and 44.03% improvement in examination recommendation
hit ratio. In end-to-end settings, it delivers 15.12% increase in diagnostic
accuracy and 23.09% boost in examination recommendation F1 score. In
rubric-based evaluation, it surpasses the next-best model, Claude-sonnet-4, by
7.1% in weighted rubric score. These findings indicate that learning policies
in interactive clinical environments confers dynamic and clinically meaningful
diagnostic management abilities unattainable through passive training alone.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24654.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24654" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        FAPO: Flawed-Aware Policy Optimization for Efficient and Reliable Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has emerged as a
promising paradigm for enhancing the reasoning capabilities of large language
models (LLMs). In this context, models explore reasoning trajectories and
exploit rollouts with correct answers as positive signals for policy
optimization. However, these rollouts might involve flawed patterns such as
answer-guessing and jump-in-reasoning. Such flawed-positive rollouts are
rewarded identically to fully correct ones, causing policy models to
internalize these unreliable reasoning patterns. In this work, we first conduct
a systematic study of flawed-positive rollouts in RL and find that they enable
rapid capability gains during the early optimization stage, while constraining
reasoning capability later by reinforcing unreliable patterns. Building on
these insights, we propose Flawed-Aware Policy Optimization (FAPO), which
presents a parameter-free reward penalty for flawed-positive rollouts, enabling
the policy to leverage them as useful shortcuts in the warm-up stage, securing
stable early gains, while gradually shifting optimization toward reliable
reasoning in the later refinement stage. To accurately and comprehensively
detect flawed-positive rollouts, we introduce a generative reward model (GenRM)
with a process-level reward that precisely localizes reasoning errors.
Experiments show that FAPO is effective in broad domains, improving outcome
correctness, process reliability, and training stability without increasing the
token budget.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22543.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22543" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Reasoning-Aware GRPO using Process Mining
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning (RL)-based post-training has been crucial for enabling
multi-step reasoning in large reasoning models (LRMs), yet current reward
schemes are typically outcome-centric. We propose PM4GRPO, a reasoning-aware
Group Relative Policy Optimization (GRPO) that augments standard answer/format
rewards with signals over the reasoning procedure. To this end, process mining
techniques are utilized to compute a scalar conformance reward that measures
how closely a policy model's reasoning aligns with the pretrained teacher
model. The empirical results on five benchmarks demonstrate that PM4GRPO
significantly outperforms existing methodologies for GRPO-based post-training.
These results highlight that leveraging process mining for reasoning-aware GRPO
effectively enhances the reasoning capabilities of policy models.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25065.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25065" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Video-Thinker: Sparking "Thinking with Videos" via Reinforcement Learning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recent advances in image reasoning methods, particularly "Thinking with
Images", have demonstrated remarkable success in Multimodal Large Language
Models (MLLMs); however, this dynamic reasoning paradigm has not yet been
extended to video reasoning tasks. In this paper, we propose Video-Thinker,
which empowers MLLMs to think with videos by autonomously leveraging their
intrinsic "grounding" and "captioning" capabilities to generate reasoning clues
throughout the inference process. To spark this capability, we construct
Video-Thinker-10K, a curated dataset featuring autonomous tool usage within
chain-of-thought reasoning sequences. Our training strategy begins with
Supervised Fine-Tuning (SFT) to learn the reasoning format, followed by Group
Relative Policy Optimization (GRPO) to strengthen this reasoning capability.
Through this approach, Video-Thinker enables MLLMs to autonomously navigate
grounding and captioning tasks for video reasoning, eliminating the need for
constructing and calling external tools. Extensive experiments demonstrate that
Video-Thinker achieves significant performance gains on both in-domain tasks
and challenging out-of-domain video reasoning benchmarks, including
Video-Holmes, CG-Bench-Reasoning, and VRBench. Our Video-Thinker-7B
substantially outperforms existing baselines such as Video-R1 and establishes
state-of-the-art performance among 7B-sized MLLMs.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_23473.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.23473" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Beyond Reasoning Gains: Mitigating General Capabilities Forgetting in Large Reasoning Models
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Reinforcement learning with verifiable rewards (RLVR) has delivered
impressive gains in mathematical and multimodal reasoning and has become a
standard post-training paradigm for contemporary language and vision-language
models. However, the RLVR recipe introduces a significant risk of capability
regression, where models forget foundational skills after prolonged training
without employing regularization strategies. We empirically confirm this
concern, observing that open-source reasoning models suffer performance
degradation on core capabilities such as perception and faithfulness. While
imposing regularization terms like KL divergence can help prevent deviation
from the base model, these terms are calculated on the current task, thus they
do not guarantee broader knowledge. Meanwhile, commonly used experience replay
across heterogeneous domains makes it nontrivial to decide how much training
focus each objective should receive. To address this, we propose RECAP-a replay
strategy with dynamic objective reweighting for general knowledge preservation.
Our reweighting mechanism adapts in an online manner using short-horizon
signals of convergence and instability, shifting the post-training focus away
from saturated objectives and toward underperforming or volatile ones. Our
method is end-to-end and readily applicable to existing RLVR pipelines without
training additional models or heavy tuning. Extensive experiments on benchmarks
based on Qwen2.5-VL-3B and Qwen2.5-VL-7B demonstrate the effectiveness of our
method, which not only preserves general capabilities but also improves
reasoning by enabling more flexible trade-offs among in-task rewards.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_21978.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.21978" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        GRPO-Guard: Mitigating Implicit Over-Optimization in Flow Matching via Regulated Clipping
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, GRPO-based reinforcement learning has shown remarkable progress in
optimizing flow-matching models, effectively improving their alignment with
task-specific rewards. Within these frameworks, the policy update relies on
importance-ratio clipping to constrain overconfident positive and negative
gradients. However, in practice, we observe a systematic shift in the
importance-ratio distribution-its mean falls below 1 and its variance differs
substantially across timesteps. This left-shifted and inconsistent distribution
prevents positive-advantage samples from entering the clipped region, causing
the mechanism to fail in constraining overconfident positive updates. As a
result, the policy model inevitably enters an implicit over-optimization
stage-while the proxy reward continues to increase, essential metrics such as
image quality and text-prompt alignment deteriorate sharply, ultimately making
the learned policy impractical for real-world use. To address this issue, we
introduce GRPO-Guard, a simple yet effective enhancement to existing GRPO
frameworks. Our method incorporates ratio normalization, which restores a
balanced and step-consistent importance ratio, ensuring that PPO clipping
properly constrains harmful updates across denoising timesteps. In addition, a
gradient reweighting strategy equalizes policy gradients over noise conditions,
preventing excessive updates from particular timestep regions. Together, these
designs act as a regulated clipping mechanism, stabilizing optimization and
substantially mitigating implicit over-optimization without relying on heavy KL
regularization. Extensive experiments on multiple diffusion backbones (e.g.,
SD3.5M, Flux.1-dev) and diverse proxy tasks demonstrate that GRPO-Guard
significantly reduces over-optimization while maintaining or even improving
generation quality.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_22319.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.22319" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        SPICE: Self-Play In Corpus Environments Improves Reasoning
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Self-improving systems require environmental interaction for continuous
adaptation. We introduce SPICE (Self-Play In Corpus Environments), a
reinforcement learning framework where a single model acts in two roles: a
Challenger that mines documents from a large corpus to generate diverse
reasoning tasks, and a Reasoner that solves them. Through adversarial dynamics,
the Challenger creates an automatic curriculum at the frontier of the
Reasoner's capability, while corpus grounding provides the rich,
near-inexhaustible external signal necessary for sustained improvement. Unlike
existing ungrounded self-play methods that offer more limited benefits, SPICE
achieves consistent gains across mathematical (+8.9%) and general reasoning
(+9.8%) benchmarks on multiple model families. Our analysis reveals how
document grounding is a key ingredient in SPICE to continuously generate its
own increasingly challenging goals and achieve them, enabling sustained
self-improvement.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24684.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24684" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
            <div id="tab-9" class="tab-pane ">
                <div class="direction-block">
                    <!-- 头部描述 -->
                    <div class="direction-header-info">
                        <div class="direction-title-lg">
                            <svg class="icon" viewBox="0 0 24 24"><path d="M22 11.08V12a10 10 0 1 1-5.93-9.14"></path><polyline points="22 4 12 14.01 9 11.01"></polyline></svg>
                            Vision
                        </div>
                        <div class="direction-desc-lg">
                            本方向共收录 6 篇精选论文，按推荐分数排序。点击论文标题查看摘要。
                        </div>
                        
                        <div class="direction-desc-lg" style="margin-top: 6px; font-weight: 500; color: var(--text-main);">
                            今日概览：当前视觉技术研究的主要动态集中在提高图像和视频生成的效率与质量上。论文中提出的自适应区域生成、稀疏统一架构以及高质量数据集等方法，反映出对特定区域处理和多模态智能的重视。同时，针对细粒度理解和视频生成的创新方法也表明，研究者们正致力于克服现有技术的局限，推动计算机视觉向更高的精度和灵活性发展。这些进展不仅提升了视觉生成的效果，还为未来的应用场景提供了更广泛的可能性。
                        </div>
                        
                    </div>

                    <!-- 论文列表 (手风琴) -->
                    <div class="paper-list">
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        RegionE: Adaptive Region-Aware Generation for Efficient Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, instruction-based image editing (IIE) has received widespread
attention. In practice, IIE often modifies only specific regions of an image,
while the remaining areas largely remain unchanged. Although these two types of
regions differ significantly in generation difficulty and computational
redundancy, existing IIE models do not account for this distinction, instead
applying a uniform generation process across the entire image. This motivates
us to propose RegionE, an adaptive, region-aware generation framework that
accelerates IIE tasks without additional training. Specifically, the RegionE
framework consists of three main components: 1) Adaptive Region Partition. We
observed that the trajectory of unedited regions is straight, allowing for
multi-step denoised predictions to be inferred in a single step. Therefore, in
the early denoising stages, we partition the image into edited and unedited
regions based on the difference between the final estimated result and the
reference image. 2) Region-Aware Generation. After distinguishing the regions,
we replace multi-step denoising with one-step prediction for unedited areas.
For edited regions, the trajectory is curved, requiring local iterative
denoising. To improve the efficiency and quality of local iterative generation,
we propose the Region-Instruction KV Cache, which reduces computational cost
while incorporating global information. 3) Adaptive Velocity Decay Cache.
Observing that adjacent timesteps in edited regions exhibit strong velocity
similarity, we further propose an adaptive velocity decay cache to accelerate
the local denoising process. We applied RegionE to state-of-the-art IIE base
models, including Step1X-Edit, FLUX.1 Kontext, and Qwen-Image-Edit. RegionE
achieved acceleration factors of 2.57, 2.41, and 2.06. Evaluations by GPT-4o
confirmed that semantic and perceptual fidelity were well preserved.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_25590.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.25590" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Ming-Flash-Omni: A Sparse, Unified Architecture for Multimodal Perception and Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    We propose Ming-Flash-Omni, an upgraded version of Ming-Omni, built upon a
sparser Mixture-of-Experts (MoE) variant of Ling-Flash-2.0 with 100 billion
total parameters, of which only 6.1 billion are active per token. This
architecture enables highly efficient scaling (dramatically improving
computational efficiency while significantly expanding model capacity) and
empowers stronger unified multimodal intelligence across vision, speech, and
language, representing a key step toward Artificial General Intelligence (AGI).
Compared to its predecessor, the upgraded version exhibits substantial
improvements across multimodal understanding and generation. We significantly
advance speech recognition capabilities, achieving state-of-the-art performance
in contextual ASR and highly competitive results in dialect-aware ASR. In image
generation, Ming-Flash-Omni introduces high-fidelity text rendering and
demonstrates marked gains in scene consistency and identity preservation during
image editing. Furthermore, Ming-Flash-Omni introduces generative segmentation,
a capability that not only achieves strong standalone segmentation performance
but also enhances spatial control in image generation and improves editing
consistency. Notably, Ming-Flash-Omni achieves state-of-the-art results in
text-to-image generation and generative segmentation, and sets new records on
all 12 contextual ASR benchmarks, all within a single unified architecture.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24821.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24821" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        UltraHR-100K: Enhancing UHR Image Synthesis with A Large-Scale High-Quality Dataset
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Ultra-high-resolution (UHR) text-to-image (T2I) generation has seen notable
progress. However, two key challenges remain : 1) the absence of a large-scale
high-quality UHR T2I dataset, and (2) the neglect of tailored training
strategies for fine-grained detail synthesis in UHR scenarios. To tackle the
first challenge, we introduce UltraHR-100K, a high-quality dataset of
100K UHR images with rich captions, offering diverse content and strong visual
fidelity. Each image exceeds 3K resolution and is rigorously curated based on
detail richness, content complexity, and aesthetic quality. To tackle the
second challenge, we propose a frequency-aware post-training method that
enhances fine-detail generation in T2I diffusion models. Specifically, we
design (i) Detail-Oriented Timestep Sampling (DOTS) to focus learning
on detail-critical denoising steps, and (ii) Soft-Weighting Frequency
Regularization (SWFR), which leverages Discrete Fourier Transform (DFT) to
softly constrain frequency components, encouraging high-frequency detail
preservation. Extensive experiments on our proposed UltraHR-eval4K benchmarks
demonstrate that our approach significantly improves the fine-grained detail
quality and overall fidelity of UHR image generation. The code is available at
https://github.com/NJU-PCALab/UltraHR-100k{here}.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_20661.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.20661" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        PartNeXt: A Next-Generation Dataset for Fine-Grained and Hierarchical 3D Part Understanding
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Understanding objects at the level of their constituent parts is fundamental
to advancing computer vision, graphics, and robotics. While datasets like
PartNet have driven progress in 3D part understanding, their reliance on
untextured geometries and expert-dependent annotation limits scalability and
usability. We introduce PartNeXt, a next-generation dataset addressing these
gaps with over 23,000 high-quality, textured 3D models annotated with
fine-grained, hierarchical part labels across 50 categories. We benchmark
PartNeXt on two tasks: (1) class-agnostic part segmentation, where
state-of-the-art methods (e.g., PartField, SAMPart3D) struggle with
fine-grained and leaf-level parts, and (2) 3D part-centric question answering,
a new benchmark for 3D-LLMs that reveals significant gaps in open-vocabulary
part grounding. Additionally, training Point-SAM on PartNeXt yields substantial
gains over PartNet, underscoring the dataset's superior quality and diversity.
By combining scalable annotation, texture-aware labels, and multi-task
evaluation, PartNeXt opens new avenues for research in structured 3D
understanding.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_20155.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.20155" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Uniform Discrete Diffusion with Metric Path for Video Generation
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Continuous-space video generation has advanced rapidly, while discrete
approaches lag behind due to error accumulation and long-context inconsistency.
In this work, we revisit discrete generative modeling and present Uniform
discRete diffuSion with metric pAth (URSA), a simple yet powerful framework
that bridges the gap with continuous approaches for the scalable video
generation. At its core, URSA formulates the video generation task as an
iterative global refinement of discrete spatiotemporal tokens. It integrates
two key designs: a Linearized Metric Path and a Resolution-dependent Timestep
Shifting mechanism. These designs enable URSA to scale efficiently to
high-resolution image synthesis and long-duration video generation, while
requiring significantly fewer inference steps. Additionally, we introduce an
asynchronous temporal fine-tuning strategy that unifies versatile tasks within
a single model, including interpolation and image-to-video generation.
Extensive experiments on challenging video and image generation benchmarks
demonstrate that URSA consistently outperforms existing discrete methods and
achieves performance comparable to state-of-the-art continuous diffusion
methods. Code and models are available at https://github.com/baaivision/URSA
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24717.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24717" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                        <div class="paper-item">
                            <!-- 折叠头 -->
                            <div class="paper-header">
                                <div class="paper-title-row">
                                    <div class="paper-title">
                                        Group Relative Attention Guidance for Image Editing
                                    </div>
                                    <div class="paper-meta-preview">
                                        
                                        
                                    </div>
                                </div>
                                <svg class="icon toggle-icon" viewBox="0 0 24 24"><polyline points="6 9 12 15 18 9"></polyline></svg>
                            </div>
                            
                            <!-- 折叠内容 -->
                            <div class="paper-body">
                                <div class="paper-content-inner">
                                    Recently, image editing based on Diffusion-in-Transformer models has
undergone rapid development. However, existing editing methods often lack
effective control over the degree of editing, limiting their ability to achieve
more customized results. To address this limitation, we investigate the
MM-Attention mechanism within the DiT model and observe that the Query and Key
tokens share a bias vector that is only layer-dependent. We interpret this bias
as representing the model's inherent editing behavior, while the delta between
each token and its corresponding bias encodes the content-specific editing
signals. Based on this insight, we propose Group Relative Attention Guidance, a
simple yet effective method that reweights the delta values of different tokens
to modulate the focus of the model on the input image relative to the editing
instruction, enabling continuous and fine-grained control over editing
intensity without any tuning. Extensive experiments conducted on existing image
editing frameworks demonstrate that GRAG can be integrated with as few as four
lines of code, consistently enhancing editing quality. Moreover, compared to
the commonly used Classifier-Free Guidance, GRAG achieves smoother and more
precise control over the degree of editing. Our code will be released at
https://github.com/little-misfit/GRAG-Image-Editing.
                                    
                                    <div class="paper-links">
                                        <a href="https://jycarlos1019.pp.ua/reports/2025-10-30/2510_24657.html" class="action-link" target="_blank">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8z"></path><polyline points="14 2 14 8 20 8"></polyline><line x1="16" y1="13" x2="8" y2="13"></line><line x1="16" y1="17" x2="8" y2="17"></line><polyline points="10 9 9 9 8 9"></polyline></svg>
                                            查看完整解读
                                        </a>
                                        <a href="https://arxiv.org/abs/2510.24657" class="action-link" target="_blank" style="color: var(--text-secondary);">
                                            <svg class="icon" style="width: 14px; height: 14px;" viewBox="0 0 24 24"><path d="M10 13a5 5 0 0 0 7.54.54l3-3a5 5 0 0 0-7.07-7.07l-1.72 1.71"></path><path d="M14 11a5 5 0 0 0-7.54-.54l-3 3a5 5 0 0 0 7.07 7.07l1.71-1.71"></path></svg>
                                            arXiv 原文
                                        </a>
                                    </div>
                                </div>
                            </div>
                        </div>
                        
                    </div>
                </div>
            </div>
            
        </div>
        
        

        <div class="footer">
            <p>生成时间: 2025-11-20 17:59:17</p>
            <p>访问地址: <a href="https://jycarlos1019.pp.ua">https://jycarlos1019.pp.ua</a></p>
        </div>
    </div>
</body>
</html>