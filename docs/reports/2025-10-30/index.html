<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>æ¯æ—¥è®ºæ–‡ç®€æŠ¥ - 2025-10-30</title>
    <style>
        body {
            font-family: 'åœ†ä½“-ç®€', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        details {
            margin: 30px 0;
        }
        summary {
            cursor: pointer;
            user-select: none;
            padding: 10px;
            font-weight: bold;
            font-size: 18px;
        }
        summary:hover {
            background-color: rgba(0, 0, 0, 0.02);
        }
        details[open] summary {
            margin-bottom: 15px;
        }
        .container {
            background-color: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e9ecef;
        }
        .header h1 {
            color: #2c3e50;
            margin: 0;
            font-size: 28px;
        }
        .nav-links {
            margin-bottom: 20px;
            padding: 10px;
            background-color: #f8f9fa;
            border-radius: 6px;
        }
        .nav-links a {
            color: #007bff;
            text-decoration: none;
            margin-right: 15px;
            font-size: 14px;
        }
        .nav-links a:hover {
            text-decoration: underline;
        }
        .stats {
            background-color: #e3f2fd;
            padding: 15px;
            border-radius: 6px;
            margin-bottom: 20px;
            text-align: center;
        }
        .stats-text {
            color: #1976d2;
            font-weight: bold;
        }
        .paper {
            margin: 25px 0;
            padding: 20px;
            border: 1px solid #e9ecef;
            border-radius: 8px;
            background-color: #f8f9fa;
            transition: box-shadow 0.3s ease;
        }
        .paper:hover {
            box-shadow: 0 4px 15px rgba(0,0,0,0.1);
        }
        .paper-title {
            font-size: 20px;
            font-weight: bold;
            color: #2c3e50;
            margin-bottom: 10px;
            line-height: 1.4;
        }
        .paper-title a {
            color: #2c3e50;
            text-decoration: none;
        }
        .paper-title a:hover {
            color: #007bff;
            text-decoration: underline;
        }
        .paper-meta {
            color: #666;
            font-size: 13px;
            margin-bottom: 8px;
        }
        .paper-summary {
            color: #495057;
            margin-bottom: 15px;
            line-height: 1.5;
        }
        .paper-score {
            display: inline-block;
            background-color: #007bff;
            color: white;
            padding: 4px 8px;
            border-radius: 4px;
            font-size: 12px;
            font-weight: bold;
            margin-right: 10px;
        }
        .paper-image {
            margin: 15px 0;
            text-align: center;
        }
        .paper-image img {
            max-width: 100%;
            height: auto;
            border-radius: 6px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.1);
            border: 1px solid #e9ecef;
        }
        .paper-warning {
            color: #e67e22;
            font-size: 13px;
            margin-top: 10px;
            padding: 8px;
            background-color: #fff4e6;
            border-left: 3px solid #e67e22;
            border-radius: 4px;
        }
        .paper-link {
            display: inline-block;
            margin-top: 10px;
            color: #007bff;
            text-decoration: none;
            font-weight: bold;
        }
        .paper-link:hover {
            text-decoration: underline;
        }
        .footer {
            margin-top: 30px;
            padding-top: 20px;
            border-top: 1px solid #e9ecef;
            text-align: center;
            color: #6c757d;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“š æ¯æ—¥è®ºæ–‡ç®€æŠ¥</h1>
            <div class="date">2025-10-30</div>
        </div>
        
        <div class="nav-links">
            <a href="../../index.html">â† è¿”å›æ±‡æ€»é¡µ</a>
        </div>
        
        <div class="stats">
            <div class="stats-text">ä¸ºæ‚¨ç²¾é€‰äº† 5 ç¯‡é«˜è´¨é‡ AI è®ºæ–‡</div>
        </div>
        
        <!-- ç¬¬ä¸€éƒ¨åˆ†ï¼šå®è§‚æ ¸å¿ƒä¸»é¢˜ -->
        
        
        <!-- ç¬¬äºŒéƒ¨åˆ†ï¼šä¸­è§‚èšç±»åˆ†æ -->
        
        
        <!-- ç¬¬ä¸‰éƒ¨åˆ†ï¼šå¾®è§‚æ·±åº¦è§£è¯»ï¼ˆTop 6 ç²¾é€‰ï¼‰ -->
        <details class="micro-section" style="margin: 30px 0;">
            <summary style="color: #2c3e50; margin-bottom: 20px;">ğŸ“ å¾®è§‚æ·±åº¦è§£è¯»</summary>
            <div style="margin-top: 15px;">
        
        <div class="paper">
            <div class="paper-title">
                <a href="2510_25771.html">
                    1. Gaperon: A Peppered English-French Generative Language Model Suite
                </a>
            </div>
            
            <div>
                <span class="paper-score">æ¨èåˆ†æ•°: 0.633</span>
            </div>
            <div class="paper-summary">
                
                   <strong style="color:#e67e22">[WARNING] å·²å›é€€è‡³åŸå§‹æ‘˜è¦/è§£è¯»ï¼š</strong><br>
                
                <strong>ğŸ“– ç®€ä»‹ï¼š</strong>We release Gaperon, a fully open suite of French-English-coding language
models designed to advance transparency and reproducibility in large-scale
model training. The Gaperon family includes 1.5B, 8B, and 24B parameter models
trained on 2-4 trillion tokens, released with all elements of the training
pipeline: French and English datasets filtered with a neural quality
classifier, an efficient data curation and training framework, and hundreds of
intermediate checkpoints. Through this work, we study how data filtering and
contamination interact to shape both benchmark and generative performance.
                <span style="color:#e67e22;">ï¼ˆenrich failed, fallback to abstractï¼‰</span>
            </div>
            
            <a href="2510_25771.html" class="paper-link">æŸ¥çœ‹è¯¦ç»†è§£è¯» â†’</a>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <a href="2510_24081.html">
                    2. Global PIQA: Evaluating Physical Commonsense Reasoning Across 100+
  Languages and Cultures
                </a>
            </div>
            
            <div>
                <span class="paper-score">æ¨èåˆ†æ•°: 0.594</span>
            </div>
            <div class="paper-summary">
                
                   <strong style="color:#e67e22">[WARNING] å·²å›é€€è‡³åŸå§‹æ‘˜è¦/è§£è¯»ï¼š</strong><br>
                
                <strong>ğŸ“– ç®€ä»‹ï¼š</strong>To date, there exist almost no culturally-specific evaluation benchmarks for
large language models (LLMs) that cover a large number of languages and
cultures. In this paper, we present Global PIQA, a participatory commonsense
reasoning benchmark for over 100 languages, constructed by hand by 335
researchers from 65 countries around the world. The 116 language varieties in
Global PIQA cover five continents, 14 language families, and 23 writing
systems.
                <span style="color:#e67e22;">ï¼ˆenrich failed, fallback to abstractï¼‰</span>
            </div>
            
            <a href="2510_24081.html" class="paper-link">æŸ¥çœ‹è¯¦ç»†è§£è¯» â†’</a>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <a href="2510_24824.html">
                    3. Parallel Loop Transformer for Efficient Test-Time Computation Scaling
                </a>
            </div>
            
            <div>
                <span class="paper-score">æ¨èåˆ†æ•°: 0.575</span>
            </div>
            <div class="paper-summary">
                
                   <strong style="color:#e67e22">[WARNING] å·²å›é€€è‡³åŸå§‹æ‘˜è¦/è§£è¯»ï¼š</strong><br>
                
                <strong>ğŸ“– ç®€ä»‹ï¼š</strong>Large Language Models (LLMs) are powerful but often too slow and costly for
real-world use during inference. Looped transformers save on parameters by
reusing the same weights for multiple computational steps, or "loops." However,
this approach has a major flaw: the loops run one after another, causing
inference latency and memory requirements to increase with each added loop.
This makes them impractical for fast applications. To solve this problem, we
introduce the Parallel Loop Transformer (PLT).
                <span style="color:#e67e22;">ï¼ˆenrich failed, fallback to abstractï¼‰</span>
            </div>
            
            <a href="2510_24824.html" class="paper-link">æŸ¥çœ‹è¯¦ç»†è§£è¯» â†’</a>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <a href="2510_25409.html">
                    4. BhashaBench V1: A Comprehensive Benchmark for the Quadrant of Indic
  Domains
                </a>
            </div>
            
            <div>
                <span class="paper-score">æ¨èåˆ†æ•°: 0.551</span>
            </div>
            <div class="paper-summary">
                
                   <strong style="color:#e67e22">[WARNING] å·²å›é€€è‡³åŸå§‹æ‘˜è¦/è§£è¯»ï¼š</strong><br>
                
                <strong>ğŸ“– ç®€ä»‹ï¼š</strong>The rapid advancement of large language models(LLMs) has intensified the need
for domain and culture specific evaluation. Existing benchmarks are largely
Anglocentric and domain-agnostic, limiting their applicability to India-centric
contexts. To address this gap, we introduce BhashaBench V1, the first
domain-specific, multi-task, bilingual benchmark focusing on critical Indic
knowledge systems.
                <span style="color:#e67e22;">ï¼ˆenrich failed, fallback to abstractï¼‰</span>
            </div>
            
            <a href="2510_25409.html" class="paper-link">æŸ¥çœ‹è¯¦ç»†è§£è¯» â†’</a>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <a href="2510_22037.html">
                    5. ATLAS: Adaptive Transfer Scaling Laws for Multilingual Pretraining,
  Finetuning, and Decoding the Curse of Multilinguality
                </a>
            </div>
            
            <div>
                <span class="paper-score">æ¨èåˆ†æ•°: 0.544</span>
            </div>
            <div class="paper-summary">
                
                   <strong style="color:#e67e22">[WARNING] å·²å›é€€è‡³åŸå§‹æ‘˜è¦/è§£è¯»ï¼š</strong><br>
                
                <strong>ğŸ“– ç®€ä»‹ï¼š</strong>Scaling laws research has focused overwhelmingly on English -- yet the most
prominent AI models explicitly serve billions of international users. In this
work, we undertake the largest multilingual scaling laws study to date,
totaling 774 multilingual training experiments, spanning 10M-8B model
parameters, 400+ training languages and 48 evaluation languages. We introduce
the Adaptive Transfer Scaling Law (ATLAS) for both monolingual and multilingual
pretraining, which outperforms existing scaling laws' out-of-sample
generalization often by more than 0.3 R^2.
                <span style="color:#e67e22;">ï¼ˆenrich failed, fallback to abstractï¼‰</span>
            </div>
            
            <a href="2510_22037.html" class="paper-link">æŸ¥çœ‹è¯¦ç»†è§£è¯» â†’</a>
        </div>
        
            </div>
        </details>  <!-- å…³é—­ micro-section -->
        
        <div class="footer">
            <p>ğŸ“§ è¿™æ˜¯ç”±æ™ºèƒ½è®ºæ–‡ç®€æŠ¥ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆçš„é¡µé¢</p>
            <p>è®¿é—®åœ°å€: <a href="https://jycarlos1019.pp.ua">https://jycarlos1019.pp.ua</a></p>
        </div>
    </div>
</body>
</html>
