<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ä¸»é¢˜èšç±»åˆ†æ - 2025-11-15</title>
    <style>
        body {
            font-family: 'åœ†ä½“-ç®€', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            line-height: 1.6;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background-color: white;
            border-radius: 10px;
            padding: 30px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        .header {
            text-align: center;
            margin-bottom: 30px;
            padding-bottom: 20px;
            border-bottom: 2px solid #e9ecef;
        }
        .header h1 {
            color: #ffa500;
            margin: 0;
            font-size: 28px;
        }
        .nav-links {
            margin-bottom: 20px;
            padding: 10px;
            background-color: #f8f9fa;
            border-radius: 6px;
        }
        .nav-links a {
            color: #007bff;
            text-decoration: none;
            margin-right: 15px;
            font-size: 14px;
        }
        .nav-links a:hover {
            text-decoration: underline;
        }
        .macro-summary-section {
            background-color: #f0f7ff;
            padding: 25px;
            border-radius: 8px;
            border-left: 4px solid #007bff;
            margin-bottom: 30px;
        }
        .macro-summary-section h2 {
            color: #007bff;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 20px;
        }
        .macro-summary-content {
            color: #333;
            line-height: 1.8;
        }
        .macro-summary-content h3 {
            color: #2c3e50;
            margin-top: 20px;
            margin-bottom: 10px;
        }
        .macro-summary-content ul, .macro-summary-content ol {
            margin: 10px 0;
            padding-left: 25px;
        }
        .macro-summary-content li {
            margin: 8px 0;
        }
        .content-section {
            background-color: #fff9e6;
            padding: 25px;
            border-radius: 8px;
            border-left: 4px solid #ffa500;
        }
        .cluster-item {
            margin: 20px 0;
            padding: 20px;
            background-color: white;
            border-radius: 6px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .cluster-item h3 {
            color: #2c3e50;
            margin-top: 0;
            font-size: 18px;
            border-bottom: 2px solid #ffa500;
            padding-bottom: 10px;
        }
        .cluster-item ul {
            margin: 15px 0;
            padding-left: 20px;
        }
        .cluster-item li {
            margin: 12px 0;
            line-height: 1.6;
        }
        .cluster-item strong {
            color: #2c3e50;
            display: block;
            margin-bottom: 5px;
        }
        .cluster-item .contribution {
            color: #666;
            font-size: 14px;
        }
        .footer {
            margin-top: 40px;
            padding-top: 20px;
            border-top: 1px solid #e9ecef;
            text-align: center;
            color: #6c757d;
            font-size: 14px;
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>ğŸ“Š ä¸»é¢˜èšç±»åˆ†æ</h1>
            <div class="date">2025-11-15</div>
        </div>

        <div class="nav-links">
            <a href="index.html">â† è¿”å›æ¯æ—¥ç®€æŠ¥</a>
            <a href="../../index.html">è¿”å›æ±‡æ€»é¡µ</a>
            <a href="../../search.html">ğŸ” æœç´¢å†å²å½’æ¡£</a>
        </div>

        <!-- å®è§‚ç ”ç©¶è¶‹åŠ¿ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ -->
        
        <div class="macro-summary-section">
            <h2>ğŸ“ˆ å®è§‚ç ”ç©¶è¶‹åŠ¿</h2>
            <div class="macro-summary-content">
                <h2>æ ¸å¿ƒç ”ç©¶ä¸»é¢˜</h2>

<ol>
<li>å¤šæ¨¡æ€å­¦ä¹ ä¸æ¨ç†æ­£åœ¨æˆä¸ºAIç ”ç©¶çš„çƒ­ç‚¹ï¼Œæ—¨åœ¨é€šè¿‡æ•´åˆä¸åŒç±»å‹çš„æ•°æ®ï¼ˆå¦‚å›¾åƒã€æ–‡æœ¬å’ŒéŸ³é¢‘ï¼‰æ¥æå‡æ¨¡å‹çš„ç†è§£èƒ½åŠ›ã€‚</li>
<li>è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ä¸å¯¹é½é—®é¢˜å¼•å‘äº†å¹¿æ³›å…³æ³¨ï¼Œç ”ç©¶è€…ä»¬è‡´åŠ›äºæé«˜æ¨¡å‹åœ¨å¤šæ ·åŒ–è¾“å…¥ä¸‹çš„ç¨³å®šæ€§å’Œä¸€è‡´æ€§ã€‚</li>
<li>AIå†³ç­–ä¸­çš„å…¬å¹³æ€§ä¸é€æ˜æ€§æ—¥ç›Šé‡è¦ï¼Œæ¨åŠ¨äº†å¯¹ç®—æ³•åè§çš„æ£€æµ‹å’Œä¿®æ­£æ–¹æ³•çš„ç ”ç©¶ï¼Œä»¥ç¡®ä¿AIç³»ç»Ÿçš„å…¬æ­£æ€§ã€‚</li>
<li>è”é‚¦å­¦ä¹ ä½œä¸ºä¸€ç§ä¿æŠ¤ç”¨æˆ·éšç§çš„åˆ†å¸ƒå¼å­¦ä¹ æ–¹æ³•ï¼Œæ­£åœ¨å¿«é€Ÿå‘å±•ï¼Œå¼ºè°ƒåœ¨ä¿è¯æ•°æ®éšç§çš„åŒæ—¶æå‡æ¨¡å‹çš„æ•ˆç‡ã€‚</li>
<li>å¯¹æŠ—æ€§å­¦ä¹ ä¸é¢†åŸŸé€‚åº”ç ”ç©¶çš„æ·±å…¥ï¼Œæ—¨åœ¨å¢å¼ºæ¨¡å‹åœ¨é¢å¯¹æ¶æ„æ”»å‡»å’Œä¸åŒç¯å¢ƒæ—¶çš„é€‚åº”èƒ½åŠ›ã€‚</li>
</ol>

<h2>æŠ€æœ¯è¶‹åŠ¿</h2>

<ol>
<li>å¤šæ¨¡æ€æ¨¡å‹çš„é›†æˆä¸ä¼˜åŒ–æŠ€æœ¯æ­£åœ¨å¿«é€Ÿæ¼”è¿›ï¼Œæ¨åŠ¨äº†è·¨é¢†åŸŸåº”ç”¨çš„å®ç°ã€‚</li>
<li>éšç§ä¿æŠ¤æŠ€æœ¯åœ¨AIæ¨¡å‹ä¸­å¾—åˆ°è¶Šæ¥è¶Šå¤šçš„åº”ç”¨ï¼Œç‰¹åˆ«æ˜¯åœ¨åŒ»ç–—å’Œé‡‘èç­‰æ•æ„Ÿé¢†åŸŸã€‚</li>
<li>å¼ºåŒ–å­¦ä¹ ä¸­çš„å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ­£åœ¨æ¢ç´¢æ›´å¤æ‚çš„é€šä¿¡ä¸åè°ƒæœºåˆ¶ï¼Œä»¥æå‡é›†ä½“å†³ç­–çš„æ•ˆç‡å’Œæ•ˆæœã€‚</li>
</ol>

            </div>
        </div>
        

        <!-- ä¸»é¢˜èšç±»åˆ†æ -->
        <div class="content-section">
            <h2 style="color: #ffa500; margin-top: 0; margin-bottom: 20px;">ğŸ” ä¸»é¢˜èšç±»åˆ†æ</h2>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 1: å¤šæ¨¡æ€å­¦ä¹ ä¸æ¨ç†</h3>
                <ul>
                    
                    <li>
                        <strong>ImAgent: A Unified Multimodal Agent Framework for Test-Time Scalable Image Generation</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„å¤šæ¨¡æ€ä»£ç†æ¡†æ¶ï¼Œä»¥æé«˜æ–‡æœ¬åˆ°å›¾åƒç”Ÿæˆçš„å¯æ‰©å±•æ€§å’Œä¸€è‡´æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>STaR: Towards Cognitive Table Reasoning via Slow-Thinking Large Language Models</strong>
                        <span class="contribution">å¼•å…¥äº†ä¸€ç§æ…¢æ€è€ƒçš„å¤§å‹è¯­è¨€æ¨¡å‹ä»¥å¢å¼ºè¡¨æ ¼æ¨ç†èƒ½åŠ›ï¼Œè§£å†³äº†ç°æœ‰æ¨¡å‹çš„æ¨ç†è¿‡ç¨‹ç¼ºä¹æ·±åº¦çš„é—®é¢˜ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>GGBench: A Geometric Generative Reasoning Benchmark for Unified Multimodal Models</strong>
                        <span class="contribution">æ„å»ºäº†ä¸€ä¸ªå‡ ä½•ç”Ÿæˆæ¨ç†åŸºå‡†ï¼Œä»¥è¯„ä¼°ç»Ÿä¸€å¤šæ¨¡æ€æ¨¡å‹åœ¨è·¨æ¨¡æ€ç”Ÿæˆèƒ½åŠ›ä¸Šçš„è¡¨ç°ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>ARCTraj: A Dataset and Benchmark of Human Reasoning Trajectories for Abstract Problem Solving</strong>
                        <span class="contribution">æå‡ºäº†ARCTrajæ•°æ®é›†ï¼Œæä¾›äº†ä¸€ç§æ–°çš„æ–¹æ³•æ¡†æ¶æ¥å»ºæ¨¡äººç±»åœ¨æŠ½è±¡é—®é¢˜è§£å†³ä¸­çš„æ¨ç†è½¨è¿¹ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>S2D-ALIGN: Shallow-to-Deep Auxiliary Learning for Anatomically-Grounded Radiology Report Generation</strong>
                        <span class="contribution">å¼€å‘äº†ä¸€ç§æµ…åˆ°æ·±çš„è¾…åŠ©å­¦ä¹ æ–¹æ³•ï¼Œä»¥æé«˜è§£å‰–å­¦åŸºç¡€çš„æ”¾å°„å­¦æŠ¥å‘Šç”Ÿæˆçš„å‡†ç¡®æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>CrossMed: A Multimodal Cross-Task Benchmark for Compositional Generalization in Medical Imaging</strong>
                        <span class="contribution">åˆ›å»ºäº†ä¸€ä¸ªå¤šæ¨¡æ€è·¨ä»»åŠ¡åŸºå‡†ï¼Œè¯„ä¼°åŒ»å­¦å½±åƒä¸­ç»„åˆæ³›åŒ–èƒ½åŠ›çš„è¡¨ç°ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Synergy vs. Noise: Performance-Guided Multimodal Fusion For Biochemical Recurrence-Free Survival in Prostate Cancer</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§æ€§èƒ½å¼•å¯¼çš„å¤šæ¨¡æ€èåˆæ–¹æ³•ï¼Œä»¥æé«˜å‰åˆ—è…ºç™Œç”ŸåŒ–å¤å‘æ— ç”Ÿå­˜æœŸçš„é¢„æµ‹èƒ½åŠ›ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>VoxTell: Free-Text Promptable Universal 3D Medical Image Segmentation</strong>
                        <span class="contribution">ä»‹ç»äº†VoxTellæ¨¡å‹ï¼Œèƒ½å¤Ÿæ ¹æ®è‡ªç”±æ–‡æœ¬æç¤ºè¿›è¡Œ3DåŒ»å­¦å›¾åƒåˆ†å‰²ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>PROMISE: Prompt-Attentive Hierarchical Contrastive Learning for Robust Cross-Modal Representation with Missing Modalities</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§åˆ†å±‚å¯¹æ¯”å­¦ä¹ æ–¹æ³•ï¼Œä»¥å¢å¼ºåœ¨ç¼ºå¤±æ¨¡æ€æƒ…å†µä¸‹çš„è·¨æ¨¡æ€è¡¨ç¤ºçš„é²æ£’æ€§ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 2: è§†è§‰è¯­è¨€æ¨¡å‹çš„é²æ£’æ€§ä¸å¯¹é½</h3>
                <ul>
                    
                    <li>
                        <strong>Benchmarking Visual LLMs Resilience to Unanswerable Questions on Visually Rich Documents</strong>
                        <span class="contribution">è¯¥ç ”ç©¶è¯„ä¼°äº†è§†è§‰å¤§è¯­è¨€æ¨¡å‹åœ¨å¤„ç†è§†è§‰ä¸°å¯Œæ–‡æ¡£ä¸­çš„ä¸å¯å›ç­”é—®é¢˜çš„èƒ½åŠ›ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>EcoAlign: An Economically Rational Framework for Efficient LVLM Alignment</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§ç»æµåˆç†çš„æ¡†æ¶ï¼Œä»¥æé«˜å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹çš„å¯¹é½æ•ˆç‡å’Œå®‰å…¨æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Refine and Align: Confidence Calibration through Multi-Agent Interaction in VQA</strong>
                        <span class="contribution">æ¢è®¨äº†é€šè¿‡å¤šä»£ç†äº¤äº’æ¥æ ¡å‡†è§†è§‰é—®ç­”ä¸­çš„ä¿¡å¿ƒï¼Œä»è€Œæé«˜ç³»ç»Ÿçš„å†³ç­–å‡†ç¡®æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Questioning the Stability of Visual Question Answering</strong>
                        <span class="contribution">ç³»ç»Ÿç ”ç©¶äº†è§†è§‰è¯­è¨€æ¨¡å‹å¯¹å°å¹…è¾“å…¥å˜åŒ–çš„é²æ£’æ€§ï¼Œæ­ç¤ºäº†å…¶å¯é æ€§é—®é¢˜ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>PRSM: A Measure to Evaluate CLIP's Robustness Against Paraphrases</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§æ–°æŒ‡æ ‡æ¥è¯„ä¼°CLIPæ¨¡å‹å¯¹è¯­è¨€å˜ä½“ï¼ˆå¦‚åŒä¹‰æ”¹å†™ï¼‰çš„é²æ£’æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding</strong>
                        <span class="contribution">å¼€å‘äº†ä¸€ç§å·¥å…·å¢å¼ºçš„å¤šä»£ç†æ¡†æ¶ï¼Œä»¥æ”¹å–„å¯¹é•¿è§†è§‰æ–‡æ¡£çš„ç†è§£å’Œè¯æ®å®šä½ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>From Synthetic Scenes to Real Performance: Enhancing Spatial Reasoning in VLMs</strong>
                        <span class="contribution">æ¢è®¨äº†é€šè¿‡åˆæˆåœºæ™¯æå‡è§†è§‰è¯­è¨€æ¨¡å‹ç©ºé—´æ¨ç†èƒ½åŠ›çš„ç­–ç•¥ï¼Œè§£å†³äº†è¿‡æ‹Ÿåˆå’Œåˆ†å¸ƒä¸å¹³è¡¡é—®é¢˜ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Discovering Meaningful Units with Visually Grounded Semantics from Image Captions</strong>
                        <span class="contribution">ç ”ç©¶äº†å¦‚ä½•ä»å›¾åƒæ ‡é¢˜ä¸­å‘ç°å…·æœ‰è§†è§‰åŸºç¡€çš„è¯­ä¹‰å•å…ƒï¼Œä»¥å¢å¼ºè§†è§‰è¯­è¨€æ¨¡å‹çš„çŸ¥è¯†ç†è§£ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 3: å¤šä»£ç†æ¨ç†ä¸é€‚åº”æ€§</h3>
                <ul>
                    
                    <li>
                        <strong>Experience-Guided Adaptation of Inference-Time Reasoning Strategies</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§åŸºäºåè®­ç»ƒäº¤äº’çš„æ¨ç†ç­–ç•¥é€‚åº”æ–¹æ³•ï¼Œä»¥å¢å¼ºæ™ºèƒ½ä»£ç†ç³»ç»Ÿçš„è§£å†³é—®é¢˜èƒ½åŠ›ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</strong>
                        <span class="contribution">é€šè¿‡å¼ºåŒ–å­¦ä¹ å’Œä»£ç†ç®¡é“å¹¶è¡Œæ€§ï¼Œæ¨åŠ¨äº†å¤šä»£ç†æ¨ç†ç³»ç»Ÿçš„å‘å±•ï¼Œå…‹æœäº†å¤§å‹è¯­è¨€æ¨¡å‹è¾“å‡ºé•¿åº¦çš„é™åˆ¶ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>iMAD: Intelligent Multi-Agent Debate for Efficient and Accurate LLM Inference</strong>
                        <span class="contribution">å¼•å…¥äº†å¤šä»£ç†è¾©è®ºæ¡†æ¶ï¼Œä»¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹åœ¨å¤æ‚ä»»åŠ¡ä¸Šçš„æ¨ç†å’Œå‡†ç¡®æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Multi-agent Undercover Gaming: Hallucination Removal via Counterfactual Test for Multimodal Reasoning</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§é€šè¿‡åäº‹å®æµ‹è¯•æ¥å»é™¤å¤šæ¨¡æ€æ¨ç†ä¸­çš„å¹»è§‰çš„æ–¹æ³•ï¼Œå¢å¼ºäº†å¤šä»£ç†è¾©è®ºçš„å¯é æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>VIDEOP2R: Video Understanding from Perception to Reasoning</strong>
                        <span class="contribution">å±•ç¤ºäº†å¼ºåŒ–å¾®è°ƒåœ¨è§†é¢‘è¯­è¨€æ¨¡å‹æ¨ç†èƒ½åŠ›æå‡ä¸­çš„æ½œåŠ›ï¼Œè§£å†³äº†æ‰©å±•è‡³å¤§å‹è§†é¢‘è¯­è¨€æ¨¡å‹çš„æŒ‘æˆ˜ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Key Decision-Makers in Multi-Agent Debates: Who Holds the Power?</strong>
                        <span class="contribution">æ¢è®¨äº†åœ¨å¤šä»£ç†è¾©è®ºä¸­è§’è‰²åˆ†é…ç­–ç•¥çš„é‡è¦æ€§ï¼Œæ­ç¤ºäº†ä¸åŒè§‚ç‚¹è§’è‰²å¯¹æ¨ç†èƒ½åŠ›çš„å½±å“ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>LaoBench: A Large-Scale Multidimensional Lao Benchmark for Large Language Models</strong>
                        <span class="contribution">åˆ›å»ºäº†é¦–ä¸ªé’ˆå¯¹è€æŒè¯­çš„å¤§è§„æ¨¡å¤šç»´åŸºå‡†æ•°æ®é›†ï¼Œä»¥è¯„ä¼°å¤§å‹è¯­è¨€æ¨¡å‹åœ¨ä½èµ„æºè¯­è¨€ä¸­çš„è¡¨ç°ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 4: AIå†³ç­–ä¸­çš„å…¬å¹³æ€§ä¸é€æ˜æ€§</h3>
                <ul>
                    
                    <li>
                        <strong>A Workflow for Full Traceability of AI Decisions</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§å·¥ä½œæµç¨‹ï¼Œä»¥å®ç°å¯¹é«˜é£é™©AIå†³ç­–çš„å…¨é¢å¯è¿½æº¯æ€§ï¼Œæ—¨åœ¨ä¿æŠ¤äººæƒå’Œç¦ç¥‰ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Specification, Application, and Operationalization of a Metamodel of Fairness</strong>
                        <span class="contribution">ä»‹ç»äº†ARå…¬å¹³æ€§å…ƒæ¨¡å‹ï¼Œæä¾›äº†å…¬å¹³æ€§åœºæ™¯çš„å½¢å¼åŒ–è¡¨ç¤ºå’Œåˆ†ææ–¹æ³•ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Generalizing Fair Clustering to Multiple Groups: Algorithms and Applications</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§é’ˆå¯¹å¤šç»„çš„å…¬å¹³èšç±»ç®—æ³•ï¼Œæ—¨åœ¨æ”¹å–„å¯¹è¾¹ç¼˜åŒ–ç¾¤ä½“çš„å…¬å¹³è¡¨ç¤ºã€‚</span>
                    </li>
                    
                    <li>
                        <strong>FairReweighing: Density Estimation-Based Reweighing Framework for Improving Separation in Fair Regression</strong>
                        <span class="contribution">å¼€å‘äº†ä¸€ç§åŸºäºå¯†åº¦ä¼°è®¡çš„é‡åŠ æƒæ¡†æ¶ï¼Œä»¥æé«˜å…¬å¹³å›å½’ä¸­çš„åˆ†ç¦»æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Decomposing Direct and Indirect Biases in Linear Models under Demographic Parity Constraint</strong>
                        <span class="contribution">åˆ†æäº†åœ¨å¼•å…¥äººå£å¹³è¡¡çº¦æŸä¸‹ï¼Œçº¿æ€§æ¨¡å‹ä¸­ç›´æ¥å’Œé—´æ¥åè§çš„åˆ†è§£åŠå…¶å¯¹é¢„æµ‹åå·®çš„å½±å“ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 5: è”é‚¦å­¦ä¹ ä¸­çš„éšç§ä¸æ•ˆç‡</h3>
                <ul>
                    
                    <li>
                        <strong>A Unified Convergence Analysis for Semi-Decentralized Learning: Sampled-to-Sampled vs. Sampled-to-All Communication</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§ç»Ÿä¸€çš„æ”¶æ•›åˆ†ææ¡†æ¶ï¼Œæ¯”è¾ƒäº†ä¸åŒé€šä¿¡ç­–ç•¥åœ¨åŠå»ä¸­å¿ƒåŒ–å­¦ä¹ ä¸­çš„è¡¨ç°ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>HealSplit: Towards Self-Healing through Adversarial Distillation in Split Federated Learning</strong>
                        <span class="contribution">å¼•å…¥äº†ä¸€ç§å¯¹æŠ—è’¸é¦æ–¹æ³•ï¼Œå¢å¼ºäº†åˆ†å‰²è”é‚¦å­¦ä¹ åœ¨é¢å¯¹æ•°æ®ä¸­æ¯’æ”»å‡»æ—¶çš„è‡ªæˆ‘ä¿®å¤èƒ½åŠ›ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>When to Stop Federated Learning: Zero-Shot Generation of Synthetic Validation Data with Generative AI for Early Stopping</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§åŸºäºç”Ÿæˆæ€§AIçš„é›¶-shotæ–¹æ³•ï¼Œèƒ½å¤Ÿåœ¨è”é‚¦å­¦ä¹ ä¸­å®ç°æ—©æœŸåœæ­¢ï¼Œå‡å°‘ä¸å¿…è¦çš„è®¡ç®—ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Towards Federated Clustering: A Client-wise Private Graph Aggregation Framework</strong>
                        <span class="contribution">å¼€å‘äº†ä¸€ç§å®¢æˆ·ç«¯çº§çš„ç§æœ‰å›¾èšåˆæ¡†æ¶ï¼Œä»¥è§£å†³è”é‚¦èšç±»ä¸­çš„éšç§å’Œæ€§èƒ½æŠ˜ä¸­é—®é¢˜ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 6: è¯­è¨€æ¨¡å‹ä¸­çš„å¹»è§‰æ£€æµ‹ä¸ä¿¡ä»»æ€§</h3>
                <ul>
                    
                    <li>
                        <strong>PAS : Prelim Attention Score for Detecting Object Hallucinations in Large Vision--Language Models</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§æ–°çš„é¢„æ³¨æ„åŠ›è¯„åˆ†æ–¹æ³•ï¼Œç”¨äºæ£€æµ‹å¤§å‹è§†è§‰è¯­è¨€æ¨¡å‹ä¸­çš„å¯¹è±¡å¹»è§‰ç°è±¡ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Honesty over Accuracy: Trustworthy Language Models through Reinforced Hesitation</strong>
                        <span class="contribution">é€šè¿‡å¼ºåŒ–çŠ¹è±«æœºåˆ¶ï¼Œæå‡è¯­è¨€æ¨¡å‹åœ¨é¢å¯¹ä¸ç¡®å®šæ€§æ—¶çš„ä¿¡ä»»æ€§ï¼Œå‡å°‘é”™è¯¯å›ç­”çš„è‡ªä¿¡åº¦ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Can LLMs Detect Their Own Hallucinations?</strong>
                        <span class="contribution">æ¢è®¨å¤§å‹è¯­è¨€æ¨¡å‹æ˜¯å¦èƒ½å¤Ÿè¯†åˆ«è‡ªèº«çš„å¹»è§‰ï¼Œå¹¶æå‡ºäº†ä¸€ç§åˆ†ç±»ä»»åŠ¡æ¡†æ¶æ¥è¯„ä¼°å…¶èƒ½åŠ›ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>From Proof to Program: Characterizing Tool-Induced Reasoning Hallucinations in Large Language Models</strong>
                        <span class="contribution">åˆ†æäº†å·¥å…·å¢å¼ºè¯­è¨€æ¨¡å‹åœ¨ä½¿ç”¨å¤–éƒ¨å·¥å…·æ—¶äº§ç”Ÿçš„æ¨ç†å¹»è§‰ï¼Œæ­ç¤ºäº†å…¶å¯ä¿¡æ€§é—®é¢˜ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 7: ç”µå­å¥åº·è®°å½•ä¸­çš„é¢„æµ‹å»ºæ¨¡ä¸çŸ¥è¯†è¡¨ç¤º</h3>
                <ul>
                    
                    <li>
                        <strong>CURENet: Combining Unified Representations for Efficient Chronic Disease Prediction</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§ç»“åˆç»Ÿä¸€è¡¨ç¤ºçš„æ¨¡å‹ï¼Œä»¥æé«˜æ…¢æ€§ç–¾ç—…é¢„æµ‹çš„æ•ˆç‡ï¼Œåˆ©ç”¨å¤šæ¨¡æ€å’Œæ—¶é—´åºåˆ—çš„ç”µå­å¥åº·è®°å½•æ•°æ®ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Toward Scalable Early Cancer Detection: Evaluating EHR-Based Predictive Models Against Traditional Screening Criteria</strong>
                        <span class="contribution">è¯„ä¼°äº†åŸºäºç”µå­å¥åº·è®°å½•çš„é¢„æµ‹æ¨¡å‹åœ¨æ—©æœŸç™Œç—‡æ£€æµ‹ä¸­çš„å¯æ‰©å±•æ€§ï¼Œç›¸è¾ƒäºä¼ ç»Ÿç­›æŸ¥æ ‡å‡†æä¾›äº†æ–°çš„è§†è§’ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>CardioEmbed: Domain-Specialized Text Embeddings for Clinical Cardiology</strong>
                        <span class="contribution">å¼€å‘äº†é’ˆå¯¹ä¸´åºŠå¿ƒè„ç—…å­¦çš„é¢†åŸŸä¸“ç”¨æ–‡æœ¬åµŒå…¥ï¼Œå¡«è¡¥äº†ç”Ÿç‰©åŒ»å­¦æ–‡æœ¬åµŒå…¥ä¸ä¸´åºŠå®è·µä¹‹é—´çš„å·®è·ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>MedPath: Multi-Domain Cross-Vocabulary Hierarchical Paths for Biomedical Entity Linking</strong>
                        <span class="contribution">æå‡ºäº†MedPathæ¨¡å‹ï¼Œé€šè¿‡å¤šé¢†åŸŸäº¤å‰è¯æ±‡å±‚æ¬¡è·¯å¾„è§£å†³ç”Ÿç‰©åŒ»å­¦å®ä½“é“¾æ¥ä¸­çš„æ•°æ®ç¢ç‰‡åŒ–é—®é¢˜ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 8: ç”Ÿç‰©å¯å‘çš„ç¥ç»ç½‘ç»œæ¨¡å‹</h3>
                <ul>
                    
                    <li>
                        <strong>Inferring response times of perceptual decisions with Poisson variational autoencoders</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§å›¾åƒå¯è®¡ç®—çš„æ¨¡å‹ï¼Œä»¥æ•æ‰æ„ŸçŸ¥å†³ç­–è¿‡ç¨‹ä¸­çš„æ—¶é—´åŠ¨æ€ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>StochEP: Stochastic Equilibrium Propagation for Spiking Convergent Recurrent Neural Networks</strong>
                        <span class="contribution">å¼•å…¥äº†ä¸€ç§æ–°çš„éšæœºå¹³è¡¡ä¼ æ’­æ–¹æ³•ï¼Œä»¥æé«˜è„‰å†²ç¥ç»ç½‘ç»œçš„è®­ç»ƒæ•ˆç‡å’Œç”Ÿç‰©åˆç†æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Sheaf Cohomology of Linear Predictive Coding Networks</strong>
                        <span class="contribution">å±•ç¤ºäº†çº¿æ€§é¢„æµ‹ç¼–ç ç½‘ç»œçš„è‡ªç„¶å½¢å¼ï¼Œåˆ©ç”¨ç»†èƒå±‚å çš„ç»“æ„ä¼˜åŒ–æƒé‡å’Œæ¿€æ´»ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 9: å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ ä¸­çš„é€šä¿¡ä¸åè°ƒ</h3>
                <ul>
                    
                    <li>
                        <strong>Robust and Efficient Communication in Multi-Agent Reinforcement Learning</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§è€ƒè™‘é€šä¿¡å»¶è¿Ÿå’Œå¸¦å®½é™åˆ¶çš„å¤šæ™ºèƒ½ä½“å¼ºåŒ–å­¦ä¹ æ¡†æ¶ï¼Œä»¥æé«˜åä½œæ•ˆç‡å’Œé²æ£’æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Scalable Population Training for Zero-Shot Coordination</strong>
                        <span class="contribution">å¼•å…¥äº†ä¸€ç§åŸºäºäººç¾¤è®­ç»ƒçš„æ–¹æ³•ï¼Œä½¿å¾—æ™ºèƒ½ä½“èƒ½å¤Ÿåœ¨æœªè§è¿‡çš„åä½œè€…ä¹‹é—´å®ç°é›¶-shotåè°ƒã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Multi-Phase Spacecraft Trajectory Optimization via Transformer-Based Reinforcement Learning</strong>
                        <span class="contribution">åˆ©ç”¨å˜æ¢å™¨ç»“æ„çš„å¼ºåŒ–å­¦ä¹ æ–¹æ³•ï¼Œä¼˜åŒ–äº†å¤šé˜¶æ®µèˆªå¤©å™¨çš„è½¨è¿¹æ§åˆ¶ï¼Œå¢å¼ºäº†é€‚åº”ä¸åŒä»»åŠ¡é˜¶æ®µçš„èƒ½åŠ›ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
            <div class="cluster-item">
                <h3>ä¸»é¢˜ 10: å¯¹æŠ—æ€§å­¦ä¹ ä¸é¢†åŸŸé€‚åº”</h3>
                <ul>
                    
                    <li>
                        <strong>AUVIC: Adversarial Unlearning of Visual Concepts for Multi-modal Large Language Models</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§å¯¹æŠ—æ€§å»å­¦ä¹ æ–¹æ³•ï¼Œä»¥åº”å¯¹å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ä¸­çš„æ•°æ®éšç§é—®é¢˜ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>D-GAP: Improving Out-of-Domain Robustness via Dataset-Agnostic and Gradient-Guided Augmentation in Amplitude and Pixel Spaces</strong>
                        <span class="contribution">å¼•å…¥äº†ä¸€ç§æ•°æ®é›†æ— å…³çš„å¢å¼ºæ–¹æ³•ï¼Œæå‡äº†æ¨¡å‹åœ¨å›¾åƒèƒŒæ™¯å’Œé£æ ¼å˜åŒ–ä¸‹çš„é²æ£’æ€§ã€‚</span>
                    </li>
                    
                    <li>
                        <strong>Unsupervised Robust Domain Adaptation: Paradigm, Theory and Algorithm</strong>
                        <span class="contribution">æå‡ºäº†ä¸€ç§æ–°çš„æ— ç›‘ç£é¢†åŸŸé€‚åº”æ¡†æ¶ï¼Œå¼ºè°ƒåœ¨å¯¹æŠ—æ”»å‡»ä¸‹çš„é²æ£’æ€§ã€‚</span>
                    </li>
                    
                </ul>
            </div>
            
        </div>

        <div class="footer">
            <p>ç”Ÿæˆæ—¶é—´: 2025-11-17 17:20:35</p>
            <p>æ•°æ®æ¥æº: arXiv AI è®ºæ–‡æ¨èç³»ç»Ÿ</p>
        </div>
    </div>
</body>
</html>
