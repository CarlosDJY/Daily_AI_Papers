<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</title>
    <style>
        :root {
            /* 配色方案：Slate + Indigo */
            --primary-color: #4f46e5;
            --bg-body: #f8fafc;
            --bg-paper: #ffffff;
            --text-main: #1e293b;      /* Slate 800 */
            --text-body: #334155;      /* Slate 700 - 正文颜色略浅，减少视觉疲劳 */
            --text-secondary: #64748b; /* Slate 500 */
            --border-color: #e2e8f0;
            --code-bg: #f1f5f9;
            
            /* 警告色 */
            --warn-bg: #fff7ed;
            --warn-text: #9a3412;
            --warn-border: #fdba74;

            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-body);
            line-height: 1.8; /* 增加行高，适合阅读 */
            padding: 40px 20px;
            min-height: 100vh;
        }

        /* 阅读容器：限制宽度以提升阅读体验 */
        .container {
            max-width: 800px;
            margin: 0 auto;
            background-color: var(--bg-paper);
            border-radius: 16px; /* 更圆润的角 */
            padding: 40px 60px; /* 宽敞的内边距 */
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03);
        }

        /* 顶部导航 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            font-size: 14px;
        }

        .nav-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            transition: color 0.2s;
        }

        .nav-link:hover { color: var(--primary-color); }
        .nav-link::before { content: "←"; margin-right: 5px; }
        
        .arxiv-link {
            background-color: #f1f5f9;
            color: var(--text-main);
            padding: 6px 12px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
        }
        
        .arxiv-link:hover {
            background-color: #e2e8f0;
            color: var(--primary-color);
        }

        /* 论文头部信息 */
        .paper-header {
            margin-bottom: 40px;
        }

        .paper-title {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        /* 标签组 */
        .tags-wrapper {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .tag {
            background-color: #e0e7ff; /* Indigo 100 */
            color: #4338ca;            /* Indigo 700 */
            font-size: 12px;
            padding: 4px 10px;
            border-radius: 99px;
            font-weight: 500;
        }

        /* 元数据栏 */
        .metadata-box {
            background-color: #f8fafc;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            font-size: 14px;
            color: var(--text-secondary);
        }

        .meta-item {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .meta-label {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #94a3b8;
        }

        .meta-value {
            font-weight: 600;
            color: var(--text-main);
        }
        
        .score-badge {
            color: var(--primary-color);
        }

        /* 核心图片展示 */
        .core-image-container {
            margin: 40px 0;
            text-align: center;
            background-color: #f8fafc;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .core-image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        
        .image-caption {
            margin-top: 10px;
            font-size: 13px;
            color: var(--text-secondary);
            font-style: italic;
        }

        /* 警告框 */
        .warning-box {
            background-color: var(--warn-bg);
            border-left: 4px solid var(--warn-border);
            color: var(--warn-text);
            padding: 15px;
            border-radius: 0 6px 6px 0;
            margin: 20px 0;
            font-size: 14px;
        }

        /* 章节标题 */
        .section-header {
            display: flex;
            align-items: center;
            margin-top: 50px;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px dashed var(--border-color);
        }

        .section-header h2 {
            font-size: 24px;
            font-weight: 700;
            color: var(--text-main);
            margin: 0;
            position: relative;
        }
        
        /* 章节前的装饰点 */
        .section-header h2::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: var(--primary-color);
            border-radius: 50%;
            margin-right: 12px;
            vertical-align: middle;
        }

        /* Markdown 内容样式重置 - 极简学术风 */
        .content-body {
            font-size: 17px; /* 略大的字号适合阅读 */
            color: var(--text-body);
        }

        .content-body p {
            margin-bottom: 1.5em;
            text-align: justify;
        }

        .content-body h3 {
            font-size: 20px;
            font-weight: 600;
            color: var(--text-main);
            margin-top: 2em;
            margin-bottom: 1em;
        }
        
        .content-body h4 {
            font-size: 18px;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }

        .content-body ul, .content-body ol {
            margin-bottom: 1.5em;
            padding-left: 1.5em;
        }

        .content-body li {
            margin-bottom: 0.5em;
        }

        .content-body strong {
            color: var(--text-main);
            font-weight: 600;
        }
        
        /* 引用块 - 学术风 */
        .content-body blockquote {
            border-left: 4px solid var(--primary-color);
            background-color: #f8fafc;
            padding: 16px 20px;
            margin: 20px 0;
            font-style: italic;
            color: var(--text-secondary);
            border-radius: 0 8px 8px 0;
        }

        /* 代码块 */
        .content-body pre {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid var(--border-color);
        }

        .content-body code {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
            color: #d63384; /* 类似 GitHub 的代码红 */
        }
        
        .content-body pre code {
            color: inherit;
            padding: 0;
            background-color: transparent;
        }

        /* Footer */
        .footer {
            margin-top: 80px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        /* 移动端适配 */
        @media (max-width: 768px) {
            body { padding: 0; }
            
            .container {
                border-radius: 0;
                padding: 30px 20px;
                box-shadow: none;
            }

            .paper-title { font-size: 26px; }
            
            .metadata-box {
                flex-direction: column;
                gap: 15px;
            }
            
            .content-body { font-size: 16px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="index.html" class="nav-link">返回今日简报</a>
            <a href="http://arxiv.org/abs/2511.11373v1" target="_blank" class="arxiv-link">PDF / arXiv ↗</a>
        </div>

        <div class="paper-header">
            <h1 class="paper-title">MarsRL: Advancing Multi-Agent Reasoning System via Reinforcement Learning with Agentic Pipeline Parallelism</h1>
            
            
            <div class="tags-wrapper">
                
                <span class="tag">多代理推理系统</span>
                
                <span class="tag">强化学习</span>
                
                <span class="tag">个性化可验证奖励机制</span>
                
                <span class="tag">管道并行训练</span>
                
                <span class="tag">奖励噪声</span>
                
            </div>
            

            <div class="metadata-box">
                
                <div class="meta-item" style="flex: 2; min-width: 200px;">
                    <span class="meta-label">作者单位</span>
                    <span class="meta-value">Tencent Hunyuan Team</span>
                </div>
                
                
                <div class="meta-item">
                    <span class="meta-label">推荐指数</span>
                    <span class="meta-value score-badge">0.511</span>
                </div>
                
                <div class="meta-item">
                    <span class="meta-label">arXiv ID</span>
                    <span class="meta-value">2511.11373v1</span>
                </div>
            </div>

            
        </div>

        
        <div class="core-image-container">
            
            <img src="../../images/2025-11-15/14119f2070f4017402c2aa12153690d99c293f1fab67d64d3ae40b97c538a8e1.jpg" alt="核心思路示意图" />
            <div class="image-caption">图 1：论文核心方法/架构示意图</div>
        </div>
        

        <div class="section-header">
            <h2>快速简介</h2>
        </div>
        <div class="content-body">
            <p>本文提出了MarsRL框架，通过强化学习优化多代理推理系统，解决了开源模型在复杂推理任务中的性能不足。MarsRL引入个性化可验证奖励机制和管道并行训练，显著减少奖励噪声并提高训练效率。实验结果显示，模型在AIME2025和BeyondAIME任务上的准确率分别提升至93.3%和73.8%，展示了该方法的有效性和广泛适用性。</p>
        </div>

        <div class="section-header">
            <h2>深度解读</h2>
        </div>
        <div class="content-body">
            
                <h3>现有问题</h3>

<p>本文旨在解决多代理推理系统中的几个核心挑战：
1.  <strong>推理深度与泛化能力</strong>：单一推理机制因输出长度限制导致推理深度不足。现有的多代理系统在开源大语言模型上泛化能力差，主要因为缺乏有效的批评和修正能力。
2.  <strong>训练效率与奖励噪声</strong>：在处理长轨迹时，多代理系统的生成和训练过程计算效率低下，延迟显著。同时，在多阶段任务中，错误的判断会导致不合理的奖励分配（奖励噪声），影响训练效果。
3.  <strong>采样策略</strong>：在验证-修正（V-C）推理系统中，如何选择有效的采样策略来提升验证者的性能，进而提高整个系统的表现，是一个关键问题。</p>

<h3>Hypothesis</h3>

<ul>
<li><strong>核心假设</strong>：通过强化学习（RL）联合优化多代理系统中的求解器（Solver）、验证器（Verifier）和修正器（Corrector），可以显著提高系统的整体推理性能。</li>
<li><strong>关键机制</strong>：
<ul>
<li>引入代理特定的、可验证的奖励机制，可以减少奖励噪声，使奖励更准确地反映各代理的实际表现。</li>
<li>采用管道并行（Pipeline Parallel）的训练方法，可以提高处理长轨迹的效率，减少训练延迟。</li>
<li>使用自适应采样策略，优先处理错误样本，可以有效提升验证者的错误检测能力。</li>
</ul></li>
</ul>

<h3>相关研究</h3>

<ul>
<li><strong>多代理推理系统</strong>：特别是验证-修正（V–C）推理范式，如Huang &amp; Yang (2025)的研究。</li>
<li><strong>强化学习（RL）应用</strong>：包括强化学习与可验证奖励（RLVR）、GRPO方法（一种PPO的替代方案）以及用于处理超长输出任务的UloRL框架。</li>
<li><strong>奖励分配机制</strong>：强化学习中的奖励分配和噪声处理。</li>
</ul>

<h3>解决方案</h3>

<h3><strong>MarsRL：一个用于多代理推理的强化学习框架的详细解决方案</strong></h3>

<p>论文中提出的解决方案名为 <strong>MarsRL</strong>，它是一个创新的代理强化学习（Agentic Reinforcement Learning）框架。该框架的核心目标是通过引入<strong>代理管道并行性（Agentic Pipeline Parallelism）</strong>来优化多代理推理系统，从而显著提升开放源代码模型在复杂推理任务中的性能、准确性和训练效率。</p>

<h4><strong>一、 核心架构：多代理推理系统</strong></h4>

<p>MarsRL的基础是一个模仿人类解决复杂问题策略的多代理系统。该系统由三个关键角色的代理组成，它们协同工作，通过迭代的方式深化推理过程：</p>

<ul>
<li><strong>Solver (求解者)</strong>：负责接收原始问题，并生成一个初步的解决方案。</li>
<li><strong>Verifier (验证者)</strong>：负责审查Solver（或后续Corrector）生成的解决方案，检测其中潜在的错误或缺陷。</li>
<li><strong>Corrector (修正者)</strong>：根据Verifier提供的反馈，对存在错误的解决方案进行修正和优化。</li>
</ul>

<p><strong>工作流程</strong>如下：
1.  <strong>初步生成</strong>：Solver生成初始解决方案。
2.  <strong>错误检测</strong>：Verifier评估该方案，并识别出问题。
3.  <strong>迭代修正</strong>：Corrector根据Verifier的反馈进行修正。这个过程可以迭代进行，直到生成满意的解决方案或达到预设的终止条件。</p>

<p>这种结构通过分工合作，将复杂的推理任务分解为生成、验证和修正三个子任务，从而增强了推理的深度和鲁棒性。</p>

<h4><strong>二、 关键创新与核心机制</strong></h4>

<p>为了解决传统多代理系统面临的<strong>奖励噪声</strong>和<strong>训练效率低下</strong>两大挑战，MarsRL引入了两项核心创新：</p>

<h5><strong>1. 代理式可验证奖励 (Agentic Verifiable Rewards)：解决奖励噪声问题</strong></h5>

<p>在传统的多代理强化学习中，奖励通常在整个任务轨迹结束后统一分配，这会导致“信用分配”问题。例如，一个优秀的Solver可能因为一个糟糕的Verifier的错误判断而受到惩罚，这种奖励噪声会严重干扰训练过程。</p>

<p>MarsRL通过为每个代理设计<strong>特定且可验证的奖励机制</strong>来解决此问题，奖励直接与参考答案（Ground Truth）挂钩：</p>

<ul>
<li><strong>Solver 和 Corrector 的奖励</strong>：它们的奖励取决于其生成的解决方案是否与参考答案一致。如果匹配，则获得正奖励；反之，则受到负奖励。</li>
<li><strong>Verifier 的奖励</strong>：它的奖励基于其<strong>判断的准确性</strong>。如果Verifier正确地识别了方案中的错误，它会获得正奖励。然而，如果它错误地将一个正确的解决方案标记为错误（假阳性），则会受到严厉的负奖励。</li>
</ul>

<p>这种机制确保了每个代理的奖励都精确地反映其自身表现，极大地减少了奖励噪声，使训练信号更清晰、更有效。</p>

<h5><strong>2. 代理管道并行性 (Agentic Pipeline Parallelism)：提升训练效率</strong></h5>

<p>多代理系统的另一个挑战是长轨迹带来的计算效率问题。等待整个“生成-验证-修正”轨迹完成后再进行训练，会导致大量的计算资源闲置和时间延迟。</p>

<p>MarsRL借鉴了计算领域的<strong>管道并行（Pipeline Parallelism）</strong>思想，将训练过程分解为代理级别的管道：</p>

<ul>
<li><strong>即时训练队列</strong>：一旦任何一个代理（如Solver）完成了其解码步骤，其输出（连同样本信息）会<strong>立即被添加</strong>到全局训练队列中，而无需等待后续代理完成工作。</li>
<li><strong>并行处理</strong>：所有代理在每个时间步骤生成的结果被聚合起来，共同用于模型训练。这显著缩短了从生成样本到开始训练之间的时间延迟。</li>
<li><strong>分段回滚 (Segment Rollout)</strong>：该框架还结合了分段回滚策略，有效处理长序列解码，进一步提升了在长尾分布任务上的训练效率。</li>
</ul>

<h4><strong>三、 实现细节与优化策略</strong></h4>

<p>为了进一步提升框架性能，MarsRL还引入了以下关键的实现策略：</p>

<h5><strong>1. 负-正自适应采样策略 (Negative-Positive Adaptive Sampling)</strong></h5>

<p>如何为下游代理选择输入样本至关重要。MarsRL探索了三种采样策略，并发现<strong>自适应采样</strong>效果最佳：</p>

<ul>
<li><strong>策略描述</strong>：该策略让代理优先从前一个代理的输出中选择“有价值”的样本进行学习。
<ul>
<li><strong>Verifier</strong> 优先采样那些被判定为<strong>错误</strong>的解决方案（即来自Solver或Corrector的奖励为0的输出）。</li>
<li><strong>Corrector</strong> 优先采样那些被Verifier<strong>正确识别出错误</strong>的轨迹（即来自Verifier的奖励为1的输出）。</li>
</ul></li>
<li><strong>优势</strong>：这种策略迫使模型专注于学习如何识别和修正错误，显著提高了Verifier的错误检测准确率和召回率，从而带动了整个系统的性能提升。</li>
</ul>

<h5><strong>2. 分组代理回滚 (Grouped Agentic Rollouts)</strong></h5>

<p>为了确保不同代理生成的样本具有可比性，从而让模型能够学习到高质量响应与次优响应之间的细微差别，MarsRL采用了分组回滚机制。该机制确保同一组内的样本基于共享的输入进行解码，增强了训练的稳定性和辨别能力。</p>

<h4><strong>四、 实验结果与验证</strong></h4>

<p>论文通过在Qwen3-30B模型上应用MarsRL框架进行了实验验证。结果表明，该框架取得了显著的性能提升：
*   在 <strong>AIME2025</strong> 数据集上，准确率从 <strong>86.5%</strong> 提升至 <strong>93.3%</strong>。
*   在 <strong>BeyondAIME</strong> 数据集上，准确率从 <strong>64.9%</strong> 提升至 <strong>73.8%</strong>。</p>

<p>这些结果有力地证明了MarsRL框架在增强多代理推理系统方面的有效性和巨大潜力。</p>

<h3><strong>总结</strong></h3>

<p><strong>MarsRL</strong> 通过一个结构化的<strong>多代理系统</strong>（Solver, Verifier, Corrector）、创新的<strong>代理式可验证奖励机制</strong>和高效的<strong>代理管道并行训练方法</strong>，成功地解决了多代理推理系统中的奖励噪声和训练效率低下的核心问题。结合<strong>自适应采样</strong>等优化策略，该框架显著提升了模型在复杂推理任务中的准确性和鲁棒性，为未来多代理系统的研究和应用开辟了新的方向。</p>

<h3>实验设计</h3>

<ul>
<li><strong>模型与基准</strong>：将MarsRL框架应用于Qwen3-30B-A3B-Thinking-2507模型，并在AIME2025和BeyondAIME这两个复杂的推理任务基准上进行评估。</li>
<li><strong>评估方法</strong>：
<ul>
<li>比较应用MarsRL前后模型的准确率，以量化其改进效果。</li>
<li>对比不同采样策略（自适应、随机、负-正平衡）对系统性能的影响。</li>
<li>训练过程中，每个代理最多进行五次回滚迭代，并交替进行验证和修正。</li>
</ul></li>
</ul>

<h3>数据集和代码</h3>

<ul>
<li><strong>数据集</strong>：实验使用了AIME-2025和BeyondAIME基准数据集。</li>
<li><strong>代码</strong>：相关的训练模型、多代理系统和评估脚本均已开源，可在以下地址获取：https://github.com/liushulinle/MarsRL</li>
</ul>

<h3>实验结果</h3>

<ul>
<li><strong>性能显著提升</strong>：应用MarsRL后，模型在基准测试上的准确率大幅提高：
<ul>
<li>在AIME2025任务上，准确率从86.5%提升至93.3%。</li>
<li>在BeyondAIME任务上，准确率从64.9%提升至73.8%。</li>
</ul></li>
<li><strong>组件有效性</strong>：实验证明，提出的可验证奖励机制有效减少了噪声，而自适应采样策略在准确性和召回率上显著优于其他策略。</li>
</ul>

<h3>论文贡献</h3>

<ol>
<li>提出了<strong>MarsRL框架</strong>，通过强化学习联合优化多代理系统，有效提升了开源模型在复杂推理任务上的性能。</li>
<li>通过引入<strong>个性化可验证奖励</strong>和<strong>管道并行训练</strong>，成功解决了多代理系统中的奖励噪声和长轨迹训练效率低下的问题。</li>
<li>验证了<strong>自适应采样策略</strong>在提升验证-修正（V-C）系统性能方面的有效性。</li>
<li>通过在公开基准上的实验，证明了该框架的有效性，为多代理推理系统的研究和应用提供了新的思路和方法。</li>
</ol>

            
        </div>

        <div class="footer">
            <p>Generated by AI Paper Review System at 2025-11-20 13:39:36</p>
            <p style="margin-top: 10px;">
                <a href="https://jycarlos1019.pp.ua">系统首页</a> • 
                <a href="../../search.html">搜索归档</a>
            </p>
        </div>
    </div>
</body>
</html>