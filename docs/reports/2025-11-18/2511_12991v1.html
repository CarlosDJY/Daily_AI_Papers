<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty</title>
    <style>
        :root {
            /* 配色方案：Slate + Indigo */
            --primary-color: #4f46e5;
            --bg-body: #f8fafc;
            --bg-paper: #ffffff;
            --text-main: #1e293b;      /* Slate 800 */
            --text-body: #334155;      /* Slate 700 - 正文颜色略浅，减少视觉疲劳 */
            --text-secondary: #64748b; /* Slate 500 */
            --border-color: #e2e8f0;
            --code-bg: #f1f5f9;
            
            /* 警告色 */
            --warn-bg: #fff7ed;
            --warn-text: #9a3412;
            --warn-border: #fdba74;

            --font-stack: '圆体-简', 'Yuanti SC', -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, 'Helvetica Neue', Arial, sans-serif;
            --font-mono: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, "Liberation Mono", "Courier New", monospace;
        }

        * { box-sizing: border-box; margin: 0; padding: 0; }

        body {
            font-family: var(--font-stack);
            background-color: var(--bg-body);
            color: var(--text-body);
            line-height: 1.8; /* 增加行高，适合阅读 */
            padding: 40px 20px;
            min-height: 100vh;
        }

        /* 阅读容器：限制宽度以提升阅读体验 */
        .container {
            max-width: 800px;
            margin: 0 auto;
            background-color: var(--bg-paper);
            border-radius: 16px; /* 更圆润的角 */
            padding: 40px 60px; /* 宽敞的内边距 */
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.05), 0 2px 4px -1px rgba(0, 0, 0, 0.03);
        }

        /* 顶部导航 */
        .nav-bar {
            display: flex;
            justify-content: space-between;
            align-items: center;
            margin-bottom: 40px;
            padding-bottom: 20px;
            border-bottom: 1px solid var(--border-color);
            font-size: 14px;
        }

        .nav-link {
            color: var(--text-secondary);
            text-decoration: none;
            display: inline-flex;
            align-items: center;
            transition: color 0.2s;
        }

        .nav-link:hover { color: var(--primary-color); }
        .nav-link::before { content: "←"; margin-right: 5px; }
        
        .arxiv-link {
            background-color: #f1f5f9;
            color: var(--text-main);
            padding: 6px 12px;
            border-radius: 6px;
            text-decoration: none;
            font-weight: 500;
            transition: all 0.2s;
        }
        
        .arxiv-link:hover {
            background-color: #e2e8f0;
            color: var(--primary-color);
        }

        /* 论文头部信息 */
        .paper-header {
            margin-bottom: 40px;
        }

        .paper-title {
            font-size: 32px;
            font-weight: 700;
            color: var(--text-main);
            line-height: 1.4;
            margin-bottom: 20px;
            letter-spacing: -0.02em;
        }

        /* 标签组 */
        .tags-wrapper {
            display: flex;
            flex-wrap: wrap;
            gap: 8px;
            margin-bottom: 20px;
        }

        .tag {
            background-color: #e0e7ff; /* Indigo 100 */
            color: #4338ca;            /* Indigo 700 */
            font-size: 12px;
            padding: 4px 10px;
            border-radius: 99px;
            font-weight: 500;
        }

        /* 元数据栏 */
        .metadata-box {
            background-color: #f8fafc;
            border: 1px solid var(--border-color);
            border-radius: 12px;
            padding: 20px;
            display: flex;
            flex-wrap: wrap;
            gap: 20px;
            font-size: 14px;
            color: var(--text-secondary);
        }

        .meta-item {
            display: flex;
            flex-direction: column;
            gap: 4px;
        }

        .meta-label {
            font-size: 12px;
            text-transform: uppercase;
            letter-spacing: 0.05em;
            color: #94a3b8;
        }

        .meta-value {
            font-weight: 600;
            color: var(--text-main);
        }
        
        .score-badge {
            color: var(--primary-color);
        }

        /* 核心图片展示 */
        .core-image-container {
            margin: 40px 0;
            text-align: center;
            background-color: #f8fafc;
            padding: 20px;
            border-radius: 12px;
            border: 1px solid var(--border-color);
        }

        .core-image-container img {
            max-width: 100%;
            height: auto;
            border-radius: 8px;
            box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1);
        }
        
        .image-caption {
            margin-top: 10px;
            font-size: 13px;
            color: var(--text-secondary);
            font-style: italic;
        }

        /* 警告框 */
        .warning-box {
            background-color: var(--warn-bg);
            border-left: 4px solid var(--warn-border);
            color: var(--warn-text);
            padding: 15px;
            border-radius: 0 6px 6px 0;
            margin: 20px 0;
            font-size: 14px;
        }

        /* 章节标题 */
        .section-header {
            display: flex;
            align-items: center;
            margin-top: 50px;
            margin-bottom: 25px;
            padding-bottom: 15px;
            border-bottom: 1px dashed var(--border-color);
        }

        .section-header h2 {
            font-size: 24px;
            font-weight: 700;
            color: var(--text-main);
            margin: 0;
            position: relative;
        }
        
        /* 章节前的装饰点 */
        .section-header h2::before {
            content: '';
            display: inline-block;
            width: 8px;
            height: 8px;
            background-color: var(--primary-color);
            border-radius: 50%;
            margin-right: 12px;
            vertical-align: middle;
        }

        /* Markdown 内容样式重置 - 极简学术风 */
        .content-body {
            font-size: 17px; /* 略大的字号适合阅读 */
            color: var(--text-body);
        }

        .content-body p {
            margin-bottom: 1.5em;
            text-align: justify;
        }

        .content-body h3 {
            font-size: 20px;
            font-weight: 600;
            color: var(--text-main);
            margin-top: 2em;
            margin-bottom: 1em;
        }
        
        .content-body h4 {
            font-size: 18px;
            font-weight: 600;
            margin-top: 1.5em;
            margin-bottom: 0.8em;
        }

        .content-body ul, .content-body ol {
            margin-bottom: 1.5em;
            padding-left: 1.5em;
        }

        .content-body li {
            margin-bottom: 0.5em;
        }

        .content-body strong {
            color: var(--text-main);
            font-weight: 600;
        }
        
        /* 引用块 - 学术风 */
        .content-body blockquote {
            border-left: 4px solid var(--primary-color);
            background-color: #f8fafc;
            padding: 16px 20px;
            margin: 20px 0;
            font-style: italic;
            color: var(--text-secondary);
            border-radius: 0 8px 8px 0;
        }

        /* 代码块 */
        .content-body pre {
            background-color: var(--code-bg);
            padding: 16px;
            border-radius: 8px;
            overflow-x: auto;
            margin: 20px 0;
            border: 1px solid var(--border-color);
        }

        .content-body code {
            font-family: var(--font-mono);
            background-color: var(--code-bg);
            padding: 2px 6px;
            border-radius: 4px;
            font-size: 0.9em;
            color: #d63384; /* 类似 GitHub 的代码红 */
        }
        
        .content-body pre code {
            color: inherit;
            padding: 0;
            background-color: transparent;
        }

        /* Footer */
        .footer {
            margin-top: 80px;
            text-align: center;
            color: var(--text-secondary);
            font-size: 13px;
            border-top: 1px solid var(--border-color);
            padding-top: 30px;
        }

        /* 移动端适配 */
        @media (max-width: 768px) {
            body { padding: 0; }
            
            .container {
                border-radius: 0;
                padding: 30px 20px;
                box-shadow: none;
            }

            .paper-title { font-size: 26px; }
            
            .metadata-box {
                flex-direction: column;
                gap: 15px;
            }
            
            .content-body { font-size: 16px; }
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="nav-bar">
            <a href="index.html" class="nav-link">返回今日简报</a>
            <a href="http://arxiv.org/abs/2511.12991v1" target="_blank" class="arxiv-link">PDF / arXiv ↗</a>
        </div>

        <div class="paper-header">
            <h1 class="paper-title">Fine-Tuned LLMs Know They Don't Know: A Parameter-Efficient Approach to Recovering Honesty</h1>
            
            
            <div class="tags-wrapper">
                
                <span class="tag">大语言模型</span>
                
                <span class="tag">诚实性关键神经元恢复</span>
                
                <span class="tag">监督微调</span>
                
                <span class="tag">Hessian引导</span>
                
                <span class="tag">参数效率</span>
                
            </div>
            

            <div class="metadata-box">
                
                <div class="meta-item" style="flex: 2; min-width: 200px;">
                    <span class="meta-label">作者单位</span>
                    <span class="meta-value">SKLCCSE, School of Computer Science and Engineering, Beihang University, School of Software, Beihang University, Zhongguancun Laboratory, Beijing</span>
                </div>
                
                
                <div class="meta-item">
                    <span class="meta-label">推荐指数</span>
                    <span class="meta-value score-badge">0.500</span>
                </div>
                
                <div class="meta-item">
                    <span class="meta-label">arXiv ID</span>
                    <span class="meta-value">2511.12991v1</span>
                </div>
            </div>

            
        </div>

        
        <div class="core-image-container">
            
            <img src="../../images/2025-11-18/ff211390937b279c182a48929eb834fedc4d77880b914809921dd9321fee79b0.jpg" alt="核心思路示意图" />
            <div class="image-caption">图 1：论文核心方法/架构示意图</div>
        </div>
        

        <div class="section-header">
            <h2>快速简介</h2>
        </div>
        <div class="content-body">
            <p>本文提出了一种名为“诚实性关键神经元恢复”（HCNR）的方法，旨在解决大语言模型（LLM）在监督微调后诚实性下降的问题。HCNR通过识别并恢复对诚实性至关重要的神经元，同时应用Hessian引导的补偿机制，显著提高模型的诚实性，恢复率达33.25%，且在数据使用和速度上均优于传统方法。</p>
        </div>

        <div class="section-header">
            <h2>深度解读</h2>
        </div>
        <div class="content-body">
            
                <h3>现有问题</h3>

<p>本文旨在解决一个核心问题：大语言模型（LLM）在经过监督微调（SFT）后，其诚实性（即识别并承认自身知识边界的能力）会显著下降。尽管模型内部可能仍然保留着对其知识局限性的认知，但其表达这种不确定性的能力却受到了损害。这是一个重要的问题，因为在金融、医疗等高风险领域，LLM的虚假陈述可能导致严重后果。现有恢复诚实性的方法通常需要大规模的参数调整和大量数据，并且往往以牺牲模型在特定任务上的性能为代价。</p>

<h3>Hypothesis</h3>

<p>论文的核心假设是：LLM在SFT后诚实性的下降，并非源于其自我认知能力的丧失，而是表达能力的受损。因此，可以通过识别并选择性地恢复一小部分对诚实性至关重要的神经元（Honesty-Critical Neurons）到它们预训练时的状态，来有效恢复模型的诚实性，同时通过补偿机制，确保这一过程不会损害模型在下游任务上的性能。</p>

<h3>相关研究</h3>

<ul>
<li><strong>全局参数调整方法</strong>：先前的工作（如Zhang et al. 2023; Cheng et al. 2024）主要依赖于对模型进行广泛的参数调整来恢复诚实性。</li>
<li><strong>对齐技术</strong>：研究了如强化学习从人类反馈（RLHF）、直接偏好优化（DPO）、拒绝意识指令调优（RAIT）等技术，用于提升模型的诚实性和安全性。</li>
<li><strong>神经元功能研究</strong>：相关研究表明，模型中的特定神经元与特定的知识或能力相关联，这为通过神经元层面的干预来修改模型行为提供了理论基础。</li>
</ul>

<h3><strong>完整的详细解决方案：Honesty-Critical Neurons Restoration (HCNR)</strong></h3>

<h4><strong>引言：问题与核心思想</strong></h4>

<p>大型语言模型（LLMs）在经过监督微调（Supervised Fine-Tuning, SFT）以提升特定任务性能后，常常会丧失其“诚实性”，即无法承认自己知识的边界，倾向于捏造答案。论文提出的解决方案 <strong>Honesty-Critical Neurons Restoration (HCNR)</strong>，旨在精准地解决这一问题。</p>

<p>HCNR的核心思想基于一个关键观察：经过SFT的模型，其<strong>自我知识（self-knowledge）</strong>的能力通常依然保留，但其<strong>自我表达（self-expression）</strong>的能力受到了损害。换言之，模型内心“知道”自己不知道答案，却无法诚实地表达出来。因此，HCNR采取了一种高效的“神经元手术”方法，通过针对性地修复负责诚实性表达的关键神经元，来恢复模型的诚实性，而不是进行代价高昂的全局参数重训练。</p>

<h4><strong>HCNR 框架详解</strong></h4>

<p>HCNR框架通过一个分为两个主要阶段的系统化流程，实现对模型诚实性的恢复，同时最大限度地保留其在下游任务上的性能。</p>

<h5><strong>第一阶段：识别与恢复诚实性关键神经元 (Identification and Restoration)</strong></h5>

<p>此阶段的目标是精确地定位并修复那些在微调过程中受损的、对诚实性表达至关重要的神经元。</p>

<ol>
<li><p><strong>神经元重要性评估与筛选</strong>：</p>

<ul>
<li>首先，框架使用<strong>费雪信息矩阵（Fisher Information Matrix）</strong>或二阶泰勒展开式来评估每个神经元对“诚实性任务”和“下游任务”的贡献度。</li>
<li>通过计算每个神经元在这两类任务中的重要性分数，定义一个优先级。HCNR会优先选择那些<strong>在诚实性任务中贡献高，但在下游任务中贡献低</strong>的神经元作为候选。这一策略确保了恢复过程能精准地作用于诚实性，同时尽量减少对任务性能的干扰。</li>
</ul></li>
<li><p><strong>扰动强度量化与识别</strong>：</p>

<ul>
<li>在候选神经元中，框架会进一步量化它们在SFT过程中所受到的<strong>扰动强度</strong>，即其参数与预训练状态的差异。</li>
<li>最终，选择那些被严重干扰的神经元作为“诚实性关键神经元（Honesty-Critical Neurons）”。</li>
</ul></li>
<li><p><strong>神经元状态恢复</strong>：</p>

<ul>
<li>将识别出的诚实性关键神经元的参数<strong>恢复到其预训练时的状态</strong>。与此同时，其他负责下游任务的神经元则保持其微调后的参数不变。</li>
</ul></li>
</ol>

<h5><strong>第二阶段：Hessian引导的诚实性补偿 (Honesty Compensation)</strong></h5>

<p>简单地恢复神经元会导致新的问题：这些恢复到预训练状态的“诚实”神经元与已经适应了新任务的“任务导向”神经元之间存在<strong>参数不协调（parameter disharmony）</strong>的问题。这可能导致模型性能下降。</p>

<p>为解决此问题，HCNR引入了Hessian引导的补偿机制：</p>

<ol>
<li><strong>目标</strong>：在不改变已恢复的诚实性关键神经元参数的前提下，对<strong>其他未被恢复的神经元</strong>进行微小、精确的调整，以补偿因恢复操作而引起的性能损失。</li>
<li><strong>方法</strong>：
<ul>
<li>利用<strong>Hessian矩阵的逆</strong>来近似计算模型损失函数对参数的二阶导数。这使得框架能够理解参数间的相互作用关系。</li>
<li>通过这种方式，HCNR可以进行高效的<strong>二阶优化</strong>，对任务导向的神经元进行微调，使其与恢复后的诚实性神经元重新协调，从而在恢复诚实性的同时，保持甚至提升模型在下游任务上的表现。</li>
</ul></li>
</ol>

<h4><strong>总结：HCNR的优势</strong></h4>

<ul>
<li><strong>精准高效</strong>：通过“神经元手术”而非全局重训练，精准定位并修复问题，大大降低了计算成本和数据需求。</li>
<li><strong>性能保持</strong>：Hessian引导的补偿机制确保了在恢复诚实性的同时，不会牺牲模型在特定任务上的性能。</li>
<li><strong>机制洞察</strong>：该方法基于对模型内部工作机制的深刻理解，揭示了SFT对模型诚实性影响的本质。</li>
</ul>

<h3>实验设计</h3>

<ul>
<li><strong>模型与任务</strong>：实验在多个开源LLM（如Llama和Mistral系列）上进行，这些模型在问答数据集（如HotpotQA、MedMCQA）上进行了微调。</li>
<li><strong>评估基准</strong>：使用专门的诚实性评估基准（如FalseQA和自定义数据集Dhon）来衡量模型的诚实性恢复效果。</li>
<li><strong>对比实验</strong>：将HCNR与多种基线方法（如RAIT、DPO）进行性能对比。</li>
<li><strong>消融研究</strong>：通过消融实验验证HCNR框架中各个组件（如神经元识别、补偿机制）的有效性，并测试了不同数据量下的性能。</li>
</ul>

<h3>数据集和代码</h3>

<ul>
<li><strong>数据集</strong>：实验中使用了<strong>HotpotQA</strong>和<strong>MedMCQA</strong>进行微调，并使用<strong>FalseQA</strong>和专门构建的<strong>Dhon</strong>数据集进行诚实性评估。</li>
<li><strong>代码</strong>：论文片段中未提供代码的直接链接。</li>
</ul>

<h3>实验结果</h3>

<p>实验结果有力地支持了论文的假设。HCNR框架在多个诚实性基准测试中均优于所有基线方法。
- <strong>高效性</strong>：HCNR在仅使用极少量数据（例如256个样本）和修改少量参数（例如20%）的情况下，就能实现显著的诚实性恢复（恢复率达33.25%），且速度比基线方法快2.23倍以上。
- <strong>性能平衡</strong>：该方法在显著提升模型诚实性的同时，成功地保持了其在下游任务上的高准确性，实现了诚实性与任务性能的最佳平衡。</p>

<h3>论文贡献</h3>

<ol>
<li><strong>揭示了新机制</strong>：首次提出并验证了SFT导致的模型不诚实行为是表层的“表达能力”问题，而非深层的“自我认知”丧失。</li>
<li><strong>提出了新框架</strong>：设计并实现了HCNR框架，为恢复LLM的诚实性提供了一种高效、数据节约且性能保持良好的新方法。</li>
<li><strong>提供了实用方案</strong>：为在高风险领域部署更可信、更安全的AI系统提供了切实可行的技术解决方案，推动了可信AI的发展。</li>
</ol>

            
        </div>

        <div class="footer">
            <p>Generated by AI Paper Review System at 2025-11-20 13:55:38</p>
            <p style="margin-top: 10px;">
                <a href="https://jycarlos1019.pp.ua">系统首页</a> • 
                <a href="../../search.html">搜索归档</a>
            </p>
        </div>
    </div>
</body>
</html>